{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import nibabel as nib\n",
    "from nilearn.plotting import plot_img, plot_stat_map, view_img, plot_prob_atlas\n",
    "from nilearn.regions import connected_label_regions\n",
    "from nilearn.glm.first_level.hemodynamic_models import spm_hrf\n",
    "from nilearn.image import concat_imgs, mean_img, index_img, smooth_img\n",
    "from nilearn.glm.first_level import FirstLevelModel\n",
    "from nilearn.glm import threshold_stats_img\n",
    "import matplotlib.pyplot as plt\n",
    "from nilearn.plotting import plot_roi\n",
    "from nilearn.maskers import NiftiMapsMasker, NiftiSpheresMasker\n",
    "from scipy.interpolate import interp1d\n",
    "import seaborn as sns\n",
    "from scipy.stats.mstats import pearsonr\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, as I understand it: \n",
    "1) Mean center valence and arousal scores. \n",
    "2) To obtain positive and negative arousal scores use \n",
    "- Pos_arousal -> (arousal_i - valence_i)/sqrt(2)\n",
    "- Neg_arousal -> (arousal_i - valence_i)/sqrt(2)\n",
    "\n",
    "- Split like and no-like horror movies. \n",
    "\n",
    "\n",
    "- We get best results for MPFC, which is the time course with most structure, try different parameters with NAcc to see if it helps. \n",
    "- Use different mask with different parameters for cortical and sub-cortical regions. \n",
    "\n",
    "- operationalize peak as delta (x_1 - x_2)/(x_1 + x_2)\n",
    "- Also, calculate slope between tr_1 and tr_2 that crosses a 1 std. \n",
    "- Get the highest correlation for comedy, see when TR happens. \n",
    "\n",
    "- What to do if there's more than one peak (i.e., if there's more than two points higher than std)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homemade functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Merge(dict1, dict2):\n",
    "\n",
    "    # create a new dictionary by merging the items of the two dictionaries using the union operator (|)\n",
    "    merged_dict = dict(dict1.items() | dict2.items())\n",
    "    \n",
    "    # return the merged dictionary\n",
    "    return merged_dict\n",
    "\n",
    "def merge_dictionaries(dict1, dict2):\n",
    "    merged_dict = dict1.copy()\n",
    "    merged_dict.update(dict2)\n",
    "    return merged_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to get different peaks (slope and deltas).\n",
    "def get_peak_slope(timecourse, timecourse_z, z_threshold, max_min):\n",
    "\n",
    "    # Store all possible peak in timecourse array.\n",
    "    possible_slope_peaks = []\n",
    "\n",
    "    # Loop through each point in timecourse array, except the first one.\n",
    "    for id_tr in range(1, 15, 1):\n",
    "\n",
    "        # For positive peaks. \n",
    "        if(max_min == \"max\"):\n",
    "\n",
    "            # If score is bigger than \"z_threshold\" z-score in timecourse_z array, append value.\n",
    "            if(timecourse_z[id_tr] > z_threshold): \n",
    "\n",
    "                slope = timecourse[id_tr] - timecourse[id_tr - 1] \n",
    "                # Check the slope is not equal to zero. \n",
    "                if(slope != 0):\n",
    "                    possible_slope_peaks.append(slope)\n",
    "        \n",
    "        # For negative peaks.\n",
    "        if(max_min == \"min\"):\n",
    "\n",
    "            # If score is bigger than \"z_threshold\" z-score in timecourse_z array, append value.\n",
    "            if(timecourse_z[id_tr] < z_threshold): \n",
    "\n",
    "                slope = timecourse[id_tr] - timecourse[id_tr - 1] \n",
    "                # Check the slope is not equal to zero. \n",
    "                if(slope != 0):\n",
    "                    possible_slope_peaks.append(slope)\n",
    "    \n",
    "    average_slope = np.mean(possible_slope_peaks)\n",
    "\n",
    "    # Return only the first value and average slope.\n",
    "    if(len(possible_slope_peaks) == 0):\n",
    "        return 0, 0\n",
    "    else: \n",
    "        return possible_slope_peaks[0], average_slope\n",
    "\n",
    "def get_narrow_peak_slope(timecourse, z_threshold, max_min):\n",
    "\n",
    "    # Z-score the 15 data points.\n",
    "    z_scored_data = stats.zscore(timecourse)\n",
    "\n",
    "    # Store all possible peak in timecourse array.\n",
    "    possible_slope_peaks = []\n",
    "\n",
    "    # Loop through each point in timecourse array, except the first one.\n",
    "    for id_tr in range(1, 15, 1):\n",
    "\n",
    "        # For positive peaks.\n",
    "        if(max_min == \"max\"):\n",
    "            # If score is bigger than \"z_threshold\" z-score in timecourse_z array, append value.\n",
    "            if(z_scored_data[id_tr] > z_threshold): \n",
    "\n",
    "                slope = timecourse[id_tr] - timecourse[id_tr - 1] \n",
    "                # Check the slope is not equal to zero. \n",
    "                if(slope != 0):\n",
    "                    possible_slope_peaks.append(slope)\n",
    "    \n",
    "        # For negative peaks.\n",
    "        if(max_min == \"min\"):\n",
    "            # If score is bigger than \"z_threshold\" z-score in timecourse_z array, append value.\n",
    "            if(z_scored_data[id_tr] < z_threshold): \n",
    "\n",
    "                slope = timecourse[id_tr] - timecourse[id_tr - 1] \n",
    "                # Check the slope is not equal to zero. \n",
    "                if(slope != 0):\n",
    "                    possible_slope_peaks.append(slope)\n",
    "    \n",
    "    average_slope = np.mean(possible_slope_peaks)\n",
    "\n",
    "    # Return only the first value and average slope.\n",
    "    if(len(possible_slope_peaks) == 0):\n",
    "        return 0, 0\n",
    "    else: \n",
    "        return possible_slope_peaks[0], average_slope\n",
    "    \n",
    "def get_peak_delta(timecourse, timecourse_z, z_threshold, max_min):\n",
    "\n",
    "    # Store all possible peak in timecourse array.\n",
    "    possible_delta_peaks = []\n",
    "\n",
    "    # Loop through each point in timecourse array, except the first one.\n",
    "    for id_tr in range(1, 15, 1):\n",
    "\n",
    "        # For positive peaks. \n",
    "        if(max_min == \"max\"):\n",
    "\n",
    "            # If score is bigger than \"z_threshold\" z-score in timecourse_z array, append value.\n",
    "            if(timecourse_z[id_tr] > z_threshold): \n",
    "\n",
    "                delta = (timecourse[id_tr] - timecourse[id_tr - 1])/(timecourse[id_tr]*100) \n",
    "                #delta = (timecourse[id_tr] - timecourse[id_tr - 1])/(timecourse[id_tr] + timecourse[id_tr - 1]) \n",
    "                # Check the slope is not equal to zero. \n",
    "                if(delta != 0):\n",
    "                    possible_delta_peaks.append(delta)\n",
    "        \n",
    "        # For negative peaks.\n",
    "        if(max_min == \"min\"):\n",
    "\n",
    "            # If score is bigger than \"z_threshold\" z-score in timecourse_z array, append value.\n",
    "            if(timecourse_z[id_tr] < z_threshold): \n",
    "\n",
    "                delta = (timecourse[id_tr] - timecourse[id_tr - 1])/(timecourse[id_tr]*100) \n",
    "                #delta = (timecourse[id_tr] - timecourse[id_tr - 1])/(timecourse[id_tr] + timecourse[id_tr - 1]) \n",
    "                # Check the slope is not equal to zero. \n",
    "                if(delta != 0):\n",
    "                    possible_delta_peaks.append(delta)\n",
    "    \n",
    "    average_delta = np.mean(possible_delta_peaks)\n",
    "\n",
    "    # Return only the first value and average slope.\n",
    "    if(len(possible_delta_peaks) == 0):\n",
    "        return 0, 0\n",
    "    else: \n",
    "        return possible_delta_peaks[0], average_delta\n",
    "    \n",
    "\n",
    "def get_narrow_peak_delta(timecourse, z_threshold, max_min):\n",
    "\n",
    "    # Z-score the 15 data points.\n",
    "    z_scored_data = stats.zscore(timecourse)\n",
    "\n",
    "    # Store all possible peak in timecourse array.\n",
    "    possible_delta_peaks = []\n",
    "\n",
    "    # Loop through each point in timecourse array, except the first one.\n",
    "    for id_tr in range(1, 15, 1):\n",
    "\n",
    "        # For positive peaks.\n",
    "        if(max_min == \"max\"):\n",
    "            # If score is bigger than \"z_threshold\" z-score in timecourse_z array, append value.\n",
    "            if(z_scored_data[id_tr] > z_threshold): \n",
    "\n",
    "                delta = (timecourse[id_tr] - timecourse[id_tr - 1])/(timecourse[id_tr]*100) \n",
    "                #delta = (timecourse[id_tr] - timecourse[id_tr - 1])/(timecourse[id_tr] + timecourse[id_tr - 1]) \n",
    "                # Check the slope is not equal to zero. \n",
    "                if(delta != 0):\n",
    "                    possible_delta_peaks.append(delta)\n",
    "    \n",
    "        # For negative peaks.\n",
    "        if(max_min == \"min\"):\n",
    "            # If score is bigger than \"z_threshold\" z-score in timecourse_z array, append value.\n",
    "            if(z_scored_data[id_tr] < z_threshold): \n",
    "\n",
    "                delta = (timecourse[id_tr] - timecourse[id_tr - 1])/(timecourse[id_tr]*100) \n",
    "                #delta = (timecourse[id_tr] - timecourse[id_tr - 1])/(timecourse[id_tr] + timecourse[id_tr - 1]) \n",
    "                # Check the slope is not equal to zero. \n",
    "                if(delta != 0):\n",
    "                    possible_delta_peaks.append(delta)\n",
    "    \n",
    "    average_delta = np.mean(possible_delta_peaks)\n",
    "\n",
    "    # Return only the first value and average slope.\n",
    "    if(len(possible_delta_peaks) == 0):\n",
    "        return 0, 0\n",
    "    else: \n",
    "        return possible_delta_peaks[0], average_delta\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trailer_correlations(ROI_df, Trailer_id, ROI_id, TR_corr_dict, type_of_corr, axs):\n",
    "\n",
    "    if(type_of_corr == \"W_score\"):\n",
    "        color=\"green\"\n",
    "        bonus_y = 0.00\n",
    "        score_label = \"_w\"\n",
    "        text_label = \"W corr\"\n",
    "    elif(type_of_corr == \"Pos_arousal\"):\n",
    "        color=\"blue\"\n",
    "        bonus_y = 0.05\n",
    "        score_label = \"_pa\"\n",
    "        text_label = \"PA corr\"\n",
    "    elif(type_of_corr == \"Neg_arousal\"):\n",
    "        color=\"red\"\n",
    "        bonus_y = -0.05\n",
    "        score_label = \"_na\"\n",
    "        text_label = \"NA corr\"\n",
    "\n",
    "    if(ROI_id == \"Bilateral_NAcc\"):\n",
    "        y_coord_seg = 0.3 + bonus_y\n",
    "        y_coord_whole = 0.4\n",
    "    elif(ROI_id == \"Bilateral_AIns\"):\n",
    "        y_coord_seg = 0.4 + bonus_y\n",
    "        y_coord_whole = 0.5\n",
    "    elif(ROI_id == \"Bilateral_MPFC\"):\n",
    "        y_coord_seg = 0.4 + bonus_y\n",
    "        y_coord_whole = 0.5\n",
    "\n",
    "    # Get the dataframe for the current trailer.\n",
    "    current_traile_df = ROI_df[Trailer_id]\n",
    "\n",
    "    # Loop for plotting each significant correlation across the 15 TRs.\n",
    "    for TR_id in range(15):\n",
    "        TR_label = str(\"TR_\" + str(TR_id) + score_label) \n",
    "        y_coord = current_traile_df[(current_traile_df[\"ROI\"] == ROI_id) & (current_traile_df[\"TR\"] == TR_id)][\"Signal\"].mean() + bonus_y\n",
    "        x_coord = TR_id\n",
    "        if(TR_corr_dict[Trailer_id][TR_label][1] < 0.05):\n",
    "            if(TR_corr_dict[Trailer_id][TR_label][0] > 0):\n",
    "                axs.text(x_coord, y_coord, \"+*\", fontsize=13, color=color, weight=\"bold\", label=text_label)\n",
    "            else:\n",
    "                axs.text(x_coord, y_coord, \"-*\", fontsize=13, color=color, weight=\"bold\", label=text_label)\n",
    "    \n",
    "    # Plot the onset correlation if significant.\n",
    "    #current_traile_df = ROI_df[Trailer_id]\n",
    "    Onset_label = str(\"Onset\" + score_label)\n",
    "    #y_coord_seg = (current_traile_df[(current_traile_df[\"ROI\"] == ROI_id)][\"Signal\"].mean() ) + bonus_y #* 0.7\n",
    "    x_coord_onset = 3\n",
    "\n",
    "    if(TR_corr_dict[Trailer_id][Onset_label][1] < 0.05):\n",
    "        if(TR_corr_dict[Trailer_id][Onset_label][0] > 0):\n",
    "            axs.text(x_coord_onset, y_coord_seg, \"on(+)\", fontsize=13, color=color, weight=\"bold\", label=text_label)\n",
    "        else:\n",
    "            axs.text(x_coord_onset, y_coord_seg, \"on(-)\", fontsize=13, color=color, weight=\"bold\", label=text_label)\n",
    "\n",
    "    # Plot the middle correlation if significant.\n",
    "    Middle_label = str(\"Middle\" + score_label)\n",
    "    x_coord_middle = 7\n",
    "    if(TR_corr_dict[Trailer_id][Middle_label][1] < 0.05):\n",
    "        if(TR_corr_dict[Trailer_id][Middle_label][0] > 0):\n",
    "            axs.text(x_coord_middle, y_coord_seg, \"mid(+)\", fontsize=13, color=color, weight=\"bold\", label=text_label)\n",
    "        else:\n",
    "            axs.text(x_coord_middle, y_coord_seg, \"mid(-)\", fontsize=13, color=color, weight=\"bold\", label=text_label)\n",
    "\n",
    "    # Plot the offset correlation if significant.\n",
    "    Offset_label = str(\"Offset\" + score_label)\n",
    "    x_coord_offset = 13\n",
    "    if(TR_corr_dict[Trailer_id][Offset_label][1] < 0.05):\n",
    "        if(TR_corr_dict[Trailer_id][Offset_label][0] > 0):\n",
    "            axs.text(x_coord_offset, y_coord_seg, \"off(+)\", fontsize=13, color=color, weight=\"bold\", label=text_label)\n",
    "        else:\n",
    "            axs.text(x_coord_offset, y_coord_seg, \"off(-)\", fontsize=13, color=color, weight=\"bold\", label=text_label)\n",
    "\n",
    "    # Plot the whole correlation if significant.\n",
    "    Whole_label = str(\"Whole\" + score_label)\n",
    "    x_coord_whole = 7\n",
    "    #y_coord_whole = (current_traile_df[(current_traile_df[\"ROI\"] == ROI_id)][\"Signal\"].max() ) + bonus_y # * 0.8\n",
    "    if(TR_corr_dict[Trailer_id][Whole_label][1] < 0.05):\n",
    "        if(TR_corr_dict[Trailer_id][Whole_label][0] > 0):\n",
    "            axs.text(x_coord_whole, y_coord_whole, \"whole(+)\", fontsize=13, color=color, weight=\"bold\", label=text_label)\n",
    "        else:\n",
    "            axs.text(x_coord_whole, y_coord_whole, \"whole(-)\", fontsize=13, color=color, weight=\"bold\", label=text_label)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to invert scores if needed. \n",
    "def transform_scores(score, invert):\n",
    "    if(invert==1): #if(not invert):\n",
    "        if(score == 1):  \n",
    "            return 4\n",
    "        elif(score == 2):  \n",
    "            return 3\n",
    "        elif(score == 3):  \n",
    "            return 2\n",
    "        elif(score == 4):  \n",
    "            return 1\n",
    "        elif(score == \"1\"):  \n",
    "            return 4\n",
    "        elif(score == \"2\"):  \n",
    "            return 3\n",
    "        elif(score == \"3\"):  \n",
    "            return 2\n",
    "        elif(score == \"4\"):  \n",
    "            return 1\n",
    "        elif(score == \"None\"):\n",
    "            return 0\n",
    "        else: \n",
    "            return \"Something went wrong here!\"\n",
    "    else:\n",
    "        if(score == \"1\"):\n",
    "            return 1\n",
    "        elif(score == \"2\"):\n",
    "            return 2\n",
    "        elif(score == \"3\"):\n",
    "            return 3\n",
    "        elif(score == \"4\"):\n",
    "            return 4\n",
    "        elif(score == 1):\n",
    "            return 1\n",
    "        elif(score == 2):\n",
    "            return 2\n",
    "        elif(score == 3):\n",
    "            return 3\n",
    "        elif(score == 4):\n",
    "            return 4\n",
    "        elif(score == \"None\"):\n",
    "            return 0\n",
    "        else:\n",
    "            return \"Something went wrong here!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percent signal change .\n",
    "def get_psc(timecourse):\n",
    "\n",
    "   # Get number of ROIs and data points in timecourse.\n",
    "   roi_num = timecourse.shape[1]\n",
    "   data_length = timecourse.shape[0]\n",
    "\n",
    "   # Copy timecourse into new array.\n",
    "   psc_timecourse = np.zeros(timecourse.shape)\n",
    "\n",
    "   # Warning for empty arrays. \n",
    "   if(roi_num ==0):\n",
    "      print(\"Watch out, this array is empty!\")\n",
    "\n",
    "   # Loop through every ROI and derive the psc. \n",
    "   for id in range(roi_num):\n",
    "\n",
    "      current_roi_avg = np.mean(timecourse[:, id], axis=0)\n",
    "\n",
    "      for idx in range(data_length):\n",
    "\n",
    "         # Formula to get percent signal change -> ((point-avg)/avg)*100.\n",
    "         psc_timecourse[idx, id] = ((timecourse[idx, id] - current_roi_avg)/ current_roi_avg)*100\n",
    "\n",
    "   return psc_timecourse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_events_data(run_dataframe):\n",
    "\n",
    "    proccesed_events_df = pd.DataFrame(columns={\"Trailer\", \"Type\", \"Onset\", \"Duration\", \"Offset\", \"W_score\", \"A_score\", \"F_score\"}) \n",
    "\n",
    "    # Initial fixation 12 sec (TR=6).\n",
    "    in_fix = 12\n",
    "\n",
    "    # Time it take subjects to complete questionnaire 12 sec (TR=6). \n",
    "    questionnaire_duration = 12\n",
    "\n",
    "    # All trailers last 30 sec (TR=15). \n",
    "    trailer_duration = 30\n",
    "\n",
    "    # Initialize this variable, though it will change through each iteration of the loop.\n",
    "    trailer_onset = 0\n",
    "\n",
    "    # Run a for loop for each row in the df. \n",
    "    for id in range(run_dataframe.shape[0]):\n",
    "\n",
    "        # Get trailer label and separate them accroding to their type. \n",
    "        trailer_name = run_dataframe[\"label\"][id]\n",
    "        trailer_type = \"Horror\" if \"h\" in run_dataframe[\"label\"][id] else \"Comedy\"\n",
    "\n",
    "        # This onsets don't work, so I need to re-calculate them\n",
    "        traile_iti = run_dataframe[\"trial_ITI\"][id]\n",
    "        \n",
    "        # For the first run add the initial fixation time to the calculation of the first trailer onset. \n",
    "        # After the first run, calculate onset by adding previous traile onset, questionnaire duration, and trial iti.\n",
    "        if (id == 0):\n",
    "            trailer_onset += in_fix\n",
    "        else:\n",
    "            trailer_onset += trailer_duration + questionnaire_duration + traile_iti\n",
    "\n",
    "        # Calculate trailer onset. \n",
    "        trailer_offset = trailer_onset + 30 \n",
    "\n",
    "        \"\"\" \n",
    "        For the questionnaire scores, as I understood it. If they were not inverted ([\"scale_flip\"] == 0), then \n",
    "        the lower the score the stronger the response. If they were inverted ([\"scale_flip\"] == 1), the higher the \n",
    "        score the stronger the response.\n",
    "        \"\"\"\n",
    "        # Check if scaled was flipped and put scores on the same scale. \n",
    "        # For me, the most intuitive is that the higher the score, the stronger the response. \n",
    "        trailer_watch_score = transform_scores(run_dataframe[\"exp_Watch.keys\"][id], run_dataframe[\"scale_flip\"][id])\n",
    "        trailer_arousal_score = transform_scores(run_dataframe[\"exp_Arousal.keys\"][id], run_dataframe[\"scale_flip\"][id])\n",
    "        trailer_feel_score = transform_scores(run_dataframe[\"exp_Feel.keys\"][id], run_dataframe[\"scale_flip\"][id])\n",
    "\n",
    "        # Place processed data on list, add list to new dataframe, and concat to main dataframe. \n",
    "        current_row_data = [[trailer_name, trailer_type, trailer_onset, trailer_duration, trailer_offset, trailer_watch_score, trailer_arousal_score, trailer_feel_score]]\n",
    "        current_row = pd.DataFrame(data=current_row_data, columns=[\"Trailer\", \"Type\", \"Onset\", \"Duration\", \"Offset\", \"W_score\", \"A_score\", \"F_score\"]) \n",
    "        proccesed_events_df = pd.concat([proccesed_events_df, current_row], ignore_index=True)\n",
    "        proccesed_events_df = proccesed_events_df[[\"Trailer\", \"Type\", \"Onset\", \"Offset\", \"Duration\", \"W_score\", \"A_score\", \"F_score\"]]\n",
    "\n",
    "    # Add a column for the mean centered arousal and valence scores.\n",
    "    mean_centered_arousal = proccesed_events_df[\"A_score\"]/proccesed_events_df[\"A_score\"].mean()\n",
    "    mean_centered_valence = proccesed_events_df[\"W_score\"]/proccesed_events_df[\"W_score\"].mean()\n",
    "\n",
    "    # Derive the positive and negative arousal scores.\n",
    "    proccesed_events_df[\"Pos_arousal\"] = (mean_centered_arousal+mean_centered_valence)/np.sqrt(2)\n",
    "    proccesed_events_df[\"Neg_arousal\"] = (mean_centered_arousal-mean_centered_valence)/np.sqrt(2)\n",
    "\n",
    "    # Derive the positive and negative arousal scaled scores. \n",
    "    pos_arousal_mean = proccesed_events_df[\"Pos_arousal\"].mean()\n",
    "    neg_arousal_mean = proccesed_events_df[\"Neg_arousal\"].mean()\n",
    "\n",
    "    pos_arousal_std = np.std(proccesed_events_df[\"Pos_arousal\"], axis=0, ddof=1)\n",
    "    neg_arousal_std = np.std(proccesed_events_df[\"Neg_arousal\"], axis=0, ddof=1)\n",
    "\n",
    "    proccesed_events_df[\"Pos_arousal_scaled\"] = (proccesed_events_df[\"Pos_arousal\"] - pos_arousal_mean)\n",
    "    proccesed_events_df[\"Neg_arousal_scaled\"] = (proccesed_events_df[\"Neg_arousal\"] - neg_arousal_mean)\n",
    "\n",
    "    # Derive scales Watch scores. \n",
    "    watch_score_mean = proccesed_events_df[\"W_score\"].mean()    \n",
    "    watch_score_std = np.std(proccesed_events_df[\"W_score\"], axis=0, ddof=1)\n",
    "    proccesed_events_df[\"W_score_scaled\"] = (proccesed_events_df[\"W_score\"] - watch_score_mean)\n",
    "\n",
    "    return proccesed_events_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:12: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<>:12: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "/var/folders/0z/1yt2h6410kb7_mgghf4q28z00000gn/T/ipykernel_3366/1498170635.py:12: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if((roi_key in dictionary_keys) and (dictionary[roi_key] is not None) and (dictionary[roi_key] is not \"None\")):\n"
     ]
    }
   ],
   "source": [
    "def access_trailer(dictionary, trailer_key): \n",
    "    try:\n",
    "        trailer_data = dictionary[trailer_key]\n",
    "        return trailer_data\n",
    "    except KeyError:\n",
    "        pass    \n",
    "\n",
    "def access_timecourse(dictionary, roi_key):\n",
    "    \n",
    "    dictionary_keys = list(dictionary.keys())\n",
    "\n",
    "    if((roi_key in dictionary_keys) and (dictionary[roi_key] is not None) and (dictionary[roi_key] is not \"None\")):\n",
    "        roi_data = np.array(dictionary[roi_key])\n",
    "        return roi_data\n",
    "    \n",
    "    \n",
    "    # try:\n",
    "    #     print(dictionary)\n",
    "    #     if(dictionary[roi_key] == None):\n",
    "    #         print(\"This is empty!\")\n",
    "    #         print(dictionary[roi_key])\n",
    "    #     if(dictionary[roi_key] is not None):\n",
    "    #         roi_data = np.array(dictionary[roi_key])\n",
    "    #         return roi_data\n",
    "    \n",
    "    # except KeyError:\n",
    "    #     pass\n",
    "\n",
    "\n",
    "def trim_timecourse_per_roi(timecourses_dictionary_list, timecourses_dictionary_raw, ROI):\n",
    "\n",
    "    # 1) Create a new dictionary to store the trimmed timecourses.\n",
    "    # First file will store the avg timecourse for each trailer per roi (ROI x trailer).\n",
    "    # Second file will store the timecourse of all subjects for each trailer per roi (ROI x subjects x trailer).\n",
    "    # Third file will store the timecourse of all subjects for each trailer per roi in a dataframe (ROI x subjects x trailer).\n",
    "    all_subjects_avg_ROI_timecourse = {}\n",
    "    all_subjects_ROI_timecourse = {}\n",
    "    all_subjects_ROI_timecourse_df = {}\n",
    "\n",
    "    bROI = \"Bilateral_\" + ROI\n",
    "    bROI_r1 = \"Bilateral_\" + ROI + \"_r1\"\n",
    "    bROI_r2 = \"Bilateral_\" + ROI + \"_r2\"\n",
    "    rROI = \"Right_\" + ROI\n",
    "    rROI_r1 = \"Right_\" + ROI + \"_r1\"\n",
    "    rROI_r2 = \"Right_\" + ROI + \"_r2\"\n",
    "    lROI = \"Left_\" + ROI\n",
    "    lROI_r1 = \"Left_\" + ROI + \"_r1\"\n",
    "    lROI_r2 = \"Left_\" + ROI + \"_r2\"\n",
    "\n",
    "    # Get all keys from the first participant dictionary.\n",
    "    trailer_list = list(timecourses_dictionary_list[\"sub-01\"].keys())\n",
    "    subjects_list = list(timecourses_dictionary_raw.keys())\n",
    "\n",
    "    if(ROI != \"V1\"):\n",
    "        # Loop through all trailer keys.\n",
    "        for id in range(len(trailer_list)): \n",
    "    \n",
    "            # Create new participant list with only participants that have the current trailer.\n",
    "            participants_with_trailer = [d for d in timecourses_dictionary_list if trailer_list[id] in timecourses_dictionary_list[d]] # Store participant id if they have the current trailer.\n",
    "        \n",
    "            # Make copy of dictionary with only participants that have the current trailer.\n",
    "            timecourses_dictionary_list_current_trailer = {k: timecourses_dictionary_list[k] for k in participants_with_trailer}\n",
    "        \n",
    "            # Loop through all participants and get the values for the current trailer.\n",
    "            # Note, a wrapper function just to return the values for the current trailer.\n",
    "            # if they exist, otherwise return None.\n",
    "            values = [access_trailer(timecourses_dictionary_list_current_trailer[d], trailer_list[id]) for d in timecourses_dictionary_list_current_trailer] \n",
    "        \n",
    "            values_Bilateral_ROI = [values[d][bROI] for d in range(len(values))]\n",
    "            values_Right_ROI = [values[d][rROI] for d in range(len(values))]\n",
    "            values_Left_ROI = [values[d][lROI] for d in range(len(values))]\n",
    "\n",
    "            # Compute the average for each roi.\n",
    "            averageBilateral_ROI = np.mean(values_Bilateral_ROI, axis=0)\n",
    "            averageRight_ROI = np.mean(values_Right_ROI, axis=0)\n",
    "            averageLeft_ROI = np.mean(values_Left_ROI, axis=0)\n",
    "\n",
    "            # Row are timepoints, columns are participants (15 x P).\n",
    "            values_Bilateral_ROI_array = np.array(values_Bilateral_ROI).T \n",
    "            values_Right_ROI_array = np.array(values_Right_ROI).T\n",
    "            values_Left_ROI_array = np.array(values_Left_ROI).T\n",
    "\n",
    "            # Create columns names for the dataframe.\n",
    "            TRs = np.arange(0, 15, 1)\n",
    "\n",
    "            # Creat empty dataframe for current trailer. \n",
    "            trailer_df = pd.DataFrame(columns=[\"Participant\", \"ROI\", \"Signal\", \"TR\"])   \n",
    "\n",
    "            current_participant_dic = {}\n",
    "\n",
    "            for current_participant in range(len(participants_with_trailer)):\n",
    "\n",
    "                # Create dictionary with all the timecourses for all trailers for each subject.\n",
    "                current_participant_dic[participants_with_trailer[current_participant]] = {bROI: values_Bilateral_ROI_array[:, current_participant],\n",
    "                                                                                       lROI: values_Left_ROI_array[:, current_participant], \n",
    "                                                                                       rROI: values_Right_ROI_array[:, current_participant]}\n",
    " \n",
    "                participant_col = np.repeat(str(participants_with_trailer[current_participant]), 15)\n",
    "                label_bi = np.repeat(bROI, 15)\n",
    "                label_left = np.repeat(lROI, 15)\n",
    "                label_right = np.repeat(rROI, 15)\n",
    "                roi_labels = [label_bi, label_left, label_right]\n",
    "\n",
    "                # Store current participant values. \n",
    "                current_participant_bi = values_Bilateral_ROI_array[:, current_participant]\n",
    "                current_participant_left = values_Left_ROI_array[:, current_participant]\n",
    "                current_participant_right = values_Right_ROI_array[:, current_participant]\n",
    "                current_participant_values = [current_participant_bi, current_participant_left, current_participant_right]\n",
    "\n",
    "                for x in range(3):\n",
    "\n",
    "                    current_participant_roi_data = {\n",
    "                        \"Participant\": participant_col,\n",
    "                        \"ROI\": roi_labels[x],\n",
    "                        \"Signal\": current_participant_values[x],\n",
    "                        \"TR\": TRs\n",
    "                    }\n",
    "            \n",
    "                    participant_df = pd.DataFrame(data=current_participant_roi_data)\n",
    "\n",
    "                    trailer_df = pd.concat([trailer_df, participant_df], ignore_index=True)\n",
    "        \n",
    "            # store the all in the results dictionaries.\n",
    "            all_subjects_avg_ROI_timecourse[trailer_list[id]] = {bROI: averageBilateral_ROI, lROI: averageLeft_ROI, rROI: averageRight_ROI}\n",
    "            all_subjects_ROI_timecourse[trailer_list[id]] = current_participant_dic\n",
    "            all_subjects_ROI_timecourse_df[trailer_list[id]] = trailer_df\n",
    "    \n",
    "    # Get average timecourse for all subjects in the current ROI. \n",
    "    Bilateral_ROI_raw_r1_list = [access_timecourse(timecourses_dictionary_raw[sub_id], bROI_r1) for sub_id in subjects_list] # if timecourses_dictionary_raw[sub_id][bROI_r1] is not None\n",
    "    Bilateral_ROI_raw_r2_list = [access_timecourse(timecourses_dictionary_raw[sub_id], bROI_r2) for sub_id in subjects_list]\n",
    "    Right_ROI_raw_r1_list = [access_timecourse(timecourses_dictionary_raw[sub_id], rROI_r1) for sub_id in subjects_list]\n",
    "    Right_ROI_raw_r2_list = [access_timecourse(timecourses_dictionary_raw[sub_id], rROI_r2) for sub_id in subjects_list]\n",
    "    Left_ROI_raw_r1_list = [access_timecourse(timecourses_dictionary_raw[sub_id], lROI_r1) for sub_id in subjects_list]\n",
    "    Left_ROI_raw_r2_list = [access_timecourse(timecourses_dictionary_raw[sub_id], lROI_r2) for sub_id in subjects_list]    \n",
    "\n",
    "    # Remove None values from the list.\n",
    "    Bilateral_ROI_raw_r1_list = [x for x in Bilateral_ROI_raw_r1_list if x is not None]\n",
    "    Bilateral_ROI_raw_r2_list = [x for x in Bilateral_ROI_raw_r2_list if x is not None]\n",
    "    Right_ROI_raw_r1_list = [x for x in Right_ROI_raw_r1_list if x is not None]\n",
    "    Right_ROI_raw_r2_list = [x for x in Right_ROI_raw_r2_list if x is not None]\n",
    "    Left_ROI_raw_r1_list = [x for x in Left_ROI_raw_r1_list if x is not None]\n",
    "    Left_ROI_raw_r2_list = [x for x in Left_ROI_raw_r2_list if x is not None]\n",
    "\n",
    "    # Get the average timecourse for all subjects in the current ROI.\n",
    "    Bilateral_ROI_raw_r1_average = np.mean(Bilateral_ROI_raw_r1_list, axis=0)\n",
    "    Bilateral_ROI_raw_r2_average = np.mean(Bilateral_ROI_raw_r2_list, axis=0)\n",
    "    Right_ROI_raw_r1_average = np.mean(Right_ROI_raw_r1_list, axis=0)\n",
    "    Right_ROI_raw_r2_average = np.mean(Right_ROI_raw_r2_list, axis=0)\n",
    "    Left_ROI_raw_r1_average = np.mean(Left_ROI_raw_r1_list, axis=0)\n",
    "    Left_ROI_raw_r2_average = np.mean(Left_ROI_raw_r2_list, axis=0)\n",
    "\n",
    "    # Create TR array.\n",
    "    TRs_raw = np.arange(0, 378, 1)\n",
    "\n",
    "    # Put everything in a list \n",
    "    ROI_raw_list = [Bilateral_ROI_raw_r1_average, Bilateral_ROI_raw_r2_average, Right_ROI_raw_r1_average, Right_ROI_raw_r2_average, Left_ROI_raw_r1_average, Left_ROI_raw_r2_average]\n",
    "    ROI_raw_labels = [bROI_r1, bROI_r2, rROI_r1, rROI_r2, lROI_r1, lROI_r2]\n",
    "\n",
    "    # Creat empty dataframe. \n",
    "    all_subjects_avg_ROI_timecourse_raw_df = pd.DataFrame(columns=[\"ROI\", \"Signal\", \"TR\"])  \n",
    "\n",
    "    for ROI_id in range(6):\n",
    "\n",
    "        current_roi_data = {        \n",
    "            \"ROI\": ROI_raw_labels[ROI_id],\n",
    "            \"Signal\": ROI_raw_list[ROI_id],\n",
    "            \"TR\": TRs_raw}\n",
    "            \n",
    "        current_roi_df = pd.DataFrame(data=current_roi_data)\n",
    "\n",
    "        all_subjects_avg_ROI_timecourse_raw_df = pd.concat([all_subjects_avg_ROI_timecourse_raw_df, current_roi_df], ignore_index=True)\n",
    "\n",
    "\n",
    "    return all_subjects_avg_ROI_timecourse, all_subjects_ROI_timecourse, all_subjects_ROI_timecourse_df, all_subjects_avg_ROI_timecourse_raw_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working dir : /Users/luisalvarez/Documents/Datasets\n"
     ]
    }
   ],
   "source": [
    "# Open a datasets directory. \n",
    "fd = os.open(\"/Users/luisalvarez/Documents/Datasets\", os.O_RDONLY)\n",
    "\n",
    "# Use os.fchdir() method to change the current dir/folder.\n",
    "os.fchdir(fd)\n",
    "\n",
    "# Safe check- Print current working directory\n",
    "print(\"Current working dir : %s\" % os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getROIs_timecourse(participant_num, type, sub_motion): \n",
    "\n",
    "    # Check if the type is correct.\n",
    "    if (type == \"psc\") or (type == \"zscore\"):\n",
    "        print(\"Getting ROI timecourse for participant: \" + participant_num + \" and type: \" + type)\n",
    "    else:\n",
    "        raise ValueError(\"Type not recognized. Please use 'psc' or 'zscore'.\")\n",
    "\n",
    "    # Add code to flag if something that is not a number is passed. \n",
    "\n",
    "    ## 1) Load data.\n",
    "    # Load relevant files for participant\n",
    "    sub_run1_func_path = \"MovieData_BIDS_preproc/sub-\" + participant_num + \"/func/sub-\" + participant_num + \"_task-movie_run-01_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz\"\n",
    "    sub_run2_func_path = \"MovieData_BIDS_preproc/sub-\" + participant_num + \"/func/sub-\" + participant_num + \"_task-movie_run-02_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz\"\n",
    "\n",
    "    sub_run1_mask_path = \"MovieData_BIDS_preproc/sub-\" + participant_num + \"/func/sub-\" + participant_num + \"_task-movie_run-01_space-MNI152NLin2009cAsym_desc-brain_mask.nii.gz\"\n",
    "    sub_run2_mask_path = \"MovieData_BIDS_preproc/sub-\" + participant_num + \"/func/sub-\" + participant_num + \"_task-movie_run-02_space-MNI152NLin2009cAsym_desc-brain_mask.nii.gz\"\n",
    "\n",
    "    sub_run1_events_path = \"MovieData_BIDS_raw/sub-\" + participant_num + \"/func/sub-\" + participant_num + \"_task-movie_run-01_events.csv\"\n",
    "    sub_run2_events_path = \"MovieData_BIDS_raw/sub-\" + participant_num + \"/func/sub-\" + participant_num + \"_task-movie_run-02_events.csv\"\n",
    "\n",
    "    sub_run1_confounds_path = \"MovieData_BIDS_preproc/sub-\" + participant_num + \"/func/sub-\" + participant_num + \"_task-movie_run-01_desc-confounds_timeseries.tsv\"\n",
    "    sub_run2_confounds_path = \"MovieData_BIDS_preproc/sub-\" + participant_num + \"/func/sub-\" + participant_num + \"_task-movie_run-02_desc-confounds_timeseries.tsv\"\n",
    "\n",
    "    # Calculate relevant parameters for GLM and ROI time-course analysis.\n",
    "    func_file = nib.load(sub_run1_func_path)\n",
    "    func_data = func_file.get_fdata()\n",
    "    n_vols = func_data.shape[3]\n",
    "    TR = 2\n",
    "    n_timepoints = n_vols*TR\n",
    "\n",
    "    # Load raw events files. \n",
    "    sub_run1_events_df = pd.read_csv(sub_run1_events_path, index_col=0)\n",
    "    sub_run2_events_df = pd.read_csv(sub_run2_events_path, index_col=0)\n",
    "\n",
    "    ## 2) Process files. \n",
    "    # Process event files. \n",
    "    sub_run1_p_events = process_events_data(sub_run1_events_df)\n",
    "    sub_run2_p_events = process_events_data(sub_run2_events_df)\n",
    "\n",
    "    # Down-sample time onsets to get vol onsets. \n",
    "    # Create array from 0 to 'n_timepoints' in steps of 1.\n",
    "    time_scale = np.arange(0, n_timepoints, 1)  \n",
    "\n",
    "    # Create array from 0 to 'n_timepoints' in steps of 2.\n",
    "    vol_scale = np.arange(0, n_timepoints, TR)  \n",
    "\n",
    "    # Get the labels of each trailer for each run. \n",
    "    run1_trailer_labels = sub_run1_p_events[\"Trailer\"].tolist()\n",
    "    run2_trailer_labels = sub_run2_p_events[\"Trailer\"].tolist()\n",
    "\n",
    "    # Create dictionary variable to store arrays with onset values for each trailer. \n",
    "    run1_onsets = {}\n",
    "    run2_onsets = {}\n",
    "\n",
    "    # Create a dictionary with all the onsets for each trailer in each run. \n",
    "    for id in range(len(run1_trailer_labels)):\n",
    "\n",
    "        # Create array of zeros.\n",
    "        run1_trailer_onsets = np.zeros(n_timepoints)\n",
    "        run2_trailer_onsets = np.zeros(n_timepoints)\n",
    "\n",
    "        # Get onset time. \n",
    "        run1_current_trailer_onset = sub_run1_p_events[\"Onset\"][id]\n",
    "        run2_current_trailer_onset = sub_run2_p_events[\"Onset\"][id]\n",
    "\n",
    "        # Assign 1 to such onset all the way til the end of the trailer (30 sec) in the array of zeros.\n",
    "        # Adjust for lag: add 4 seconds at the onset and offset\n",
    "        # Let's add 4 seconds to the onset and offset to account for the lag in the BOLD signal.\n",
    "        #run1_trailer_onsets[int(run1_current_trailer_onset + 4):int(run1_current_trailer_onset)+ 30 + 4] = 1\n",
    "        #run2_trailer_onsets[int(run2_current_trailer_onset + 4):int(run2_current_trailer_onset)+ 30 + 4] = 1\n",
    "        run1_trailer_onsets[int(run1_current_trailer_onset + 5):int(run1_current_trailer_onset)+ 30 + 5] = 1\n",
    "        run2_trailer_onsets[int(run2_current_trailer_onset + 5):int(run2_current_trailer_onset)+ 30 + 5] = 1\n",
    "\n",
    "        # Create resampler objects for each trailer/run of reward.\n",
    "        run1_resampler = interp1d(time_scale, run1_trailer_onsets)\n",
    "        run2_resampler = interp1d(time_scale, run2_trailer_onsets)\n",
    "\n",
    "        # Create downsampled arrays for each trailer. \n",
    "        # Note this vol arrays are half the length than the time arrays.\n",
    "        run1_trailer_vol_onsets = run1_resampler(vol_scale)\n",
    "        run2_trailer_vol_onsets = run2_resampler(vol_scale)\n",
    "\n",
    "        # Append/store the downsampled volumes arrays to each dictionary.\n",
    "        # I'm doing it this way, so the code is more interpretable\n",
    "        run1_onsets[run1_trailer_labels[id]] = run1_trailer_vol_onsets\n",
    "        run2_onsets[run2_trailer_labels[id]] = run2_trailer_vol_onsets\n",
    "\n",
    "    ## 3) Load confound data. \n",
    "    sub_run1_confounds_df = pd.read_csv(sub_run1_confounds_path, sep='\\t')\n",
    "    sub_run2_confounds_df = pd.read_csv(sub_run2_confounds_path, sep='\\t')\n",
    "    default_confounds = [\"white_matter\", \"csf\", \"csf_wm\", \"framewise_displacement\", \"dvars\", \"rmsd\", \"tcompcor\"] \n",
    "    # [\"white_matter\", \"csf\", \"csf_wm\", \"framewise_displacement\",  \"rmsd\", \"tcompcor\"]\n",
    "    \n",
    "    cohen_confounds = ['c_comp_cor_00','c_comp_cor_01','c_comp_cor_02','w_comp_cor_00','w_comp_cor_01','w_comp_cor_02',\n",
    "                       'w_comp_cor_03','w_comp_cor_04','trans_x','trans_y','trans_z','rot_x','rot_y','rot_z','trans_x_derivative1'\n",
    "                       ,'trans_y_derivative1','trans_z_derivative1','rot_x_derivative1','rot_y_derivative1','rot_z_derivative1',\n",
    "                       'trans_x_power2','trans_y_power2','trans_z_power2','rot_x_power2','rot_y_power2','rot_z_power2',\n",
    "                       'trans_x_derivative1_power2','trans_y_derivative1_power2','trans_z_derivative1_power2','rot_x_derivative1_power2',\n",
    "                       'rot_y_derivative1_power2','rot_z_derivative1_power2','cosine00'] #, 'c_comp_cor_03', 'c_comp_cor_04'\n",
    "\n",
    "    sub_run1_motion_s_confounds = [i for i in sub_run1_confounds_df.columns if \"state\" in i] #\"motion\"\n",
    "    sub_run2_motion_s_confounds = [i for i in sub_run2_confounds_df.columns if \"state\" in i] \n",
    "\n",
    "    sub_run1_motion_confounds = [i for i in sub_run1_confounds_df.columns if \"motion\" in i] #\"motion\"\n",
    "    sub_run2_motion_confounds = [i for i in sub_run2_confounds_df.columns if \"motion\" in i] \n",
    "\n",
    "    sub_run1_motion_rot_confounds = [i for i in sub_run1_confounds_df.columns if \"rot\" in i] \n",
    "    sub_run2_motion_rot_confounds = [i for i in sub_run2_confounds_df.columns if \"rot\" in i] \n",
    "\n",
    "    sub_run1_motion_trans_confounds = [i for i in sub_run1_confounds_df.columns if \"trans\" in i] \n",
    "    sub_run2_motion_trans_confounds = [i for i in sub_run2_confounds_df.columns if \"trans\" in i] \n",
    "\n",
    "    sub_run1_filtered_confounds_df = sub_run1_confounds_df[default_confounds + sub_run1_motion_s_confounds + sub_run1_motion_rot_confounds + sub_run1_motion_trans_confounds]\n",
    "    sub_run2_filtered_confounds_df = sub_run2_confounds_df[default_confounds + sub_run2_motion_s_confounds + sub_run2_motion_rot_confounds + sub_run2_motion_trans_confounds]\n",
    "\n",
    "    sub_run1_cohen_confounds_df = sub_run1_confounds_df[cohen_confounds ] #+ sub_run1_motion_s_confounds\n",
    "    sub_run2_cohen_confounds_df = sub_run2_confounds_df[cohen_confounds ] #+ sub_run2_motion_s_confounds\n",
    "\n",
    "    # Change NaNs to 0s. \n",
    "    sub_run1_filtered_confounds_df = sub_run1_filtered_confounds_df.fillna(0) \n",
    "    sub_run2_filtered_confounds_df = sub_run2_filtered_confounds_df.fillna(0) \n",
    "\n",
    "    sub_run1_cohen_confounds_df = sub_run1_cohen_confounds_df.fillna(0) \n",
    "    sub_run2_cohen_confounds_df = sub_run2_cohen_confounds_df.fillna(0) \n",
    "\n",
    "    # Specify the parameters to apply to the given analysis.\n",
    "    if(type==\"zscore\"):\n",
    "        detrend = True\n",
    "        standardize = type\n",
    "        standardize_confounds = False\n",
    "        confounds1 = sub_run1_cohen_confounds_df\n",
    "        confounds2 = sub_run2_cohen_confounds_df\n",
    "        smoothing=8\n",
    "        set_of_seeds = [(10, 12, -2), (-10, 12, -2), # Nucleus accumbes (NAcc) right, left\n",
    "                (34, 24, -4), (-34, 24, -4), # Anterior Insula right (AIns), left\n",
    "                (4, 45, 0), (-4, 45, 0)] # Medial Prefrontal Cortex (MPFC) right, left\n",
    "\n",
    "    # Let's try to detrend and not standarize confounds for PSC ... \n",
    "    elif(type==\"psc\"):\n",
    "        detrend = False #True # False\n",
    "        standardize = type\n",
    "        standardize_confounds = type #False #type\n",
    "        confounds1 = sub_run1_filtered_confounds_df\n",
    "        confounds2 = sub_run2_filtered_confounds_df\n",
    "        smoothing= 6 # from 4 to 6\n",
    "        set_of_seeds = [(10, 12, -2), (-10, 12, -2), # Nucleus accumbes (NAcc) right, left\n",
    "                (34, 24, -4), (-34, 24, -4), # Anterior Insula right (AIns), left\n",
    "                (4, 45, 0), (-4, 45, 0), # Medial Prefrontal Cortex (MPFC) right, left\n",
    "                (8, -76, 10), (-8, -76, 10)] # Visual cortex right, left\n",
    "\n",
    "    # Apply TPJ. \n",
    "    ## 4) Apply mask to func data. \n",
    "    masker_AIM_ROI_r1 = NiftiSpheresMasker(\n",
    "        seeds=set_of_seeds,\n",
    "        allow_overlap=True,\n",
    "        smoothing_fwhm=smoothing, # Applying a Gaussian filter with a 4mm kernel\n",
    "        detrend=detrend,\n",
    "        radius=6, \n",
    "        mask_img=sub_run1_mask_path,\n",
    "        standardize=standardize, \n",
    "        t_r=2,\n",
    "        standardize_confounds=standardize_confounds,\n",
    "        high_pass=1/360, # High cutoff frequency in Hertz.\n",
    "        #low_pass=0.1 # from 1.0\n",
    "        )\n",
    "\n",
    "    masker_AIM_ROI_r2 = NiftiSpheresMasker(\n",
    "        seeds=set_of_seeds,\n",
    "        allow_overlap=True,\n",
    "        smoothing_fwhm=smoothing, # Applying a Gaussian filter with a 4mm kernel\n",
    "        detrend=detrend,\n",
    "        radius=6, \n",
    "        mask_img=sub_run2_mask_path,\n",
    "        standardize=standardize, \n",
    "        t_r=2,\n",
    "        standardize_confounds=standardize_confounds,\n",
    "        high_pass=1/360, # High cutoff frequency in Hertz.\n",
    "        #low_pass=0.1 # from 1.0\n",
    "        )\n",
    "\n",
    "    ROI_raw_timecourses = {}\n",
    "    # Mask the func data and get a time series for the ROI. \n",
    "    # Note this is similar to fitting the GLM, but without the event files.\n",
    "    ## Add if statement to only apply the masker if the run has acceptable motion parameters. \n",
    "\n",
    "    if((sub_motion[0] < 1) and (sub_motion[1] < 1)):\n",
    "\n",
    "        # Apply function to get the percent signal change from each ROI timecourse. \n",
    "        sub_r1_AIM_ROI = masker_AIM_ROI_r1.fit_transform(sub_run1_func_path, confounds=confounds1)\n",
    "\n",
    "        ## 5) Get the timecourses from each movie trailer. \n",
    "        # Create dictionary variable to store arrays with time series arrays for each trailer.\n",
    "        run1_timeseries = {}\n",
    "\n",
    "        # Get the trailers presented in each run. \n",
    "        r1_keys = list(run1_onsets.keys())\n",
    "\n",
    "        # Loop through each traile and get its corresponding ROI timecourse\n",
    "        for id in range(len(r1_keys)):\n",
    "\n",
    "            run1_timeseries[r1_keys[id]] = {\n",
    "                \"Bilateral_NAcc\": np.mean(sub_r1_AIM_ROI[run1_onsets[r1_keys[id]].astype(bool)][:, 0:2], axis=1),\n",
    "                \"Left_NAcc\": sub_r1_AIM_ROI[run1_onsets[r1_keys[id]].astype(bool)][:, 1],\n",
    "                \"Right_NAcc\": sub_r1_AIM_ROI[run1_onsets[r1_keys[id]].astype(bool)][:, 0],\n",
    "                \"Bilateral_AIns\": np.mean(sub_r1_AIM_ROI[run1_onsets[r1_keys[id]].astype(bool)][:, 2:4], axis=1),\n",
    "                \"Left_AIns\": sub_r1_AIM_ROI[run1_onsets[r1_keys[id]].astype(bool)][:, 3],\n",
    "                \"Right_AIns\": sub_r1_AIM_ROI[run1_onsets[r1_keys[id]].astype(bool)][:, 2],\n",
    "                \"Bilateral_MPFC\": np.mean(sub_r1_AIM_ROI[run1_onsets[r1_keys[id]].astype(bool)][:, 4:6], axis=1),\n",
    "                \"Left_MPFC\": sub_r1_AIM_ROI[run1_onsets[r1_keys[id]].astype(bool)][:, 5],\n",
    "                \"Right_MPFC\": sub_r1_AIM_ROI[run1_onsets[r1_keys[id]].astype(bool)][:, 4]}\n",
    "        \n",
    "        # This code gives me the un-trimmed timecourses for each ROI. \n",
    "        if(type==\"psc\"):\n",
    "            ROI_raw_timecourses[\"Bilateral_NAcc_r1\"] = np.mean(sub_r1_AIM_ROI[:, 0:2], axis=1)\n",
    "            ROI_raw_timecourses[\"Left_NAcc_r1\"] = sub_r1_AIM_ROI[:, 1]\n",
    "            ROI_raw_timecourses[\"Right_NAcc_r1\"] = sub_r1_AIM_ROI[:, 0]\n",
    "            ROI_raw_timecourses[\"Bilateral_AIns_r1\"] = np.mean(sub_r1_AIM_ROI[:, 2:4], axis=1)\n",
    "            ROI_raw_timecourses[\"Left_AIns_r1\"] = sub_r1_AIM_ROI[:, 3]\n",
    "            ROI_raw_timecourses[\"Right_AIns_r1\"] = sub_r1_AIM_ROI[:, 2]\n",
    "            ROI_raw_timecourses[\"Bilateral_MPFC_r1\"] = np.mean(sub_r1_AIM_ROI[:, 4:6], axis=1)\n",
    "            ROI_raw_timecourses[\"Left_MPFC_r1\"] = sub_r1_AIM_ROI[:, 5]\n",
    "            ROI_raw_timecourses[\"Right_MPFC_r1\"] = sub_r1_AIM_ROI[:, 4]\n",
    "            ROI_raw_timecourses[\"Bilateral_V1_r1\"] = np.mean(sub_r1_AIM_ROI[:, 6:8], axis=1)\n",
    "            ROI_raw_timecourses[\"Left_V1_r1\"] = sub_r1_AIM_ROI[:, 6]\n",
    "            ROI_raw_timecourses[\"Right_V1_r1\"] = sub_r1_AIM_ROI[:, 7]\n",
    "        else:\n",
    "            ROI_raw_timecourses[\"Bilateral_NAcc_r1\"] = np.mean(sub_r1_AIM_ROI[:, 0:2], axis=1)\n",
    "            ROI_raw_timecourses[\"Left_NAcc_r1\"] = sub_r1_AIM_ROI[:, 1]\n",
    "            ROI_raw_timecourses[\"Right_NAcc_r1\"] = sub_r1_AIM_ROI[:, 0]\n",
    "            ROI_raw_timecourses[\"Bilateral_AIns_r1\"] = np.mean(sub_r1_AIM_ROI[:, 2:4], axis=1)\n",
    "            ROI_raw_timecourses[\"Left_AIns_r1\"] = sub_r1_AIM_ROI[:, 3]\n",
    "            ROI_raw_timecourses[\"Right_AIns_r1\"] = sub_r1_AIM_ROI[:, 2]\n",
    "            ROI_raw_timecourses[\"Bilateral_MPFC_r1\"] = np.mean(sub_r1_AIM_ROI[:, 4:6], axis=1)\n",
    "            ROI_raw_timecourses[\"Left_MPFC_r1\"] = sub_r1_AIM_ROI[:, 5]\n",
    "            ROI_raw_timecourses[\"Right_MPFC_r1\"] = sub_r1_AIM_ROI[:, 4]\n",
    "\n",
    "\n",
    "\n",
    "    # Apply for second run.\n",
    "    if((sub_motion[2] < 1) and (sub_motion[3] < 1)):\n",
    "\n",
    "        # Apply function to get the percent signal change from each ROI timecourse.\n",
    "        sub_r2_AIM_ROI = masker_AIM_ROI_r2.fit_transform(sub_run2_func_path, confounds=confounds2)\n",
    "\n",
    "        # Create dictionary variable to store arrays with time series arrays for each trailer.\n",
    "        run2_timeseries = {}\n",
    "\n",
    "        r2_keys = list(run2_onsets.keys())\n",
    "\n",
    "        # Loop through each traile and get its corresponding ROI timecourse\n",
    "        for id in range(len(r2_keys)):\n",
    "            run2_timeseries[r2_keys[id]] = {\n",
    "                \"Bilateral_NAcc\": np.mean(sub_r2_AIM_ROI[run2_onsets[r2_keys[id]].astype(bool)][:, 0:2], axis=1),\n",
    "                \"Left_NAcc\": sub_r2_AIM_ROI[run2_onsets[r2_keys[id]].astype(bool)][:, 1],\n",
    "                \"Right_NAcc\": sub_r2_AIM_ROI[run2_onsets[r2_keys[id]].astype(bool)][:, 0],\n",
    "                \"Bilateral_AIns\": np.mean(sub_r2_AIM_ROI[run2_onsets[r2_keys[id]].astype(bool)][:, 2:4], axis=1),\n",
    "                \"Left_AIns\": sub_r2_AIM_ROI[run2_onsets[r2_keys[id]].astype(bool)][:, 3],\n",
    "                \"Right_AIns\": sub_r2_AIM_ROI[run2_onsets[r2_keys[id]].astype(bool)][:, 2],\n",
    "                \"Bilateral_MPFC\": np.mean(sub_r2_AIM_ROI[run2_onsets[r2_keys[id]].astype(bool)][:, 4:6], axis=1),\n",
    "                \"Left_MPFC\": sub_r2_AIM_ROI[run2_onsets[r2_keys[id]].astype(bool)][:, 5],\n",
    "                \"Right_MPFC\": sub_r2_AIM_ROI[run2_onsets[r2_keys[id]].astype(bool)][:, 4]}\n",
    "        \n",
    "        # This code gives me the un-trimmed timecourses for each ROI.\n",
    "        if(type==\"psc\"):\n",
    "            ROI_raw_timecourses[\"Bilateral_NAcc_r2\"] = np.mean(sub_r2_AIM_ROI[:, 0:2], axis=1)\n",
    "            ROI_raw_timecourses[\"Left_NAcc_r2\"] = sub_r2_AIM_ROI[:, 1]\n",
    "            ROI_raw_timecourses[\"Right_NAcc_r2\"] = sub_r2_AIM_ROI[:, 0]\n",
    "            ROI_raw_timecourses[\"Bilateral_AIns_r2\"] = np.mean(sub_r2_AIM_ROI[:, 2:4], axis=1)\n",
    "            ROI_raw_timecourses[\"Left_AIns_r2\"] = sub_r2_AIM_ROI[:, 3]\n",
    "            ROI_raw_timecourses[\"Right_AIns_r2\"] = sub_r2_AIM_ROI[:, 2]\n",
    "            ROI_raw_timecourses[\"Bilateral_MPFC_r2\"] = np.mean(sub_r2_AIM_ROI[:, 4:6], axis=1)\n",
    "            ROI_raw_timecourses[\"Left_MPFC_r2\"] = sub_r2_AIM_ROI[:, 5]\n",
    "            ROI_raw_timecourses[\"Right_MPFC_r2\"] = sub_r2_AIM_ROI[:, 4]\n",
    "            ROI_raw_timecourses[\"Bilateral_V1_r2\"] = np.mean(sub_r2_AIM_ROI[:, 6:8], axis=1)\n",
    "            ROI_raw_timecourses[\"Left_V1_r2\"] = sub_r2_AIM_ROI[:, 6]\n",
    "            ROI_raw_timecourses[\"Right_V1_r2\"] = sub_r2_AIM_ROI[:, 7]\n",
    "        else:\n",
    "            ROI_raw_timecourses[\"Bilateral_NAcc_r2\"] = np.mean(sub_r2_AIM_ROI[:, 0:2], axis=1)\n",
    "            ROI_raw_timecourses[\"Left_NAcc_r2\"] = sub_r2_AIM_ROI[:, 1]\n",
    "            ROI_raw_timecourses[\"Right_NAcc_r2\"] = sub_r2_AIM_ROI[:, 0]\n",
    "            ROI_raw_timecourses[\"Bilateral_AIns_r2\"] = np.mean(sub_r2_AIM_ROI[:, 2:4], axis=1)\n",
    "            ROI_raw_timecourses[\"Left_AIns_r2\"] = sub_r2_AIM_ROI[:, 3]\n",
    "            ROI_raw_timecourses[\"Right_AIns_r2\"] = sub_r2_AIM_ROI[:, 2]\n",
    "            ROI_raw_timecourses[\"Bilateral_MPFC_r2\"] = np.mean(sub_r2_AIM_ROI[:, 4:6], axis=1)\n",
    "            ROI_raw_timecourses[\"Left_MPFC_r2\"] = sub_r2_AIM_ROI[:, 5]\n",
    "            ROI_raw_timecourses[\"Right_MPFC_r2\"] = sub_r2_AIM_ROI[:, 4]\n",
    "\n",
    "    ## 6) Merge timecourses and events data. \n",
    "    if((sub_motion[0] < 1) and (sub_motion[2] < 1)): \n",
    "        all_timeseries = merge_dictionaries(run1_timeseries, run2_timeseries)   \n",
    "        sub_all_p_events = pd.concat([sub_run1_p_events, sub_run2_p_events], ignore_index=True)\n",
    "        print(\"Participant \" + participant_num + \" has acceptable data for both runs.\")\n",
    "    elif((sub_motion[0] < 1) and (sub_motion[2] > 1)): \n",
    "        all_timeseries = run1_timeseries\n",
    "        sub_all_p_events = sub_run1_p_events\n",
    "        print(\"Participant \" + participant_num + \" has acceptable data for run 1 only.\")\n",
    "    elif((sub_motion[0] > 1) and (sub_motion[2] < 1)):\n",
    "        all_timeseries = run2_timeseries\n",
    "        sub_all_p_events = sub_run2_p_events\n",
    "        print(\"Participant \" + participant_num + \" has acceptable data for run 2 only.\")\n",
    "    else:\n",
    "        print(\"Participant \" + participant_num + \" has unacceptable data for both runs.\")\n",
    "        all_timeseries = {}\n",
    "        sub_all_p_events = []\n",
    "        \n",
    "    return all_timeseries, sub_all_p_events, ROI_raw_timecourses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getROIs_timecourse_detrend(participant_num, type, sub_motion): \n",
    "\n",
    "    # Check if the type is correct.\n",
    "    if (type == \"psc\") or (type == \"zscore\"):\n",
    "        print(\"Getting ROI timecourse for participant: \" + participant_num + \" and type: \" + type)\n",
    "    else:\n",
    "        raise ValueError(\"Type not recognized. Please use 'psc' or 'zscore'.\")\n",
    "\n",
    "    # Add code to flag if something that is not a number is passed. \n",
    "\n",
    "    ## 1) Load data.\n",
    "    # Load relevant files for participant\n",
    "    sub_run1_func_path = \"MovieData_BIDS_preproc/sub-\" + participant_num + \"/func/sub-\" + participant_num + \"_task-movie_run-01_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz\"\n",
    "    sub_run2_func_path = \"MovieData_BIDS_preproc/sub-\" + participant_num + \"/func/sub-\" + participant_num + \"_task-movie_run-02_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz\"\n",
    "\n",
    "    sub_run1_mask_path = \"MovieData_BIDS_preproc/sub-\" + participant_num + \"/func/sub-\" + participant_num + \"_task-movie_run-01_space-MNI152NLin2009cAsym_desc-brain_mask.nii.gz\"\n",
    "    sub_run2_mask_path = \"MovieData_BIDS_preproc/sub-\" + participant_num + \"/func/sub-\" + participant_num + \"_task-movie_run-02_space-MNI152NLin2009cAsym_desc-brain_mask.nii.gz\"\n",
    "\n",
    "    sub_run1_events_path = \"MovieData_BIDS_raw/sub-\" + participant_num + \"/func/sub-\" + participant_num + \"_task-movie_run-01_events.csv\"\n",
    "    sub_run2_events_path = \"MovieData_BIDS_raw/sub-\" + participant_num + \"/func/sub-\" + participant_num + \"_task-movie_run-02_events.csv\"\n",
    "\n",
    "    sub_run1_confounds_path = \"MovieData_BIDS_preproc/sub-\" + participant_num + \"/func/sub-\" + participant_num + \"_task-movie_run-01_desc-confounds_timeseries.tsv\"\n",
    "    sub_run2_confounds_path = \"MovieData_BIDS_preproc/sub-\" + participant_num + \"/func/sub-\" + participant_num + \"_task-movie_run-02_desc-confounds_timeseries.tsv\"\n",
    "\n",
    "    # Calculate relevant parameters for GLM and ROI time-course analysis.\n",
    "    func_file = nib.load(sub_run1_func_path)\n",
    "    func_data = func_file.get_fdata()\n",
    "    n_vols = func_data.shape[3]\n",
    "    TR = 2\n",
    "    n_timepoints = n_vols*TR\n",
    "\n",
    "    # Load raw events files. \n",
    "    sub_run1_events_df = pd.read_csv(sub_run1_events_path, index_col=0)\n",
    "    sub_run2_events_df = pd.read_csv(sub_run2_events_path, index_col=0)\n",
    "\n",
    "    ## 2) Process files. \n",
    "    # Process event files. \n",
    "    sub_run1_p_events = process_events_data(sub_run1_events_df)\n",
    "    sub_run2_p_events = process_events_data(sub_run2_events_df)\n",
    "\n",
    "    # Down-sample time onsets to get vol onsets. \n",
    "    # Create array from 0 to 'n_timepoints' in steps of 1.\n",
    "    time_scale = np.arange(0, n_timepoints, 1)  \n",
    "\n",
    "    # Create array from 0 to 'n_timepoints' in steps of 2.\n",
    "    vol_scale = np.arange(0, n_timepoints, TR)  \n",
    "\n",
    "    # Get the labels of each trailer for each run. \n",
    "    run1_trailer_labels = sub_run1_p_events[\"Trailer\"].tolist()\n",
    "    run2_trailer_labels = sub_run2_p_events[\"Trailer\"].tolist()\n",
    "\n",
    "    # Create dictionary variable to store arrays with onset values for each trailer. \n",
    "    run1_onsets = {}\n",
    "    run2_onsets = {}\n",
    "\n",
    "    # Create a dictionary with all the onsets for each trailer in each run. \n",
    "    for id in range(len(run1_trailer_labels)):\n",
    "\n",
    "        # Create array of zeros.\n",
    "        run1_trailer_onsets = np.zeros(n_timepoints)\n",
    "        run2_trailer_onsets = np.zeros(n_timepoints)\n",
    "\n",
    "        # Get onset time. \n",
    "        run1_current_trailer_onset = sub_run1_p_events[\"Onset\"][id]\n",
    "        run2_current_trailer_onset = sub_run2_p_events[\"Onset\"][id]\n",
    "\n",
    "        # Assign 1 to such onset all the way til the end of the trailer (30 sec) in the array of zeros.\n",
    "        # Adjust for lag: add 4 seconds at the onset and offset\n",
    "        # Let's add 4 seconds to the onset and offset to account for the lag in the BOLD signal.\n",
    "        #run1_trailer_onsets[int(run1_current_trailer_onset + 4):int(run1_current_trailer_onset)+ 30 + 4] = 1\n",
    "        #run2_trailer_onsets[int(run2_current_trailer_onset + 4):int(run2_current_trailer_onset)+ 30 + 4] = 1\n",
    "        run1_trailer_onsets[int(run1_current_trailer_onset + 5):int(run1_current_trailer_onset)+ 30 + 5] = 1\n",
    "        run2_trailer_onsets[int(run2_current_trailer_onset + 5):int(run2_current_trailer_onset)+ 30 + 5] = 1\n",
    "\n",
    "        # Create resampler objects for each trailer/run of reward.\n",
    "        run1_resampler = interp1d(time_scale, run1_trailer_onsets)\n",
    "        run2_resampler = interp1d(time_scale, run2_trailer_onsets)\n",
    "\n",
    "        # Create downsampled arrays for each trailer. \n",
    "        # Note this vol arrays are half the length than the time arrays.\n",
    "        run1_trailer_vol_onsets = run1_resampler(vol_scale)\n",
    "        run2_trailer_vol_onsets = run2_resampler(vol_scale)\n",
    "\n",
    "        # Append/store the downsampled volumes arrays to each dictionary.\n",
    "        # I'm doing it this way, so the code is more interpretable\n",
    "        run1_onsets[run1_trailer_labels[id]] = run1_trailer_vol_onsets\n",
    "        run2_onsets[run2_trailer_labels[id]] = run2_trailer_vol_onsets\n",
    "\n",
    "    ## 3) Load confound data. \n",
    "    sub_run1_confounds_df = pd.read_csv(sub_run1_confounds_path, sep='\\t')\n",
    "    sub_run2_confounds_df = pd.read_csv(sub_run2_confounds_path, sep='\\t')\n",
    "    default_confounds = [\"white_matter\", \"csf\", \"csf_wm\", \"framewise_displacement\", \"dvars\", \"rmsd\", \"tcompcor\",\n",
    "                         'c_comp_cor_00','c_comp_cor_01', 'w_comp_cor_00','w_comp_cor_01', 'a_comp_cor_00','a_comp_cor_01']\n",
    "    # [\"white_matter\", \"csf\", \"csf_wm\", \"framewise_displacement\",  \"rmsd\", \"tcompcor\"]\n",
    "    \n",
    "    cohen_confounds = ['c_comp_cor_00','c_comp_cor_01','c_comp_cor_02','w_comp_cor_00','w_comp_cor_01','w_comp_cor_02',\n",
    "                       'w_comp_cor_03','w_comp_cor_04','trans_x','trans_y','trans_z','rot_x','rot_y','rot_z','trans_x_derivative1'\n",
    "                       ,'trans_y_derivative1','trans_z_derivative1','rot_x_derivative1','rot_y_derivative1','rot_z_derivative1',\n",
    "                       'trans_x_power2','trans_y_power2','trans_z_power2','rot_x_power2','rot_y_power2','rot_z_power2',\n",
    "                       'trans_x_derivative1_power2','trans_y_derivative1_power2','trans_z_derivative1_power2','rot_x_derivative1_power2',\n",
    "                       'rot_y_derivative1_power2','rot_z_derivative1_power2','cosine00'] #, 'c_comp_cor_03', 'c_comp_cor_04'\n",
    "\n",
    "    sub_run1_motion_s_confounds = [i for i in sub_run1_confounds_df.columns if \"state\" in i] #\"motion\"\n",
    "    sub_run2_motion_s_confounds = [i for i in sub_run2_confounds_df.columns if \"state\" in i] \n",
    "\n",
    "    sub_run1_motion_confounds = [i for i in sub_run1_confounds_df.columns if \"motion\" in i] #\"motion\"\n",
    "    sub_run2_motion_confounds = [i for i in sub_run2_confounds_df.columns if \"motion\" in i] \n",
    "\n",
    "    sub_run1_motion_rot_confounds = [i for i in sub_run1_confounds_df.columns if \"rot\" in i] \n",
    "    sub_run2_motion_rot_confounds = [i for i in sub_run2_confounds_df.columns if \"rot\" in i] \n",
    "\n",
    "    sub_run1_motion_trans_confounds = [i for i in sub_run1_confounds_df.columns if \"trans\" in i] \n",
    "    sub_run2_motion_trans_confounds = [i for i in sub_run2_confounds_df.columns if \"trans\" in i] \n",
    "\n",
    "    sub_run1_filtered_confounds_df = sub_run1_confounds_df[default_confounds + sub_run1_motion_s_confounds + sub_run1_motion_rot_confounds + sub_run1_motion_trans_confounds]\n",
    "    sub_run2_filtered_confounds_df = sub_run2_confounds_df[default_confounds + sub_run2_motion_s_confounds + sub_run2_motion_rot_confounds + sub_run2_motion_trans_confounds]\n",
    "\n",
    "    sub_run1_cohen_confounds_df = sub_run1_confounds_df[cohen_confounds ] #+ sub_run1_motion_s_confounds\n",
    "    sub_run2_cohen_confounds_df = sub_run2_confounds_df[cohen_confounds ] #+ sub_run2_motion_s_confounds\n",
    "\n",
    "    # Change NaNs to 0s. \n",
    "    sub_run1_filtered_confounds_df = sub_run1_filtered_confounds_df.fillna(0) \n",
    "    sub_run2_filtered_confounds_df = sub_run2_filtered_confounds_df.fillna(0) \n",
    "\n",
    "    sub_run1_cohen_confounds_df = sub_run1_cohen_confounds_df.fillna(0) \n",
    "    sub_run2_cohen_confounds_df = sub_run2_cohen_confounds_df.fillna(0) \n",
    "\n",
    "    # Specify the parameters to apply to the given analysis.\n",
    "    if(type==\"zscore\"):\n",
    "        detrend = True\n",
    "        standardize = type\n",
    "        standardize_confounds = False\n",
    "        confounds1 = sub_run1_cohen_confounds_df\n",
    "        confounds2 = sub_run2_cohen_confounds_df\n",
    "        smoothing=8\n",
    "        set_of_seeds = [(10, 12, -2), (-10, 12, -2), # Nucleus accumbes (NAcc) right, left\n",
    "                (34, 24, -4), (-34, 24, -4), # Anterior Insula right (AIns), left\n",
    "                (4, 45, 0), (-4, 45, 0)] # Medial Prefrontal Cortex (MPFC) right, left\n",
    "\n",
    "    # Let's try to detrend and not standarize confounds for PSC ... \n",
    "    elif(type==\"psc\"):\n",
    "        detrend = True #True # False\n",
    "        standardize = type\n",
    "        standardize_confounds = type #False #type\n",
    "        confounds1 = sub_run1_filtered_confounds_df\n",
    "        confounds2 = sub_run2_filtered_confounds_df\n",
    "        smoothing= 6 # from 4 to 6\n",
    "        set_of_seeds = [(10, 12, -2), (-10, 12, -2), # Nucleus accumbes (NAcc) right, left\n",
    "                (34, 24, -4), (-34, 24, -4), # Anterior Insula right (AIns), left\n",
    "                (4, 45, 0), (-4, 45, 0), # Medial Prefrontal Cortex (MPFC) right, left\n",
    "                (8, -76, 10), (-8, -76, 10)] # Visual cortex right, left\n",
    "\n",
    "    # Apply TPJ. \n",
    "    ## 4) Apply mask to func data. \n",
    "    masker_AIM_ROI_r1 = NiftiSpheresMasker(\n",
    "        seeds=set_of_seeds,\n",
    "        allow_overlap=True,\n",
    "        smoothing_fwhm=smoothing, # Applying a Gaussian filter with a 4mm kernel\n",
    "        detrend=detrend,\n",
    "        radius=6, \n",
    "        mask_img=sub_run1_mask_path,\n",
    "        standardize=standardize, \n",
    "        t_r=2,\n",
    "        standardize_confounds=standardize_confounds,\n",
    "        high_pass=1/360, # High cutoff frequency in Hertz.\n",
    "        #low_pass=0.1 # from 1.0\n",
    "        )\n",
    "\n",
    "    masker_AIM_ROI_r2 = NiftiSpheresMasker(\n",
    "        seeds=set_of_seeds,\n",
    "        allow_overlap=True,\n",
    "        smoothing_fwhm=smoothing, # Applying a Gaussian filter with a 4mm kernel\n",
    "        detrend=detrend,\n",
    "        radius=6, \n",
    "        mask_img=sub_run2_mask_path,\n",
    "        standardize=standardize, \n",
    "        t_r=2,\n",
    "        standardize_confounds=standardize_confounds,\n",
    "        high_pass=1/360, # High cutoff frequency in Hertz.\n",
    "        #low_pass=0.1 # from 1.0\n",
    "        )\n",
    "\n",
    "    ROI_raw_timecourses = {}\n",
    "    # Mask the func data and get a time series for the ROI. \n",
    "    # Note this is similar to fitting the GLM, but without the event files.\n",
    "    ## Add if statement to only apply the masker if the run has acceptable motion parameters. \n",
    "\n",
    "    if((sub_motion[0] < 1) and (sub_motion[1] < 1)):\n",
    "\n",
    "        # Apply function to get the percent signal change from each ROI timecourse. \n",
    "        sub_r1_AIM_ROI = masker_AIM_ROI_r1.fit_transform(sub_run1_func_path, confounds=confounds1)\n",
    "\n",
    "        ## 5) Get the timecourses from each movie trailer. \n",
    "        # Create dictionary variable to store arrays with time series arrays for each trailer.\n",
    "        run1_timeseries = {}\n",
    "\n",
    "        # Get the trailers presented in each run. \n",
    "        r1_keys = list(run1_onsets.keys())\n",
    "\n",
    "        # Loop through each traile and get its corresponding ROI timecourse\n",
    "        for id in range(len(r1_keys)):\n",
    "\n",
    "            run1_timeseries[r1_keys[id]] = {\n",
    "                \"Bilateral_NAcc\": np.mean(sub_r1_AIM_ROI[run1_onsets[r1_keys[id]].astype(bool)][:, 0:2], axis=1),\n",
    "                \"Left_NAcc\": sub_r1_AIM_ROI[run1_onsets[r1_keys[id]].astype(bool)][:, 1],\n",
    "                \"Right_NAcc\": sub_r1_AIM_ROI[run1_onsets[r1_keys[id]].astype(bool)][:, 0],\n",
    "                \"Bilateral_AIns\": np.mean(sub_r1_AIM_ROI[run1_onsets[r1_keys[id]].astype(bool)][:, 2:4], axis=1),\n",
    "                \"Left_AIns\": sub_r1_AIM_ROI[run1_onsets[r1_keys[id]].astype(bool)][:, 3],\n",
    "                \"Right_AIns\": sub_r1_AIM_ROI[run1_onsets[r1_keys[id]].astype(bool)][:, 2],\n",
    "                \"Bilateral_MPFC\": np.mean(sub_r1_AIM_ROI[run1_onsets[r1_keys[id]].astype(bool)][:, 4:6], axis=1),\n",
    "                \"Left_MPFC\": sub_r1_AIM_ROI[run1_onsets[r1_keys[id]].astype(bool)][:, 5],\n",
    "                \"Right_MPFC\": sub_r1_AIM_ROI[run1_onsets[r1_keys[id]].astype(bool)][:, 4]}\n",
    "        \n",
    "        # This code gives me the un-trimmed timecourses for each ROI. \n",
    "        if(type==\"psc\"):\n",
    "            ROI_raw_timecourses[\"Bilateral_NAcc_r1\"] = np.mean(sub_r1_AIM_ROI[:, 0:2], axis=1)\n",
    "            ROI_raw_timecourses[\"Left_NAcc_r1\"] = sub_r1_AIM_ROI[:, 1]\n",
    "            ROI_raw_timecourses[\"Right_NAcc_r1\"] = sub_r1_AIM_ROI[:, 0]\n",
    "            ROI_raw_timecourses[\"Bilateral_AIns_r1\"] = np.mean(sub_r1_AIM_ROI[:, 2:4], axis=1)\n",
    "            ROI_raw_timecourses[\"Left_AIns_r1\"] = sub_r1_AIM_ROI[:, 3]\n",
    "            ROI_raw_timecourses[\"Right_AIns_r1\"] = sub_r1_AIM_ROI[:, 2]\n",
    "            ROI_raw_timecourses[\"Bilateral_MPFC_r1\"] = np.mean(sub_r1_AIM_ROI[:, 4:6], axis=1)\n",
    "            ROI_raw_timecourses[\"Left_MPFC_r1\"] = sub_r1_AIM_ROI[:, 5]\n",
    "            ROI_raw_timecourses[\"Right_MPFC_r1\"] = sub_r1_AIM_ROI[:, 4]\n",
    "            ROI_raw_timecourses[\"Bilateral_V1_r1\"] = np.mean(sub_r1_AIM_ROI[:, 6:8], axis=1)\n",
    "            ROI_raw_timecourses[\"Left_V1_r1\"] = sub_r1_AIM_ROI[:, 6]\n",
    "            ROI_raw_timecourses[\"Right_V1_r1\"] = sub_r1_AIM_ROI[:, 7]\n",
    "        else:\n",
    "            ROI_raw_timecourses[\"Bilateral_NAcc_r1\"] = np.mean(sub_r1_AIM_ROI[:, 0:2], axis=1)\n",
    "            ROI_raw_timecourses[\"Left_NAcc_r1\"] = sub_r1_AIM_ROI[:, 1]\n",
    "            ROI_raw_timecourses[\"Right_NAcc_r1\"] = sub_r1_AIM_ROI[:, 0]\n",
    "            ROI_raw_timecourses[\"Bilateral_AIns_r1\"] = np.mean(sub_r1_AIM_ROI[:, 2:4], axis=1)\n",
    "            ROI_raw_timecourses[\"Left_AIns_r1\"] = sub_r1_AIM_ROI[:, 3]\n",
    "            ROI_raw_timecourses[\"Right_AIns_r1\"] = sub_r1_AIM_ROI[:, 2]\n",
    "            ROI_raw_timecourses[\"Bilateral_MPFC_r1\"] = np.mean(sub_r1_AIM_ROI[:, 4:6], axis=1)\n",
    "            ROI_raw_timecourses[\"Left_MPFC_r1\"] = sub_r1_AIM_ROI[:, 5]\n",
    "            ROI_raw_timecourses[\"Right_MPFC_r1\"] = sub_r1_AIM_ROI[:, 4]\n",
    "\n",
    "\n",
    "\n",
    "    # Apply for second run.\n",
    "    if((sub_motion[2] < 1) and (sub_motion[3] < 1)):\n",
    "\n",
    "        # Apply function to get the percent signal change from each ROI timecourse.\n",
    "        sub_r2_AIM_ROI = masker_AIM_ROI_r2.fit_transform(sub_run2_func_path, confounds=confounds2)\n",
    "\n",
    "        # Create dictionary variable to store arrays with time series arrays for each trailer.\n",
    "        run2_timeseries = {}\n",
    "\n",
    "        r2_keys = list(run2_onsets.keys())\n",
    "\n",
    "        # Loop through each traile and get its corresponding ROI timecourse\n",
    "        for id in range(len(r2_keys)):\n",
    "            run2_timeseries[r2_keys[id]] = {\n",
    "                \"Bilateral_NAcc\": np.mean(sub_r2_AIM_ROI[run2_onsets[r2_keys[id]].astype(bool)][:, 0:2], axis=1),\n",
    "                \"Left_NAcc\": sub_r2_AIM_ROI[run2_onsets[r2_keys[id]].astype(bool)][:, 1],\n",
    "                \"Right_NAcc\": sub_r2_AIM_ROI[run2_onsets[r2_keys[id]].astype(bool)][:, 0],\n",
    "                \"Bilateral_AIns\": np.mean(sub_r2_AIM_ROI[run2_onsets[r2_keys[id]].astype(bool)][:, 2:4], axis=1),\n",
    "                \"Left_AIns\": sub_r2_AIM_ROI[run2_onsets[r2_keys[id]].astype(bool)][:, 3],\n",
    "                \"Right_AIns\": sub_r2_AIM_ROI[run2_onsets[r2_keys[id]].astype(bool)][:, 2],\n",
    "                \"Bilateral_MPFC\": np.mean(sub_r2_AIM_ROI[run2_onsets[r2_keys[id]].astype(bool)][:, 4:6], axis=1),\n",
    "                \"Left_MPFC\": sub_r2_AIM_ROI[run2_onsets[r2_keys[id]].astype(bool)][:, 5],\n",
    "                \"Right_MPFC\": sub_r2_AIM_ROI[run2_onsets[r2_keys[id]].astype(bool)][:, 4]}\n",
    "        \n",
    "        # This code gives me the un-trimmed timecourses for each ROI.\n",
    "        if(type==\"psc\"):\n",
    "            ROI_raw_timecourses[\"Bilateral_NAcc_r2\"] = np.mean(sub_r2_AIM_ROI[:, 0:2], axis=1)\n",
    "            ROI_raw_timecourses[\"Left_NAcc_r2\"] = sub_r2_AIM_ROI[:, 1]\n",
    "            ROI_raw_timecourses[\"Right_NAcc_r2\"] = sub_r2_AIM_ROI[:, 0]\n",
    "            ROI_raw_timecourses[\"Bilateral_AIns_r2\"] = np.mean(sub_r2_AIM_ROI[:, 2:4], axis=1)\n",
    "            ROI_raw_timecourses[\"Left_AIns_r2\"] = sub_r2_AIM_ROI[:, 3]\n",
    "            ROI_raw_timecourses[\"Right_AIns_r2\"] = sub_r2_AIM_ROI[:, 2]\n",
    "            ROI_raw_timecourses[\"Bilateral_MPFC_r2\"] = np.mean(sub_r2_AIM_ROI[:, 4:6], axis=1)\n",
    "            ROI_raw_timecourses[\"Left_MPFC_r2\"] = sub_r2_AIM_ROI[:, 5]\n",
    "            ROI_raw_timecourses[\"Right_MPFC_r2\"] = sub_r2_AIM_ROI[:, 4]\n",
    "            ROI_raw_timecourses[\"Bilateral_V1_r2\"] = np.mean(sub_r2_AIM_ROI[:, 6:8], axis=1)\n",
    "            ROI_raw_timecourses[\"Left_V1_r2\"] = sub_r2_AIM_ROI[:, 6]\n",
    "            ROI_raw_timecourses[\"Right_V1_r2\"] = sub_r2_AIM_ROI[:, 7]\n",
    "        else:\n",
    "            ROI_raw_timecourses[\"Bilateral_NAcc_r2\"] = np.mean(sub_r2_AIM_ROI[:, 0:2], axis=1)\n",
    "            ROI_raw_timecourses[\"Left_NAcc_r2\"] = sub_r2_AIM_ROI[:, 1]\n",
    "            ROI_raw_timecourses[\"Right_NAcc_r2\"] = sub_r2_AIM_ROI[:, 0]\n",
    "            ROI_raw_timecourses[\"Bilateral_AIns_r2\"] = np.mean(sub_r2_AIM_ROI[:, 2:4], axis=1)\n",
    "            ROI_raw_timecourses[\"Left_AIns_r2\"] = sub_r2_AIM_ROI[:, 3]\n",
    "            ROI_raw_timecourses[\"Right_AIns_r2\"] = sub_r2_AIM_ROI[:, 2]\n",
    "            ROI_raw_timecourses[\"Bilateral_MPFC_r2\"] = np.mean(sub_r2_AIM_ROI[:, 4:6], axis=1)\n",
    "            ROI_raw_timecourses[\"Left_MPFC_r2\"] = sub_r2_AIM_ROI[:, 5]\n",
    "            ROI_raw_timecourses[\"Right_MPFC_r2\"] = sub_r2_AIM_ROI[:, 4]\n",
    "\n",
    "    ## 6) Merge timecourses and events data. \n",
    "    if((sub_motion[0] < 1) and (sub_motion[2] < 1)): \n",
    "        all_timeseries = merge_dictionaries(run1_timeseries, run2_timeseries)   \n",
    "        sub_all_p_events = pd.concat([sub_run1_p_events, sub_run2_p_events], ignore_index=True)\n",
    "        print(\"Participant \" + participant_num + \" has acceptable data for both runs.\")\n",
    "    elif((sub_motion[0] < 1) and (sub_motion[2] > 1)): \n",
    "        all_timeseries = run1_timeseries\n",
    "        sub_all_p_events = sub_run1_p_events\n",
    "        print(\"Participant \" + participant_num + \" has acceptable data for run 1 only.\")\n",
    "    elif((sub_motion[0] > 1) and (sub_motion[2] < 1)):\n",
    "        all_timeseries = run2_timeseries\n",
    "        sub_all_p_events = sub_run2_p_events\n",
    "        print(\"Participant \" + participant_num + \" has acceptable data for run 2 only.\")\n",
    "    else:\n",
    "        print(\"Participant \" + participant_num + \" has unacceptable data for both runs.\")\n",
    "        all_timeseries = {}\n",
    "        sub_all_p_events = []\n",
    "        \n",
    "    return all_timeseries, sub_all_p_events, ROI_raw_timecourses\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get neural activation time courses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define each subject motion parameters from fMRIprep.\n",
    "# The first value is the max framewise displacement, the second is the mean framewise displacement.\n",
    "# Two values per run. \n",
    "sub01_motion = [0.911, 0.095, 0.571, 0.077]\n",
    "sub02_motion = [0.663, 0.073, 0.734, 0.108]\n",
    "sub03_motion = [0.456, 0.048, 0.300, 0.049]\n",
    "sub04_motion = [0.607, 0.080, 0.519, 0.076]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting ROI timecourse for participant: 01 and type: psc\n",
      "Participant 01 has acceptable data for both runs.\n",
      "Getting ROI timecourse for participant: 02 and type: psc\n",
      "Participant 02 has acceptable data for both runs.\n",
      "Getting ROI timecourse for participant: 03 and type: psc\n",
      "Participant 03 has acceptable data for both runs.\n",
      "Getting ROI timecourse for participant: 04 and type: psc\n",
      "Participant 04 has acceptable data for both runs.\n",
      "Getting ROI timecourse for participant: 08 and type: psc\n",
      "Participant 08 has acceptable data for both runs.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Default set-up: \n",
    "    Confounds: \"white_matter\", \"csf\", \"csf_wm\", \"framewise_displacement\", \"dvars\", \"rmsd\", \"tcompcor\"\n",
    "    \"rotation_x\", \"rotation_y\", \"rotation_z\", \"translation_x\", \"translation_y\", \"translation_z\",  \"non-steady-stateX\".\n",
    "    Detrend: False\n",
    "    Hemodynamic response function lag: 4 seconds\n",
    "\"\"\" \n",
    "\n",
    "sub01_ROIs_timecourse, sub01_events_df, sub01_ROIs_timecourse_raw = getROIs_timecourse(\"01\", \"psc\", sub01_motion) # Max-Mean FD -> Run 1: (0.911 , 0.095) Run 2: (0.571, 0.077)\n",
    "sub02_ROIs_timecourse, sub02_events_df, sub02_ROIs_timecourse_raw = getROIs_timecourse(\"02\", \"psc\", sub02_motion) # Max-Mean FD -> Run 1: (0.663 , 0.073) Run 2: (0.734, 0.108) \n",
    "sub03_ROIs_timecourse, sub03_events_df, sub03_ROIs_timecourse_raw = getROIs_timecourse(\"03\", \"psc\", sub03_motion) # Max-Mean FD -> Run 1: (0.456 , 0.048) Run 2: (0.300, 0.049) \n",
    "sub04_ROIs_timecourse, sub04_events_df, sub04_ROIs_timecourse_raw = getROIs_timecourse(\"04\", \"psc\", sub04_motion) # Max-Mean FD -> Run 1: (0.607 , 0.080) Run 2: (0.519, 0.076)\n",
    "sub08_ROIs_timecourse, sub08_events_df, sub08_ROIs_timecourse_raw = getROIs_timecourse(\"08\", \"psc\", sub04_motion) # Max-Mean FD -> Run 1: (0.607 , 0.080) Run 2: (0.519, 0.076) \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get average timecourse for each ROI.\n",
    "all_bNAcc_r1_array = np.vstack((sub01_ROIs_timecourse_raw[\"Bilateral_NAcc_r1\"], sub02_ROIs_timecourse_raw[\"Bilateral_NAcc_r1\"], sub03_ROIs_timecourse_raw[\"Bilateral_NAcc_r1\"], sub04_ROIs_timecourse_raw[\"Bilateral_NAcc_r1\"],\n",
    "                                sub08_ROIs_timecourse_raw[\"Bilateral_NAcc_r1\"]))\n",
    "all_bNAcc_r1_avg = np.mean(all_bNAcc_r1_array, axis=0)\n",
    "\n",
    "all_bAIns_r1_array = np.vstack((sub01_ROIs_timecourse_raw[\"Bilateral_AIns_r1\"], sub02_ROIs_timecourse_raw[\"Bilateral_AIns_r1\"], sub03_ROIs_timecourse_raw[\"Bilateral_AIns_r1\"], sub04_ROIs_timecourse_raw[\"Bilateral_AIns_r1\"],\n",
    "                                sub08_ROIs_timecourse_raw[\"Bilateral_AIns_r1\"]))\n",
    "all_bAIns_r1_avg = np.mean(all_bAIns_r1_array, axis=0)\n",
    "\n",
    "all_bMPFC_r1_array = np.vstack((sub01_ROIs_timecourse_raw[\"Bilateral_MPFC_r1\"], sub02_ROIs_timecourse_raw[\"Bilateral_MPFC_r1\"], sub03_ROIs_timecourse_raw[\"Bilateral_MPFC_r1\"], sub04_ROIs_timecourse_raw[\"Bilateral_MPFC_r1\"],\n",
    "                                sub08_ROIs_timecourse_raw[\"Bilateral_MPFC_r1\"]))\n",
    "all_bMPFC_r1_avg = np.mean(all_bMPFC_r1_array, axis=0)\n",
    "\n",
    "all_bNAcc_r2_array = np.vstack((sub01_ROIs_timecourse_raw[\"Bilateral_NAcc_r2\"], sub02_ROIs_timecourse_raw[\"Bilateral_NAcc_r2\"], sub03_ROIs_timecourse_raw[\"Bilateral_NAcc_r2\"], sub04_ROIs_timecourse_raw[\"Bilateral_NAcc_r2\"],\n",
    "                                sub08_ROIs_timecourse_raw[\"Bilateral_NAcc_r2\"]))\n",
    "all_bNAcc_r2_avg = np.mean(all_bNAcc_r2_array, axis=0)\n",
    "\n",
    "all_bAIns_r2_array = np.vstack((sub01_ROIs_timecourse_raw[\"Bilateral_AIns_r2\"], sub02_ROIs_timecourse_raw[\"Bilateral_AIns_r2\"], sub03_ROIs_timecourse_raw[\"Bilateral_AIns_r2\"], sub04_ROIs_timecourse_raw[\"Bilateral_AIns_r2\"],\n",
    "                                sub08_ROIs_timecourse_raw[\"Bilateral_AIns_r2\"]))\n",
    "all_bAIns_r2_avg = np.mean(all_bAIns_r2_array, axis=0)\n",
    "\n",
    "all_bMPFC_r2_array = np.vstack((sub01_ROIs_timecourse_raw[\"Bilateral_MPFC_r2\"], sub02_ROIs_timecourse_raw[\"Bilateral_MPFC_r2\"], sub03_ROIs_timecourse_raw[\"Bilateral_MPFC_r2\"], sub04_ROIs_timecourse_raw[\"Bilateral_MPFC_r2\"],\n",
    "                                sub08_ROIs_timecourse_raw[\"Bilateral_MPFC_r2\"]))\n",
    "all_bMPFC_r2_avg = np.mean(all_bMPFC_r2_array, axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1 NAcc and MPFC have a correlation 0.2772310943366475 with a p-value of 4.255268758197753e-08\n",
      "\n",
      "Run 1 NAcc and AIns have a correlation 0.39414353026779597 with a p-value of 1.6892182517886263e-15\n",
      "\n",
      "Run 1 MPFC and AIns have a correlation 0.357633033760682 with a p-value of 7.576024020636746e-13\n"
     ]
    }
   ],
   "source": [
    "# Compute correlations between each roi.\n",
    "\n",
    "# NAcc & MPFC \n",
    "bNAcc_bMPFC_r1 = pearsonr(all_bNAcc_r1_avg, all_bMPFC_r1_avg)\n",
    "bNAcc_bMPFC_r2 = pearsonr(all_bNAcc_r2_avg, all_bMPFC_r2_avg)\n",
    "print(\"Run 1 NAcc and MPFC have a correlation \" + str(bNAcc_bMPFC_r1[0]) + \" with a p-value of \" + str(bNAcc_bMPFC_r1[1]))\n",
    "#print(\"Run 2 NAcc and MPFC have a correlation \" + str(bNAcc_bMPFC_r2[0]) + \" with a p-value of \" + str(bNAcc_bMPFC_r2[1]))\n",
    "\n",
    "# NAcc & AIns\n",
    "bNAcc_bAIns_r1 = pearsonr(all_bNAcc_r1_avg, all_bAIns_r1_avg)\n",
    "bNAcc_bAIns_r2 = pearsonr(all_bNAcc_r2_avg, all_bAIns_r2_avg)\n",
    "print(\"\\nRun 1 NAcc and AIns have a correlation \" + str(bNAcc_bAIns_r1[0]) + \" with a p-value of \" + str(bNAcc_bAIns_r1[1]))\n",
    "#print(\"Run 2 NAcc and AIns have a correlation \" + str(bNAcc_bAIns_r2[0]) + \" with a p-value of \" + str(bNAcc_bAIns_r2[1]))\n",
    "\n",
    "# MPFC & AIns\n",
    "bMPFC_bAIns_r1 = pearsonr(all_bMPFC_r1_avg, all_bAIns_r1_avg)\n",
    "bMPFC_bAIns_r2 = pearsonr(all_bMPFC_r2_avg, all_bAIns_r2_avg)\n",
    "print(\"\\nRun 1 MPFC and AIns have a correlation \" + str(bMPFC_bAIns_r1[0]) + \" with a p-value of \" + str(bMPFC_bAIns_r1[1]))\n",
    "#print(\"Run 2 MPFC and AIns have a correlation \" + str(bMPFC_bAIns_r2[0]) + \" with a p-value of \" + str(bMPFC_bAIns_r2[1]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting ROI timecourse for participant: 01 and type: psc\n",
      "Participant 01 has acceptable data for both runs.\n",
      "Getting ROI timecourse for participant: 02 and type: psc\n",
      "Participant 02 has acceptable data for both runs.\n",
      "Getting ROI timecourse for participant: 03 and type: psc\n",
      "Participant 03 has acceptable data for both runs.\n",
      "Getting ROI timecourse for participant: 04 and type: psc\n",
      "Participant 04 has acceptable data for both runs.\n",
      "Getting ROI timecourse for participant: 08 and type: psc\n",
      "Participant 08 has acceptable data for both runs.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Default set-up: \n",
    "    Confounds: \"white_matter\", \"csf\", \"csf_wm\", \"framewise_displacement\", \"dvars\", \"rmsd\", \"tcompcor\"\n",
    "    \"rotation_x\", \"rotation_y\", \"rotation_z\", \"translation_x\", \"translation_y\", \"translation_z\",  \"non-steady-stateX\".\n",
    "    Detrend: True\n",
    "    Hemodynamic response function lag: 4 seconds\n",
    "\"\"\" \n",
    "\n",
    "sub01_ROIs_timecourse, sub01_events_df, sub01_ROIs_timecourse_raw_detrended = getROIs_timecourse_detrend(\"01\", \"psc\", sub01_motion) # Max-Mean FD -> Run 1: (0.911 , 0.095) Run 2: (0.571, 0.077)\n",
    "sub02_ROIs_timecourse, sub02_events_df, sub02_ROIs_timecourse_raw_detrended = getROIs_timecourse_detrend(\"02\", \"psc\", sub02_motion) # Max-Mean FD -> Run 1: (0.663 , 0.073) Run 2: (0.734, 0.108) \n",
    "sub03_ROIs_timecourse, sub03_events_df, sub03_ROIs_timecourse_raw_detrended = getROIs_timecourse_detrend(\"03\", \"psc\", sub03_motion) # Max-Mean FD -> Run 1: (0.456 , 0.048) Run 2: (0.300, 0.049) \n",
    "sub04_ROIs_timecourse, sub04_events_df, sub04_ROIs_timecourse_raw_detrended = getROIs_timecourse_detrend(\"04\", \"psc\", sub04_motion) # Max-Mean FD -> Run 1: (0.607 , 0.080) Run 2: (0.519, 0.076) \n",
    "sub08_ROIs_timecourse, sub08_events_df, sub08_ROIs_timecourse_raw_detrended = getROIs_timecourse_detrend(\"08\", \"psc\", sub04_motion) # Max-Mean FD -> Run 1: (0.607 , 0.080) Run 2: (0.519, 0.076)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get average timecourse for each ROI.\n",
    "all_bNAcc_r1_detrended_array = np.vstack((sub01_ROIs_timecourse_raw_detrended[\"Bilateral_NAcc_r1\"], sub02_ROIs_timecourse_raw_detrended[\"Bilateral_NAcc_r1\"], sub03_ROIs_timecourse_raw_detrended[\"Bilateral_NAcc_r1\"], sub04_ROIs_timecourse_raw_detrended[\"Bilateral_NAcc_r1\"],\n",
    "                                          sub08_ROIs_timecourse_raw_detrended[\"Bilateral_NAcc_r1\"]))\n",
    "all_bNAcc_r1_detrended_avg = np.mean(all_bNAcc_r1_detrended_array, axis=0)\n",
    "\n",
    "all_bAIns_r1_detrended_array = np.vstack((sub01_ROIs_timecourse_raw_detrended[\"Bilateral_AIns_r1\"], sub02_ROIs_timecourse_raw_detrended[\"Bilateral_AIns_r1\"], sub03_ROIs_timecourse_raw_detrended[\"Bilateral_AIns_r1\"], sub04_ROIs_timecourse_raw_detrended[\"Bilateral_AIns_r1\"],\n",
    "                                          sub08_ROIs_timecourse_raw_detrended[\"Bilateral_AIns_r1\"]))\n",
    "all_bAIns_r1_detrended_avg = np.mean(all_bAIns_r1_detrended_array, axis=0)\n",
    "\n",
    "all_bMPFC_r1_detrended_array = np.vstack((sub01_ROIs_timecourse_raw_detrended[\"Bilateral_MPFC_r1\"], sub02_ROIs_timecourse_raw_detrended[\"Bilateral_MPFC_r1\"], sub03_ROIs_timecourse_raw_detrended[\"Bilateral_MPFC_r1\"], sub04_ROIs_timecourse_raw_detrended[\"Bilateral_MPFC_r1\"],\n",
    "                                          sub08_ROIs_timecourse_raw_detrended[\"Bilateral_MPFC_r1\"]))\n",
    "all_bMPFC_r1_detrended_avg = np.mean(all_bMPFC_r1_detrended_array, axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1 NAcc and MPFC have a correlation 0.3516110815258614 with a p-value of 1.9293754094885433e-12\n",
      "Run 1 NAcc and AIns have a correlation 0.4075357489024547 with a p-value of 1.476249732498815e-16\n",
      "Run 1 MPFC and AIns have a correlation 0.32051828404854243 with a p-value of 1.7703619570055808e-10\n"
     ]
    }
   ],
   "source": [
    "# Compute correlations between each roi.\n",
    "\n",
    "# NAcc & MPFC \n",
    "bNAcc_bMPFC_r1_detrended = pearsonr(all_bNAcc_r1_detrended_avg, all_bMPFC_r1_detrended_avg)\n",
    "print(\"Run 1 NAcc and MPFC have a correlation \" + str(bNAcc_bMPFC_r1_detrended[0]) + \" with a p-value of \" + str(bNAcc_bMPFC_r1_detrended[1]))\n",
    "\n",
    "# NAcc & AIns\n",
    "bNAcc_bAIns_r1_detrended = pearsonr(all_bNAcc_r1_detrended_avg, all_bAIns_r1_detrended_avg)\n",
    "print(\"Run 1 NAcc and AIns have a correlation \" + str(bNAcc_bAIns_r1_detrended[0]) + \" with a p-value of \" + str(bNAcc_bAIns_r1_detrended[1]))\n",
    "\n",
    "# MPFC & AIns\n",
    "bMPFC_bAIns_r1_detrended = pearsonr(all_bMPFC_r1_detrended_avg, all_bAIns_r1_detrended_avg)\n",
    "print(\"Run 1 MPFC and AIns have a correlation \" + str(bMPFC_bAIns_r1_detrended[0]) + \" with a p-value of \" + str(bMPFC_bAIns_r1_detrended[1]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
