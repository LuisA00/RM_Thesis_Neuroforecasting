{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import nibabel as nib\n",
    "from nilearn.plotting import plot_img, plot_stat_map, view_img, plot_prob_atlas\n",
    "from nilearn.regions import connected_label_regions\n",
    "from nilearn.glm.first_level.hemodynamic_models import spm_hrf\n",
    "from nilearn.image import concat_imgs, mean_img, index_img, smooth_img\n",
    "from nilearn.glm.first_level import FirstLevelModel\n",
    "from nilearn.glm import threshold_stats_img\n",
    "import matplotlib.pyplot as plt\n",
    "from nilearn.plotting import plot_design_matrix\n",
    "from nilearn.interfaces.fmriprep import load_confounds_strategy\n",
    "from nilearn.plotting import plot_roi\n",
    "from nilearn.maskers import NiftiMapsMasker, NiftiSpheresMasker\n",
    "from scipy.interpolate import interp1d\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homemade functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Merge(dict1, dict2):\n",
    "\n",
    "    # create a new dictionary by merging the items of the two dictionaries using the union operator (|)\n",
    "    merged_dict = dict(dict1.items() | dict2.items())\n",
    "    \n",
    "    # return the merged dictionary\n",
    "    return merged_dict\n",
    "\n",
    "def merge_dictionaries(dict1, dict2):\n",
    "    merged_dict = dict1.copy()\n",
    "    merged_dict.update(dict2)\n",
    "    return merged_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to invert scores if needed. \n",
    "def invert_score(score):\n",
    "    if(score == 1):  \n",
    "        return 4\n",
    "    elif(score == 2):  \n",
    "        return 3\n",
    "    elif(score == 3):  \n",
    "        return 2\n",
    "    elif(score == 4):  \n",
    "        return 1\n",
    "    else: \n",
    "        return \"Something went wrong here!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percent signal change .\n",
    "def get_psc(timecourse):\n",
    "\n",
    "   # Get number of ROIs and data points in timecourse.\n",
    "   roi_num = timecourse.shape[1]\n",
    "   data_length = timecourse.shape[0]\n",
    "\n",
    "   # Copy timecourse into new array.\n",
    "   psc_timecourse = np.zeros(timecourse.shape)\n",
    "\n",
    "   # Warning for empty arrays. \n",
    "   if(roi_num ==0):\n",
    "      print(\"Watch out, this array is empty!\")\n",
    "\n",
    "   # Loop through every ROI and derive the psc. \n",
    "   for id in range(roi_num):\n",
    "\n",
    "      current_roi_avg = np.mean(timecourse[:, id], axis=0)\n",
    "\n",
    "      for idx in range(data_length):\n",
    "\n",
    "         # Formula to get percent signal change -> ((point-avg)/avg)*100.\n",
    "         psc_timecourse[idx, id] = ((timecourse[idx, id] - current_roi_avg)/ current_roi_avg)*100\n",
    "\n",
    "   return psc_timecourse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_events_data(run_dataframe):\n",
    "\n",
    "    proccesed_events_df = pd.DataFrame(columns={\"Trailer\", \"Type\", \"Onset\", \"Duration\", \"Offset\", \"W_score\", \"A_score\", \"F_score\"}) \n",
    "\n",
    "    # Initial fixation 12 sec (TR=6).\n",
    "    in_fix = 12\n",
    "\n",
    "    # Time it take subjects to complete questionnaire 12 sec (TR=6). \n",
    "    questionnaire_duration = 12\n",
    "\n",
    "    # All trailers last 30 sec (TR=15). \n",
    "    trailer_duration = 30\n",
    "\n",
    "    # Initialize this variable, though it will change through each iteration of the loop.\n",
    "    trailer_onset = 0\n",
    "\n",
    "    # Run a for loop for each row in the df. \n",
    "    for id in range(run_dataframe.shape[0]):\n",
    "\n",
    "        # Get trailer label and separate them accroding to their type. \n",
    "        trailer_name = run_dataframe[\"label\"][id]\n",
    "        trailer_type = \"Horror\" if \"h\" in run_dataframe[\"label\"][id] else \"Comedy\"\n",
    "\n",
    "        # This onsets don't work, so I need to re-calculate them\n",
    "        traile_iti = run_dataframe[\"trial_ITI\"][id]\n",
    "        \n",
    "        # For the first run add the initial fixation time to the calculation of the first trailer onset. \n",
    "        # After the first run, calculate onset by adding previous traile onset, questionnaire duration, and trial iti.\n",
    "        if (id == 0):\n",
    "            trailer_onset += in_fix\n",
    "        else:\n",
    "            trailer_onset += trailer_duration + questionnaire_duration + traile_iti\n",
    "\n",
    "        # Calculate trailer onset. \n",
    "        trailer_offset = trailer_onset + 30 \n",
    "\n",
    "        \"\"\" \n",
    "        For the questionnaire scores, as I understood it. If they were not inverted ([\"scale_flip\"] == 0), then \n",
    "        the lower the score the stronger the response. If they were inverted ([\"scale_flip\"] == 1), the higher the \n",
    "        score the stronger the response.\n",
    "        \"\"\"\n",
    "        trailer_watch_score = run_dataframe[\"exp_Watch.keys\"][id].astype(int)\n",
    "        trailer_arousal_score = run_dataframe[\"exp_Arousal.keys\"][id].astype(int)\n",
    "        trailer_feel_score = run_dataframe[\"exp_Feel.keys\"][id].astype(int)\n",
    "\n",
    "        # Check if scaled was flipped and put scores on the same scale. \n",
    "        # For me, the most intuitive is that the higher the score, the stronger the response. \n",
    "        if(run_dataframe[\"scale_flip\"][id] == 0):\n",
    "            trailer_watch_score = run_dataframe[\"exp_Watch.keys\"].apply(invert_score)[id].astype(int)\n",
    "            trailer_arousal_score = run_dataframe[\"exp_Arousal.keys\"].apply(invert_score)[id].astype(int)\n",
    "            trailer_feel_score = run_dataframe[\"exp_Feel.keys\"].apply(invert_score)[id].astype(int)\n",
    "        \n",
    "        # Place processed data on list, add list to new dataframe, and concat to main dataframe. \n",
    "        current_row_data = [[trailer_name, trailer_type, trailer_onset, trailer_duration, trailer_offset, trailer_watch_score, trailer_arousal_score, trailer_feel_score]]\n",
    "        current_row = pd.DataFrame(data=current_row_data, columns=[\"Trailer\", \"Type\", \"Onset\", \"Duration\", \"Offset\", \"W_score\", \"A_score\", \"F_score\"]) \n",
    "        proccesed_events_df = pd.concat([proccesed_events_df, current_row], ignore_index=True)\n",
    "        proccesed_events_df = proccesed_events_df[[\"Trailer\", \"Type\", \"Onset\", \"Offset\", \"Duration\", \"W_score\", \"A_score\", \"F_score\"]]\n",
    "\n",
    "    return proccesed_events_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working dir : /Users/luisalvarez/Documents/Datasets\n"
     ]
    }
   ],
   "source": [
    "# Open a datasets directory. \n",
    "fd = os.open(\"/Users/luisalvarez/Documents/Datasets\", os.O_RDONLY)\n",
    "\n",
    "# Use os.fchdir() method to change the current dir/folder.\n",
    "os.fchdir(fd)\n",
    "\n",
    "# Safe check- Print current working directory\n",
    "print(\"Current working dir : %s\" % os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getNAcc_timecourse(participant_num): \n",
    "\n",
    "    # Add code to flag if something that is not a number is passed. \n",
    "\n",
    "    ## 1) Load data.\n",
    "    # Load relevant files for participant\n",
    "    sub_run1_func_path = \"MovieData_BIDS_preproc/sub-\" + participant_num + \"/func/sub-\" + participant_num + \"_task-movie_run-01_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz\"\n",
    "    sub_run2_func_path = \"MovieData_BIDS_preproc/sub-\" + participant_num + \"/func/sub-\" + participant_num + \"_task-movie_run-02_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz\"\n",
    "\n",
    "    sub_run1_mask_path = \"MovieData_BIDS_preproc/sub-\" + participant_num + \"/func/sub-\" + participant_num + \"_task-movie_run-01_space-MNI152NLin2009cAsym_desc-brain_mask.nii.gz\"\n",
    "    sub_run2_mask_path = \"MovieData_BIDS_preproc/sub-\" + participant_num + \"/func/sub-\" + participant_num + \"_task-movie_run-02_space-MNI152NLin2009cAsym_desc-brain_mask.nii.gz\"\n",
    "\n",
    "    sub_run1_events_path = \"MovieData_BIDS_raw/sub-\" + participant_num + \"/func/sub-\" + participant_num + \"_task-movie_run-01_events.csv\"\n",
    "    sub_run2_events_path = \"MovieData_BIDS_raw/sub-\" + participant_num + \"/func/sub-\" + participant_num + \"_task-movie_run-02_events.csv\"\n",
    "\n",
    "    sub_run1_confounds_path = \"MovieData_BIDS_preproc/sub-\" + participant_num + \"/func/sub-\" + participant_num + \"_task-movie_run-01_desc-confounds_timeseries.tsv\"\n",
    "    sub_run2_confounds_path = \"MovieData_BIDS_preproc/sub-\" + participant_num + \"/func/sub-\" + participant_num + \"_task-movie_run-01_desc-confounds_timeseries.tsv\"\n",
    "\n",
    "    # Calculate relevant parameters for GLM and ROI time-course analysis.\n",
    "    func_file = nib.load(sub_run1_func_path)\n",
    "    func_data = func_file.get_fdata()\n",
    "    n_vols = func_data.shape[3]\n",
    "    TR = 2\n",
    "    n_timepoints = n_vols*TR\n",
    "\n",
    "    # Load raw events files. \n",
    "    sub_run1_events_df = pd.read_csv(sub_run1_events_path, index_col=0)\n",
    "    sub_run2_events_df = pd.read_csv(sub_run2_events_path, index_col=0)\n",
    "\n",
    "    ## 2) Process files. \n",
    "    # Process event files. \n",
    "    sub_run1_p_events = process_events_data(sub_run1_events_df)\n",
    "    sub_run2_p_events = process_events_data(sub_run2_events_df)\n",
    "\n",
    "    # Down-sample time onsets to get vol onsets. \n",
    "    # Create array from 0 to 'n_timepoints' in steps of 1.\n",
    "    time_scale = np.arange(0, n_timepoints, 1)  \n",
    "\n",
    "    # Create array from 0 to 'n_timepoints' in steps of 2.\n",
    "    vol_scale = np.arange(0, n_timepoints, TR)  \n",
    "\n",
    "    # Get the labels of each trailer for each run. \n",
    "    run1_trailer_labels = sub_run1_p_events[\"Trailer\"].tolist()\n",
    "    run2_trailer_labels = sub_run2_p_events[\"Trailer\"].tolist()\n",
    "\n",
    "    # Create dictionary variable to store arrays with onset values for each trailer. \n",
    "    run1_onsets = {}\n",
    "    run2_onsets = {}\n",
    "\n",
    "    # Create a dictionary with all the onsets for each trailer in each run. \n",
    "    for id in range(len(run1_trailer_labels)):\n",
    "\n",
    "        # Create array of zeros.\n",
    "        run1_trailer_onsets = np.zeros(n_timepoints)\n",
    "        run2_trailer_onsets = np.zeros(n_timepoints)\n",
    "\n",
    "        # Get onset time. \n",
    "        run1_current_trailer_onset = sub_run1_p_events[\"Onset\"][id]\n",
    "        run2_current_trailer_onset = sub_run2_p_events[\"Onset\"][id]\n",
    "\n",
    "        # Assign 1 to such onset all the way til the end of the trailer (30 sec) in the array of zeros.\n",
    "        # Adjust for lag: add 4 seconds at the onset and offset\n",
    "        run1_trailer_onsets[int(run1_current_trailer_onset + 4):int(run1_current_trailer_onset)+ 30 + 4] = 1\n",
    "        run2_trailer_onsets[int(run2_current_trailer_onset + 4):int(run2_current_trailer_onset)+ 30 + 4] = 1\n",
    "\n",
    "        # Create resampler objects for each trailer/run of reward.\n",
    "        run1_resampler = interp1d(time_scale, run1_trailer_onsets)\n",
    "        run2_resampler = interp1d(time_scale, run2_trailer_onsets)\n",
    "\n",
    "        # Create downsampled arrays for each trailer. \n",
    "        # Note this vol arrays are half the length than the time arrays.\n",
    "        run1_trailer_vol_onsets = run1_resampler(vol_scale)\n",
    "        run2_trailer_vol_onsets = run2_resampler(vol_scale)\n",
    "\n",
    "        # Append/store the downsampled volumes arrays to each dictionary.\n",
    "        # I'm doing it this way, so the code is more interpretable\n",
    "        run1_onsets[run1_trailer_labels[id]] = run1_trailer_vol_onsets\n",
    "        run2_onsets[run2_trailer_labels[id]] = run2_trailer_vol_onsets\n",
    "\n",
    "    ## 3) Load confound data. \n",
    "    sub_run1_confounds_df = pd.read_csv(sub_run1_confounds_path, sep='\\t')\n",
    "    sub_run2_confounds_df = pd.read_csv(sub_run2_confounds_path, sep='\\t')\n",
    "    default_confounds = ['trans_y', 'trans_x', 'trans_z', 'rot_y', 'rot_x', 'rot_z',\n",
    "                        \"white_matter\", \"csf\", \"csf_wm\",  \"framewise_displacement\", \"dvars\"]\n",
    "\n",
    "    # Add confound columns if they contain 'motion' in the title. \n",
    "    sub_run1_motion_confounds = [i for i in sub_run1_confounds_df.columns if \"state\" in i] #\"motion\"\n",
    "    sub_run2_motion_confounds = [i for i in sub_run2_confounds_df.columns if \"state\" in i] \n",
    "    sub_run1_filtered_confounds_df = sub_run1_confounds_df[default_confounds + sub_run1_motion_confounds[0:3]]\n",
    "    sub_run2_filtered_confounds_df = sub_run2_confounds_df[default_confounds + sub_run2_motion_confounds[0:3]]\n",
    "\n",
    "    # Change NaNs to 0s. \n",
    "    sub_run1_filtered_confounds_df = sub_run1_filtered_confounds_df.fillna(0) \n",
    "    sub_run2_filtered_confounds_df = sub_run2_filtered_confounds_df.fillna(0) \n",
    "\n",
    "    ## 4) Apply mask to func data. \n",
    "    # Applying a Gaussian filter with a 4mm kernel\n",
    "    sub_run1_func_smooth = smooth_img(sub_run1_func_path, 4)\n",
    "    sub_run2_func_smooth = smooth_img(sub_run2_func_path, 4)\n",
    "\n",
    "    masker_sNAcc_r1 = NiftiSpheresMasker(\n",
    "        seeds=[(10, 12, -2), (-10, 12, -2)],  # right, left\n",
    "        radius=8, \n",
    "        mask_img=sub_run1_mask_path,\n",
    "        standardize=False, \n",
    "        t_r=2,\n",
    "        standardize_confounds=True, \n",
    "        high_pass=0.002,\n",
    "        low_pass=0.1 # from 1.0\n",
    "        )\n",
    "\n",
    "    masker_sNAcc_r2 = NiftiSpheresMasker(\n",
    "        seeds=[(10, 12, -2), (-10, 12, -2)],  # right, left\n",
    "        radius=8, \n",
    "        mask_img=sub_run2_mask_path,\n",
    "        standardize=False, \n",
    "        t_r=2,\n",
    "        standardize_confounds=True,\n",
    "        high_pass=0.002,\n",
    "        low_pass=0.1 # from 1.0\n",
    "        )\n",
    "\n",
    "    # Mask the func data and get a time series for the ROI. \n",
    "    # Note this is similar to fitting the GLM, but without the event files.\n",
    "    sub_r1_bNAcc = masker_sNAcc_r1.fit_transform(sub_run1_func_smooth, confounds=sub_run1_filtered_confounds_df)\n",
    "    sub_r2_bNAcc = masker_sNAcc_r2.fit_transform(sub_run2_func_smooth, confounds=sub_run2_filtered_confounds_df)\n",
    "\n",
    "    # Apply function to get the percent signal change from each ROI timecourse. \n",
    "    sub_r1_bNAcc_psc = get_psc(sub_r1_bNAcc)\n",
    "    sub_r2_bNAcc_psc = get_psc(sub_r2_bNAcc)\n",
    "\n",
    "    ## 5) Get the timecourses from each movie trailer. \n",
    "    # Create dictionary variable to store arrays with time series arrays for each trailer. \n",
    "    run1_timeseries = {}\n",
    "    run2_timeseries = {}\n",
    "\n",
    "    # Get the trailers presented in each run. \n",
    "    r1_keys = list(run1_onsets.keys())\n",
    "    r2_keys = list(run2_onsets.keys())\n",
    "\n",
    "    # Loop through each traile and get its corresponding ROI timecourse\n",
    "    # from bilateral, left, and right, NAcc.\n",
    "    for id in range(len(r1_keys)):\n",
    "\n",
    "        run1_timeseries[r1_keys[id]] = {\n",
    "            \"Bilateral_NAcc\": np.mean(sub_r1_bNAcc_psc[run1_onsets[r1_keys[id]].astype(bool)][:, :], axis=1),\n",
    "            \"Left_NAcc\": sub_r1_bNAcc_psc[run1_onsets[r1_keys[id]].astype(bool)][:, 1],\n",
    "            \"Right_NAcc\": sub_r1_bNAcc_psc[run1_onsets[r1_keys[id]].astype(bool)][:, 0]}\n",
    "    \n",
    "        run2_timeseries[r2_keys[id]] = {\n",
    "            \"Bilateral_NAcc\": np.mean(sub_r2_bNAcc_psc[run2_onsets[r2_keys[id]].astype(bool)][:, :], axis=1),\n",
    "            \"Left_NAcc\": sub_r2_bNAcc_psc[run2_onsets[r2_keys[id]].astype(bool)][:, 1],\n",
    "            \"Right_NAcc\": sub_r2_bNAcc_psc[run2_onsets[r2_keys[id]].astype(bool)][:, 0]}\n",
    "\n",
    "    all_timeseries = merge_dictionaries(run1_timeseries, run2_timeseries)\n",
    "\n",
    "    return all_timeseries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub01_NAcc_timecourse = getNAcc_timecourse(\"01\")\n",
    "sub02_NAcc_timecourse = getNAcc_timecourse(\"02\")\n",
    "sub03_NAcc_timecourse = getNAcc_timecourse(\"03\")\n",
    "sub04_NAcc_timecourse = getNAcc_timecourse(\"04\")\n",
    "sub05_NAcc_timecourse = getNAcc_timecourse(\"05\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
