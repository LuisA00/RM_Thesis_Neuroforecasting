{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import nibabel as nib\n",
    "from nilearn.plotting import plot_img, plot_stat_map, view_img, plot_prob_atlas\n",
    "from nilearn.glm.first_level.hemodynamic_models import spm_hrf\n",
    "from nilearn.image import concat_imgs, mean_img, index_img, smooth_img\n",
    "from nilearn.glm import threshold_stats_img\n",
    "import matplotlib.pyplot as plt\n",
    "from nilearn.plotting import plot_roi\n",
    "from nilearn.maskers import NiftiSpheresMasker, NiftiMasker\n",
    "from scipy.interpolate import interp1d\n",
    "import seaborn as sns\n",
    "from scipy.stats.mstats import pearsonr\n",
    "from scipy.stats import zscore\n",
    "import scipy.stats as stats\n",
    "from nilearn.image import high_variance_confounds\n",
    "from nilearn.interfaces.fmriprep import load_confounds, load_confounds_strategy\n",
    "from scipy import stats as st\n",
    "from nilearn import image as nimg\n",
    "from nilearn.image import resample_to_img\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change 3dFourier to 3dFBandpass\n",
    "# Do linear warps, do not use non-linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possibilities: \n",
    "- MPFC 2 tr after nacc and ains.\n",
    "\n",
    "- Also, calculate slope between tr_1 and tr_2 that crosses a 1 std. \n",
    "- Get the highest correlation for comedy, see when TR happens. \n",
    "- What to do if there's more than one peak (i.e., if there's more than two points higher than std)?\n",
    "\n",
    "- Code to get Arousal, Feel, and Watch z-score signal for each roi. \n",
    "- Get peaks ids from Like-Horror and Like-Comedy sub-groups. Compare these peaks with Yes-Horror and Yes-Comedy.\n",
    "- Update extended peaks labels for AIns and MPFC.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homemade functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_censored_vols(roi_timeseries, censored_vols):\n",
    "\n",
    "    # Create a new array to store the processed data.\n",
    "    processed_array = np.zeros(roi_timeseries.shape)\n",
    "\n",
    "    # Loop through each ROI and timepoint.\n",
    "    for roi_id in range(roi_timeseries.shape[1]):\n",
    "\n",
    "        length_array = np.arange(roi_timeseries.shape[0])\n",
    "\n",
    "        for vol in range(roi_timeseries.shape[0]):\n",
    "\n",
    "            # If the current volume is censored, replace the value with NaN.\n",
    "            if vol not in censored_vols:\n",
    "\n",
    "                processed_array[vol, roi_id] = np.nan\n",
    "\n",
    "            else:\n",
    "                \n",
    "                processed_array[vol, roi_id] = roi_timeseries[vol, roi_id]\n",
    "\n",
    "    return processed_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_dem_peak_ROI(all_participants_dic, all_participants_events_dic, sub_id, ROI_id, trailer_id, dem_id, tail):\n",
    "\n",
    "    # Identy to which run the trailer belongs to.\n",
    "    r1_keys = ['rh6', 'rc2', 'uh1', 'rc9', 'rh2', 'uc4', 'rh1', 'rc6', 'rh9', 'rc11', 'uh2', 'uc1', 'rh3', 'rc12', 'rh11', 'rc5']\n",
    "    r2_keys = ['rh12', 'rc3', 'rh7', 'rc1', 'rh10', 'rc8', 'rh4', 'uc3', 'rh5', 'rc4', 'rh8', 'uc2', 'uh3', 'rc10', 'uh4', 'rc7']\n",
    "\n",
    "    run_id = ''    \n",
    "\n",
    "    if(trailer_id in r1_keys):\n",
    "        run_id = '_r1'\n",
    "    elif(trailer_id in r2_keys):\n",
    "        run_id = '_r2'\n",
    "    else:\n",
    "        print(\"Trailer_id not found in any run: \" + str(trailer_id))\n",
    "\n",
    "    # Define roi labels\n",
    "    if(ROI_id == 'NAcc'):\n",
    "        ROI_key = 'Bilateral_NAcc' + run_id\n",
    "    elif(ROI_id == 'AIns'):\n",
    "        ROI_key = 'Bilateral_AIns' + run_id\n",
    "    elif(ROI_id == 'MPFC'):\n",
    "        ROI_key = 'Bilateral_MPFC' + run_id\n",
    "    else:\n",
    "        print(\"Incorrect ROI_id. Please use 'NAcc', 'AIns' or 'MPFC'.\")\n",
    "\n",
    "    # Get the timecourse of the ROI.\n",
    "    timecourse = all_participants_dic[sub_id][ROI_key]\n",
    "    time_length = len(timecourse)*2\n",
    "\n",
    "    # Get onset for trailer.\n",
    "    onset = all_participants_events_dic[sub_id].loc[all_participants_events_dic[sub_id][\"Trailer\"] == trailer_id][\"Onset\"].values[0]\n",
    "\n",
    "    # Re-scale.\n",
    "    # Create array from 0 to 'n_timepoints' in steps of 1.\n",
    "    time_scale = np.arange(0, time_length, 1)  \n",
    "\n",
    "    # Create array from 0 to 'n_timepoints' in steps of 2.\n",
    "    vol_scale = np.arange(0, time_length, 2)  \n",
    "\n",
    "    # Interpolate timecourse.\n",
    "    trailer_onset = np.zeros(time_length)\n",
    "\n",
    "    # Add add 4 sec from hemodynamic lag to onset.\n",
    "    onset = onset + 4 \n",
    "\n",
    "    # Get the onset in seconds.\n",
    "    trailer_onset[int(onset)] = 1\n",
    "\n",
    "    # Interpolate the timecourse.\n",
    "    run_resampler = interp1d(time_scale, trailer_onset)\n",
    "    run_trailer_vol_onset = run_resampler(vol_scale)\n",
    "\n",
    "    # Get the peak of the timecourse.\n",
    "    actual_onset = np.where(run_trailer_vol_onset == 1)[0][0]\n",
    "\n",
    "    # Get peak location on raw timecourse.\n",
    "    peak_id = int(actual_onset + int(dem_id))\n",
    "\n",
    "    # Get the peak value.\n",
    "    tail = int(tail)\n",
    "    dem_peak = timecourse[peak_id:peak_id + tail]\n",
    "    dem_peak = np.mean(dem_peak)\n",
    "\n",
    "    return dem_peak\n",
    "\n",
    "def get_avg_dem_peak(timecourse, peak_id):\n",
    "\n",
    "    # Make sure peak id is an int value.\n",
    "    try:\n",
    "        peak_id = int(peak_id)\n",
    "    except TypeError:\n",
    "        print('We have two peaks here: ' + str(peak_id))\n",
    "        peak_id = int(peak_id[0])\n",
    "\n",
    "    peak_id = int(peak_id)\n",
    "\n",
    "    # Check if the peak is the first or last timepoint.\n",
    "    # For the peak, I am getting the average for all time points \n",
    "    # one TR before and after the peak.\n",
    "    if(peak_id == 0):\n",
    "        peak = np.mean(timecourse[peak_id:peak_id+1])\n",
    "    elif(peak_id == len(timecourse)-1):\n",
    "        #peak = np.mean(timecourse[peak_id-1:peak_id])\n",
    "        peak = timecourse[peak_id]\n",
    "    else:\n",
    "        peak = np.mean(timecourse[peak_id:peak_id+1])    \n",
    "    return peak\n",
    "\n",
    "def get_avg_dem_peak2(timecourse, peak_id):\n",
    "\n",
    "    # Make sure peak id is an int value.\n",
    "    try:\n",
    "        peak_id = int(peak_id)\n",
    "    except TypeError:\n",
    "        print('We have two peaks here: ' + str(peak_id))\n",
    "        peak_id = int(peak_id[0])\n",
    "\n",
    "    peak_id = int(peak_id)\n",
    "\n",
    "    # Check if the peak is the first or last timepoint.\n",
    "    # For the peak, I am getting the average for all time points \n",
    "    # one TR before and after the peak.\n",
    "    if(peak_id == 0):\n",
    "        peak = np.mean(timecourse[peak_id:peak_id+1])\n",
    "    elif(peak_id == len(timecourse)-1):\n",
    "        peak = np.mean(timecourse[peak_id:peak_id])\n",
    "        #peak = timecourse[peak_id]\n",
    "    else:\n",
    "        peak = np.mean(timecourse[peak_id:peak_id+1])    \n",
    "    return peak\n",
    "\n",
    "def getPeak(timecourse, peak_id, tail):\n",
    "\n",
    "    # Make sure peak id is an int value.\n",
    "    #try:\n",
    "    #    peak_id = int(peak_id)\n",
    "    #except TypeError:\n",
    "    #    print('We have two peaks here: ' + str(peak_id))\n",
    "    #    peak_id = int(peak_id[0])\n",
    "\n",
    "    peak_id = int(peak_id)\n",
    "    tail_key = int(peak_id + tail)\n",
    "\n",
    "    backward_tail = int(peak_id - tail)\n",
    "\n",
    "    # Check if the peak is the first or last timepoint.\n",
    "    if(peak_id == 0):\n",
    "\n",
    "        # Good one\n",
    "        peak = np.nanmean(timecourse[peak_id:tail_key])\n",
    "\n",
    "        #Test.\n",
    "        #peak = np.nanmean(timecourse[peak_id])\n",
    "\n",
    "    elif(peak_id == len(timecourse)-1):\n",
    "        \n",
    "\n",
    "        # Good one.\n",
    "        #peak = np.nanmean(timecourse[peak_id-1:peak_id])\n",
    "        peak = np.nanmean(timecourse[peak_id])\n",
    "\n",
    "        #Test.\n",
    "        #peak = np.nanmean(timecourse[backward_tail:peak_id])\n",
    "\n",
    "    else:\n",
    "        \n",
    "        # Good one.\n",
    "        #peak = np.nanmean(timecourse[peak_id-1:tail_key])  \n",
    "        peak = np.nanmean(timecourse[peak_id:tail_key])   \n",
    "\n",
    "        #Test.\n",
    "        #peak = np.nanmean(timecourse[backward_tail:tail_key])\n",
    "    return peak\n",
    "\n",
    "\n",
    "def getPeakB(timecourse, peak_id, tail):\n",
    "\n",
    "\n",
    "    # Make sure peak id is an int value.\n",
    "    peak_id = int(peak_id)\n",
    "\n",
    "    backward_tail = int(peak_id - tail)\n",
    "\n",
    "    # Check if the peak is the first or last timepoint.  \n",
    "    if(peak_id == 0):\n",
    "\n",
    "        peak = timecourse[peak_id]\n",
    "\n",
    "    elif(peak_id == len(timecourse)-1):\n",
    "        \n",
    "        peak = np.nanmean(timecourse[backward_tail:peak_id])\n",
    "\n",
    "    elif( (peak_id == 1) & (tail > 0) ):\n",
    "        \n",
    "        peak = np.nanmean(timecourse[peak_id -1 :peak_id])\n",
    "\n",
    "    elif ((peak_id == 2) & (tail == 1) ):\n",
    "\n",
    "        peak = np.nanmean(timecourse[peak_id -1 :peak_id ])\n",
    "\n",
    "    elif ( (peak_id == 3) & (tail == 1) ):\n",
    "\n",
    "        peak = np.nanmean(timecourse[peak_id -1 :peak_id ])\n",
    "\n",
    "    elif ( (peak_id == 2) & (tail > 1) ):\n",
    "            \n",
    "        peak = np.nanmean(timecourse[peak_id -2 :peak_id])\n",
    "\n",
    "    elif ( (peak_id == 3) & (tail == 2) ):\n",
    "\n",
    "        peak = np.nanmean(timecourse[peak_id -2 :peak_id])\n",
    "\n",
    "    else:\n",
    "        \n",
    "        peak = np.nanmean(timecourse[backward_tail:peak_id])\n",
    "\n",
    "    return peak\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_censored_vols2(roi_timeseries, censored_vols, vols_num):\n",
    "\n",
    "    # Create a new array to store the processed data.\n",
    "    processed_array = np.zeros((vols_num, roi_timeseries.shape[1]))\n",
    "\n",
    "    # Loop through each ROI and timepoint.\n",
    "    for roi_id in range(roi_timeseries.shape[1]):\n",
    "       \n",
    "        vol_runner = 0 \n",
    "\n",
    "        for vol in range(vols_num):\n",
    "\n",
    "            # If the current volume is censored, replace the value with NaN.\n",
    "            if vol in censored_vols:\n",
    "\n",
    "                processed_array[vol, roi_id] = roi_timeseries[vol_runner, roi_id]\n",
    "                vol_runner += 1\n",
    "\n",
    "            else:\n",
    "                \n",
    "                processed_array[vol, roi_id] = np.nan\n",
    "                \n",
    "    return processed_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Merge(dict1, dict2):\n",
    "\n",
    "    # create a new dictionary by merging the items of the two dictionaries using the union operator (|)\n",
    "    merged_dict = dict(dict1.items() | dict2.items())\n",
    "    \n",
    "    # return the merged dictionary\n",
    "    return merged_dict\n",
    "\n",
    "def merge_dictionaries(dict1, dict2):\n",
    "    merged_dict = dict1.copy()\n",
    "    merged_dict.update(dict2)\n",
    "    return merged_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_peak(timecourse, max_min):\n",
    "\n",
    "    # Get the maximum peak of the timecourse.\n",
    "    if(max_min == 'max'):\n",
    "        peak_id = np.argmax(timecourse)\n",
    "    elif(max_min == 'min'):\n",
    "        peak_id = np.argmin(timecourse)\n",
    "    else: \n",
    "        print(\"Incorrect max_min value. Please use 'max' or 'min'.\")\n",
    "\n",
    "    # Check if the peak is the first or last timepoint.\n",
    "    # For the peak, I am getting the average for all time points \n",
    "    # one TR before and after the peak.\n",
    "    if(peak_id == 0):\n",
    "        peak = np.mean(timecourse[peak_id:peak_id+1])\n",
    "    elif(peak_id == len(timecourse)-1):\n",
    "        peak = np.mean(timecourse[peak_id-1:peak_id])\n",
    "    else:\n",
    "        peak = np.mean(timecourse[peak_id-1:peak_id+1])\n",
    "    \n",
    "    return peak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_individual_peak(timecourse, max_min):\n",
    "\n",
    "    # Get the maximum peak of the timecourse.\n",
    "    if(max_min == 'max'):\n",
    "        peak_id = np.argmax(timecourse)\n",
    "    elif(max_min == 'min'):\n",
    "        peak_id = np.argmin(timecourse)\n",
    "    else: \n",
    "        print(\"Incorrect max_min value. Please use 'max' or 'min'.\")\n",
    "\n",
    "    # Check if the peak is the first or last timepoint.\n",
    "    # If    \n",
    "    if(peak_id == 0):\n",
    "        peak = np.mean(timecourse[peak_id:peak_id+1])\n",
    "    elif(peak_id == len(timecourse)-1):\n",
    "        #peak = np.mean(timecourse[peak_id-1:peak_id])\n",
    "        peak = timecourse[peak_id]\n",
    "    else:\n",
    "        peak = np.mean(timecourse[peak_id:peak_id+1])\n",
    "    \n",
    "    return peak_id, peak\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to get different peaks (slope and deltas).\n",
    "#participant_dictionaries_raw\n",
    "\n",
    "# get_dem_peak_slope(participant_dictionaries_raw, participants_events, sub_id, ROI_id, trailer_id, dem_id):\n",
    "\n",
    "def get_dem_peak_slope(all_participants_dic, all_participants_events_dic, sub_id, ROI_id, trailer_id, dem_id):\n",
    "\n",
    "    # Identy to which run the trailer belongs to.\n",
    "    r1_keys = ['rh6', 'rc2', 'uh1', 'rc9', 'rh2', 'uc4', 'rh1', 'rc6', 'rh9', 'rc11', 'uh2', 'uc1', 'rh3', 'rc12', 'rh11', 'rc5']\n",
    "    r2_keys = ['rh12', 'rc3', 'rh7', 'rc1', 'rh10', 'rc8', 'rh4', 'uc3', 'rh5', 'rc4', 'rh8', 'uc2', 'uh3', 'rc10', 'uh4', 'rc7']\n",
    "\n",
    "    run_id = ''    \n",
    "\n",
    "    if(trailer_id in r1_keys):\n",
    "        run_id = '_r1'\n",
    "    elif(trailer_id in r2_keys):\n",
    "        run_id = '_r2'\n",
    "    else:\n",
    "        print(\"Trailer_id not found in any run: \" + str(trailer_id))\n",
    "\n",
    "    # Define roi labels\n",
    "    if(ROI_id == 'NAcc'):\n",
    "        ROI_key = 'Bilateral_NAcc' + run_id\n",
    "    elif(ROI_id == 'AIns'):\n",
    "        ROI_key = 'Bilateral_AIns' + run_id\n",
    "    elif(ROI_id == 'MPFC'):\n",
    "        ROI_key = 'Bilateral_MPFC' + run_id\n",
    "    else:\n",
    "        print(\"Incorrect ROI_id. Please use 'NAcc', 'AIns' or 'MPFC'.\")\n",
    "\n",
    "    # Get the timecourse of the ROI.\n",
    "    timecourse = all_participants_dic[sub_id][ROI_key]\n",
    "    time_length = len(timecourse)*2\n",
    "\n",
    "    # Get onset for trailer.\n",
    "    onset = all_participants_events_dic[sub_id].loc[all_participants_events_dic[sub_id][\"Trailer\"] == trailer_id][\"Onset\"].values[0]\n",
    "\n",
    "    # Re-scale.\n",
    "    # Create array from 0 to 'n_timepoints' in steps of 1.\n",
    "    time_scale = np.arange(0, time_length, 1)  \n",
    "\n",
    "    # Create array from 0 to 'n_timepoints' in steps of 2.\n",
    "    vol_scale = np.arange(0, time_length, 2)  \n",
    "\n",
    "    # Interpolate timecourse.\n",
    "    trailer_onset = np.zeros(time_length)\n",
    "\n",
    "    # Add add 4 sec from hemodynamic lag to onset.\n",
    "    onset = onset + 4 \n",
    "\n",
    "    # Get the onset in seconds.\n",
    "    trailer_onset[int(onset)] = 1\n",
    "\n",
    "    # Interpolate the timecourse.\n",
    "    run_resampler = interp1d(time_scale, trailer_onset)\n",
    "    run_trailer_vol_onset = run_resampler(vol_scale)\n",
    "\n",
    "    # Get the peak of the timecourse.\n",
    "    actual_onset = np.where(run_trailer_vol_onset == 1)[0][0]\n",
    "\n",
    "    # Get peak location on raw timecourse.\n",
    "    peak_id = int(actual_onset + int(dem_id))\n",
    "\n",
    "    # Calculate slope. \n",
    "    #try:\n",
    "    peak_slope = timecourse[peak_id] - timecourse[peak_id - 1]\n",
    "    #except IndexError:\n",
    "    #    print(\"IndexError: \" + str(sub_id) + \" \" + str(ROI_key) + \" \" + str(trailer_id) + \" \" + str(dem_id) + \" \" + str(peak_id))\n",
    "\n",
    "    return peak_slope\n",
    "    \n",
    "def get_peak_slope(timecourse, timecourse_z, z_threshold, max_min):\n",
    "\n",
    "    # Store all possible peak in timecourse array.\n",
    "    possible_slope_peaks = []\n",
    "\n",
    "    # Loop through each point in timecourse array, except the first one.\n",
    "    for id_tr in range(1, 15, 1):\n",
    "\n",
    "        # For positive peaks. \n",
    "        if(max_min == \"max\"):\n",
    "\n",
    "            # If score is bigger than \"z_threshold\" z-score in timecourse_z array, append value.\n",
    "            if(timecourse_z[id_tr] > z_threshold): \n",
    "\n",
    "                slope = timecourse[id_tr] - timecourse[id_tr - 1] \n",
    "                # Check the slope is not equal to zero. \n",
    "                if(slope != 0):\n",
    "                    possible_slope_peaks.append(slope)\n",
    "        \n",
    "        # For negative peaks.\n",
    "        if(max_min == \"min\"):\n",
    "\n",
    "            # If score is bigger than \"z_threshold\" z-score in timecourse_z array, append value.\n",
    "            if(timecourse_z[id_tr] < z_threshold): \n",
    "\n",
    "                slope = timecourse[id_tr] - timecourse[id_tr - 1] \n",
    "                # Check the slope is not equal to zero. \n",
    "                if(slope != 0):\n",
    "                    possible_slope_peaks.append(slope)\n",
    "    \n",
    "    average_slope = np.mean(possible_slope_peaks)\n",
    "\n",
    "    # Return only the first value and average slope.\n",
    "    if(len(possible_slope_peaks) == 0):\n",
    "        return 0, 0\n",
    "    else: \n",
    "        return possible_slope_peaks[0], average_slope\n",
    "\n",
    "def get_narrow_peak_slope(timecourse, z_threshold, max_min):\n",
    "\n",
    "    # Z-score the 15 data points.\n",
    "    z_scored_data = stats.zscore(timecourse)\n",
    "\n",
    "    # Store all possible peak in timecourse array.\n",
    "    possible_slope_peaks = []\n",
    "\n",
    "    # Loop through each point in timecourse array, except the first one.\n",
    "    for id_tr in range(1, 15, 1):\n",
    "\n",
    "        # For positive peaks.\n",
    "        if(max_min == \"max\"):\n",
    "            # If score is bigger than \"z_threshold\" z-score in timecourse_z array, append value.\n",
    "            if(z_scored_data[id_tr] > z_threshold): \n",
    "\n",
    "                slope = timecourse[id_tr] - timecourse[id_tr - 1] \n",
    "                # Check the slope is not equal to zero. \n",
    "                if(slope != 0):\n",
    "                    possible_slope_peaks.append(slope)\n",
    "    \n",
    "        # For negative peaks.\n",
    "        if(max_min == \"min\"):\n",
    "            # If score is bigger than \"z_threshold\" z-score in timecourse_z array, append value.\n",
    "            if(z_scored_data[id_tr] < z_threshold): \n",
    "\n",
    "                slope = timecourse[id_tr] - timecourse[id_tr - 1] \n",
    "                # Check the slope is not equal to zero. \n",
    "                if(slope != 0):\n",
    "                    possible_slope_peaks.append(slope)\n",
    "    \n",
    "    average_slope = np.mean(possible_slope_peaks)\n",
    "\n",
    "    # Return only the first value and average slope.\n",
    "    if(len(possible_slope_peaks) == 0):\n",
    "        return 0, 0\n",
    "    else: \n",
    "        return possible_slope_peaks[0], average_slope\n",
    "    \n",
    "def get_peak_delta(timecourse, timecourse_z, z_threshold, max_min):\n",
    "\n",
    "    # Store all possible peak in timecourse array.\n",
    "    possible_delta_peaks = []\n",
    "\n",
    "    # Loop through each point in timecourse array, except the first one.\n",
    "    for id_tr in range(1, 15, 1):\n",
    "\n",
    "        # For positive peaks. \n",
    "        if(max_min == \"max\"):\n",
    "\n",
    "            # If score is bigger than \"z_threshold\" z-score in timecourse_z array, append value.\n",
    "            if(timecourse_z[id_tr] > z_threshold): \n",
    "\n",
    "                delta = (timecourse[id_tr] - timecourse[id_tr - 1])/(timecourse[id_tr]*100) \n",
    "                #delta = (timecourse[id_tr] - timecourse[id_tr - 1])/(timecourse[id_tr] + timecourse[id_tr - 1]) \n",
    "                # Check the slope is not equal to zero. \n",
    "                if(delta != 0):\n",
    "                    possible_delta_peaks.append(delta)\n",
    "        \n",
    "        # For negative peaks.\n",
    "        if(max_min == \"min\"):\n",
    "\n",
    "            # If score is bigger than \"z_threshold\" z-score in timecourse_z array, append value.\n",
    "            if(timecourse_z[id_tr] < z_threshold): \n",
    "\n",
    "                delta = (timecourse[id_tr] - timecourse[id_tr - 1])/(timecourse[id_tr]*100) \n",
    "                #delta = (timecourse[id_tr] - timecourse[id_tr - 1])/(timecourse[id_tr] + timecourse[id_tr - 1]) \n",
    "                # Check the slope is not equal to zero. \n",
    "                if(delta != 0):\n",
    "                    possible_delta_peaks.append(delta)\n",
    "    \n",
    "    average_delta = np.mean(possible_delta_peaks)\n",
    "\n",
    "    # Return only the first value and average slope.\n",
    "    if(len(possible_delta_peaks) == 0):\n",
    "        return 0, 0\n",
    "    else: \n",
    "        return possible_delta_peaks[0], average_delta\n",
    "    \n",
    "def get_narrow_peak_delta(timecourse, z_threshold, max_min):\n",
    "\n",
    "    # Z-score the 15 data points.\n",
    "    z_scored_data = stats.zscore(timecourse)\n",
    "\n",
    "    # Store all possible peak in timecourse array.\n",
    "    possible_delta_peaks = []\n",
    "\n",
    "    # Loop through each point in timecourse array, except the first one.\n",
    "    for id_tr in range(1, 15, 1):\n",
    "\n",
    "        # For positive peaks.\n",
    "        if(max_min == \"max\"):\n",
    "            # If score is bigger than \"z_threshold\" z-score in timecourse_z array, append value.\n",
    "            if(z_scored_data[id_tr] > z_threshold): \n",
    "\n",
    "                delta = (timecourse[id_tr] - timecourse[id_tr - 1])/(timecourse[id_tr]*100) \n",
    "                #delta = (timecourse[id_tr] - timecourse[id_tr - 1])/(timecourse[id_tr] + timecourse[id_tr - 1]) \n",
    "                # Check the slope is not equal to zero. \n",
    "                if(delta != 0):\n",
    "                    possible_delta_peaks.append(delta)\n",
    "    \n",
    "        # For negative peaks.\n",
    "        if(max_min == \"min\"):\n",
    "            # If score is bigger than \"z_threshold\" z-score in timecourse_z array, append value.\n",
    "            if(z_scored_data[id_tr] < z_threshold): \n",
    "\n",
    "                delta = (timecourse[id_tr] - timecourse[id_tr - 1])/(timecourse[id_tr]*100) \n",
    "                #delta = (timecourse[id_tr] - timecourse[id_tr - 1])/(timecourse[id_tr] + timecourse[id_tr - 1]) \n",
    "                # Check the slope is not equal to zero. \n",
    "                if(delta != 0):\n",
    "                    possible_delta_peaks.append(delta)\n",
    "    \n",
    "    average_delta = np.mean(possible_delta_peaks)\n",
    "\n",
    "    # Return only the first value and average slope.\n",
    "    if(len(possible_delta_peaks) == 0):\n",
    "        return 0, 0\n",
    "    else: \n",
    "        return possible_delta_peaks[0], average_delta\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trailer_correlations(ROI_df, Trailer_id, ROI_id, TR_corr_dict, type_of_corr, axs):\n",
    "\n",
    "    if(type_of_corr == \"W_score\"):\n",
    "        color=\"green\"\n",
    "        bonus_y = 0.00\n",
    "        score_label = \"_w\"\n",
    "        text_label = \"W corr\"\n",
    "    elif(type_of_corr == \"Pos_arousal\"):\n",
    "        color=\"blue\"\n",
    "        bonus_y = 0.05\n",
    "        score_label = \"_pa\"\n",
    "        text_label = \"PA corr\"\n",
    "    elif(type_of_corr == \"Neg_arousal\"):\n",
    "        color=\"red\"\n",
    "        bonus_y = -0.05\n",
    "        score_label = \"_na\"\n",
    "        text_label = \"NA corr\"\n",
    "\n",
    "    if(ROI_id == \"Bilateral_NAcc\"):\n",
    "        y_coord_seg = 0.3 + bonus_y\n",
    "        y_coord_whole = 0.4\n",
    "    elif(ROI_id == \"Bilateral_AIns\"):\n",
    "        y_coord_seg = 0.4 + bonus_y\n",
    "        y_coord_whole = 0.5\n",
    "    elif(ROI_id == \"Bilateral_MPFC\"):\n",
    "        y_coord_seg = 0.4 + bonus_y\n",
    "        y_coord_whole = 0.5\n",
    "\n",
    "    # Get the dataframe for the current trailer.\n",
    "    current_traile_df = ROI_df[Trailer_id]\n",
    "\n",
    "    # Loop for plotting each significant correlation across the 15 TRs.\n",
    "    for TR_id in range(15):\n",
    "        TR_label = str(\"TR_\" + str(TR_id) + score_label) \n",
    "        y_coord = current_traile_df[(current_traile_df[\"ROI\"] == ROI_id) & (current_traile_df[\"TR\"] == TR_id)][\"Signal\"].mean() + bonus_y\n",
    "        x_coord = TR_id\n",
    "        if(TR_corr_dict[Trailer_id][TR_label][1] < 0.05):\n",
    "            if(TR_corr_dict[Trailer_id][TR_label][0] > 0):\n",
    "                axs.text(x_coord, y_coord, \"+*\", fontsize=13, color=color, weight=\"bold\", label=text_label)\n",
    "            else:\n",
    "                axs.text(x_coord, y_coord, \"-*\", fontsize=13, color=color, weight=\"bold\", label=text_label)\n",
    "    \n",
    "    # Plot the onset correlation if significant.\n",
    "    #current_traile_df = ROI_df[Trailer_id]\n",
    "    Onset_label = str(\"Onset\" + score_label)\n",
    "    #y_coord_seg = (current_traile_df[(current_traile_df[\"ROI\"] == ROI_id)][\"Signal\"].mean() ) + bonus_y #* 0.7\n",
    "    x_coord_onset = 3\n",
    "\n",
    "    if(TR_corr_dict[Trailer_id][Onset_label][1] < 0.05):\n",
    "        if(TR_corr_dict[Trailer_id][Onset_label][0] > 0):\n",
    "            axs.text(x_coord_onset, y_coord_seg, \"on(+)\", fontsize=13, color=color, weight=\"bold\", label=text_label)\n",
    "        else:\n",
    "            axs.text(x_coord_onset, y_coord_seg, \"on(-)\", fontsize=13, color=color, weight=\"bold\", label=text_label)\n",
    "\n",
    "    # Plot the middle correlation if significant.\n",
    "    Middle_label = str(\"Middle\" + score_label)\n",
    "    x_coord_middle = 7\n",
    "    if(TR_corr_dict[Trailer_id][Middle_label][1] < 0.05):\n",
    "        if(TR_corr_dict[Trailer_id][Middle_label][0] > 0):\n",
    "            axs.text(x_coord_middle, y_coord_seg, \"mid(+)\", fontsize=13, color=color, weight=\"bold\", label=text_label)\n",
    "        else:\n",
    "            axs.text(x_coord_middle, y_coord_seg, \"mid(-)\", fontsize=13, color=color, weight=\"bold\", label=text_label)\n",
    "\n",
    "    # Plot the offset correlation if significant.\n",
    "    Offset_label = str(\"Offset\" + score_label)\n",
    "    x_coord_offset = 13\n",
    "    if(TR_corr_dict[Trailer_id][Offset_label][1] < 0.05):\n",
    "        if(TR_corr_dict[Trailer_id][Offset_label][0] > 0):\n",
    "            axs.text(x_coord_offset, y_coord_seg, \"off(+)\", fontsize=13, color=color, weight=\"bold\", label=text_label)\n",
    "        else:\n",
    "            axs.text(x_coord_offset, y_coord_seg, \"off(-)\", fontsize=13, color=color, weight=\"bold\", label=text_label)\n",
    "\n",
    "    # Plot the whole correlation if significant.\n",
    "    Whole_label = str(\"Whole\" + score_label)\n",
    "    x_coord_whole = 7\n",
    "    #y_coord_whole = (current_traile_df[(current_traile_df[\"ROI\"] == ROI_id)][\"Signal\"].max() ) + bonus_y # * 0.8\n",
    "    if(TR_corr_dict[Trailer_id][Whole_label][1] < 0.05):\n",
    "        if(TR_corr_dict[Trailer_id][Whole_label][0] > 0):\n",
    "            axs.text(x_coord_whole, y_coord_whole, \"whole(+)\", fontsize=13, color=color, weight=\"bold\", label=text_label)\n",
    "        else:\n",
    "            axs.text(x_coord_whole, y_coord_whole, \"whole(-)\", fontsize=13, color=color, weight=\"bold\", label=text_label)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to invert scores if needed. \n",
    "def transform_scores(score, invert):\n",
    "    if(invert==1): #if(not invert):\n",
    "        if(score == 1):  \n",
    "            return 4\n",
    "        elif(score == 2):  \n",
    "            return 3\n",
    "        elif(score == 3):  \n",
    "            return 2\n",
    "        elif(score == 4):  \n",
    "            return 1\n",
    "        elif(score == \"1\"):  \n",
    "            return 4\n",
    "        elif(score == \"2\"):  \n",
    "            return 3\n",
    "        elif(score == \"3\"):  \n",
    "            return 2\n",
    "        elif(score == \"4\"):  \n",
    "            return 1\n",
    "        elif(score == \"None\"):\n",
    "            return 0\n",
    "        else: \n",
    "            return \"Something went wrong here!\"\n",
    "    else:\n",
    "        if(score == \"1\"):\n",
    "            return 1\n",
    "        elif(score == \"2\"):\n",
    "            return 2\n",
    "        elif(score == \"3\"):\n",
    "            return 3\n",
    "        elif(score == \"4\"):\n",
    "            return 4\n",
    "        elif(score == 1):\n",
    "            return 1\n",
    "        elif(score == 2):\n",
    "            return 2\n",
    "        elif(score == 3):\n",
    "            return 3\n",
    "        elif(score == 4):\n",
    "            return 4\n",
    "        elif(score == \"None\"):\n",
    "            return 0\n",
    "        else:\n",
    "            return \"Something went wrong here!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percent signal change .\n",
    "def get_psc(timecourse):\n",
    "\n",
    "   # Get number of ROIs and data points in timecourse.\n",
    "   roi_num = timecourse.shape[1]\n",
    "   data_length = timecourse.shape[0]\n",
    "\n",
    "   # Copy timecourse into new array.\n",
    "   psc_timecourse = np.zeros(timecourse.shape)\n",
    "\n",
    "   # Warning for empty arrays. \n",
    "   if(roi_num ==0):\n",
    "      print(\"Watch out, this array is empty!\")\n",
    "\n",
    "   # Loop through every ROI and derive the psc. \n",
    "   for id in range(roi_num):\n",
    "\n",
    "      current_roi_avg = np.mean(timecourse[:, id], axis=0)\n",
    "\n",
    "      for idx in range(data_length):\n",
    "\n",
    "         # Formula to get percent signal change -> ((point-avg)/avg)*100.\n",
    "         psc_timecourse[idx, id] = ((timecourse[idx, id] - current_roi_avg)/ current_roi_avg)*100\n",
    "\n",
    "   return psc_timecourse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# watch -> valence -> arousal\n",
    "def process_events_data(run_dataframe):\n",
    "\n",
    "    proccesed_events_df = pd.DataFrame(columns={\"Trailer\", \"Type\", \"Onset\", \"Duration\", \"Offset\", \"W_score\", \"A_score\", \"F_score\"}) \n",
    "\n",
    "    # Initial fixation 12 sec (TR=6).\n",
    "    in_fix = 12 \n",
    "\n",
    "    # Time it take subjects to complete questionnaire 12 sec (TR=6). \n",
    "    questionnaire_duration = 12\n",
    "\n",
    "    # All trailers last 30 sec (TR=15). \n",
    "    trailer_duration = 30\n",
    "\n",
    "    # Initialize this variable, though it will change through each iteration of the loop.\n",
    "    trailer_onset = 0\n",
    "\n",
    "    # Run a for loop for each row in the df. \n",
    "    for id in range(run_dataframe.shape[0]):\n",
    "\n",
    "        # Get trailer label and separate them accroding to their type. \n",
    "        trailer_name = run_dataframe[\"label\"][id]\n",
    "        trailer_type = \"Horror\" if \"h\" in run_dataframe[\"label\"][id] else \"Comedy\"\n",
    "        traile_iti = run_dataframe[\"trial_ITI\"][id]\n",
    "        \n",
    "        # For the first run add the initial fixation time to the calculation of the first trailer onset. \n",
    "        # After the first run, calculate onset by adding previous traile onset, questionnaire duration, and trial iti.\n",
    "        if (id == 0):\n",
    "            trailer_onset += in_fix \n",
    "        else:\n",
    "            trailer_onset += trailer_duration + questionnaire_duration + traile_iti\n",
    "\n",
    "        # Calculate trailer offser. \n",
    "        trailer_offset = trailer_onset + 30 \n",
    "\n",
    "        \"\"\" \n",
    "        For the questionnaire scores, as I understood it. If they were not inverted ([\"scale_flip\"] == 0), then \n",
    "        the lower the score the stronger the response. If they were inverted ([\"scale_flip\"] == 1), the higher the \n",
    "        score the stronger the response.\n",
    "        \"\"\"\n",
    "        # Check if scaled was flipped and put scores on the same scale. \n",
    "        # For me, the most intuitive is that the higher the score, the stronger the response. \n",
    "        trailer_watch_score = transform_scores(run_dataframe[\"exp_Watch.keys\"][id], run_dataframe[\"scale_flip\"][id])\n",
    "        trailer_arousal_score = transform_scores(run_dataframe[\"exp_Arousal.keys\"][id], run_dataframe[\"scale_flip\"][id])\n",
    "        trailer_feel_score = transform_scores(run_dataframe[\"exp_Feel.keys\"][id], run_dataframe[\"scale_flip\"][id])\n",
    "\n",
    "        # Place processed data on list, add list to new dataframe, and concat to main dataframe. \n",
    "        current_row_data = [[trailer_name, trailer_type, trailer_onset, trailer_duration, trailer_offset, trailer_watch_score, trailer_arousal_score, trailer_feel_score]]\n",
    "        current_row = pd.DataFrame(data=current_row_data, columns=[\"Trailer\", \"Type\", \"Onset\", \"Duration\", \"Offset\", \"W_score\", \"A_score\", \"F_score\"]) \n",
    "        proccesed_events_df = pd.concat([proccesed_events_df, current_row], ignore_index=True)\n",
    "        proccesed_events_df = proccesed_events_df[[\"Trailer\", \"Type\", \"Onset\", \"Offset\", \"Duration\", \"W_score\", \"A_score\", \"F_score\"]]\n",
    "\n",
    "    # Add a column for the mean centered arousal and valence scores.\n",
    "    mean_centered_arousal = proccesed_events_df[\"A_score\"]/proccesed_events_df[\"A_score\"].mean()\n",
    "    mean_centered_valence = proccesed_events_df[\"F_score\"]/proccesed_events_df[\"F_score\"].mean()\n",
    "\n",
    "    # Derive the positive and negative arousal scores.\n",
    "    proccesed_events_df[\"Pos_arousal\"] = (mean_centered_arousal+mean_centered_valence)/np.sqrt(2)\n",
    "    proccesed_events_df[\"Neg_arousal\"] = (mean_centered_arousal-mean_centered_valence)/np.sqrt(2)\n",
    "\n",
    "    # Derive the positive and negative arousal scaled scores. \n",
    "    pos_arousal_mean = proccesed_events_df[\"Pos_arousal\"].mean()\n",
    "    neg_arousal_mean = proccesed_events_df[\"Neg_arousal\"].mean()\n",
    "\n",
    "    pos_arousal_std = np.std(proccesed_events_df[\"Pos_arousal\"], axis=0, ddof=1)\n",
    "    neg_arousal_std = np.std(proccesed_events_df[\"Neg_arousal\"], axis=0, ddof=1)\n",
    "\n",
    "    proccesed_events_df[\"Pos_arousal_scaled\"] = (proccesed_events_df[\"Pos_arousal\"] - pos_arousal_mean)\n",
    "    proccesed_events_df[\"Neg_arousal_scaled\"] = (proccesed_events_df[\"Neg_arousal\"] - neg_arousal_mean)\n",
    "\n",
    "    # Derive scales Watch scores. \n",
    "    watch_score_mean = proccesed_events_df[\"W_score\"].mean()    \n",
    "    watch_score_std = np.std(proccesed_events_df[\"W_score\"], axis=0, ddof=1)\n",
    "    proccesed_events_df[\"W_score_scaled\"] = (proccesed_events_df[\"W_score\"] - watch_score_mean)\n",
    "\n",
    "    return proccesed_events_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def access_trailer(dictionary, trailer_key): \n",
    "    try:\n",
    "        trailer_data = dictionary[trailer_key]\n",
    "        return trailer_data\n",
    "    except KeyError:\n",
    "        pass    \n",
    "\n",
    "def access_timecourse(dictionary, roi_key):\n",
    "    \n",
    "    dictionary_keys = list(dictionary.keys())\n",
    "\n",
    "    if((roi_key in dictionary_keys) and (dictionary[roi_key] is not None) and (dictionary[roi_key] is not \"None\")):\n",
    "        roi_data = np.array(dictionary[roi_key])\n",
    "        return roi_data\n",
    "    \n",
    "    \n",
    "    # try:\n",
    "    #     print(dictionary)\n",
    "    #     if(dictionary[roi_key] == None):\n",
    "    #         print(\"This is empty!\")\n",
    "    #         print(dictionary[roi_key])\n",
    "    #     if(dictionary[roi_key] is not None):\n",
    "    #         roi_data = np.array(dictionary[roi_key])\n",
    "    #         return roi_data\n",
    "    \n",
    "    # except KeyError:\n",
    "    #     pass\n",
    "\n",
    "def trim_timecourse_per_roi(timecourses_dictionary_list, timecourses_dictionary_raw, ROI):\n",
    "\n",
    "    # 1) Create a new dictionary to store the trimmed timecourses.\n",
    "    # First file will store the avg timecourse for each trailer per roi (ROI x trailer).\n",
    "    # Second file will store the timecourse of all subjects for each trailer per roi (ROI x subjects x trailer).\n",
    "    # Third file will store the timecourse of all subjects for each trailer per roi in a dataframe (ROI x subjects x trailer).\n",
    "    all_subjects_avg_ROI_timecourse = {}\n",
    "    all_subjects_ROI_timecourse = {}\n",
    "    all_subjects_ROI_timecourse_df = {}\n",
    "\n",
    "    bROI = \"Bilateral_\" + ROI\n",
    "    bROI_r1 = \"Bilateral_\" + ROI + \"_r1\"\n",
    "    bROI_r2 = \"Bilateral_\" + ROI + \"_r2\"\n",
    "\n",
    "    # Get all keys from the first participant dictionary.\n",
    "    trailer_list = list(timecourses_dictionary_list[\"sub-01\"].keys())\n",
    "    subjects_list = list(timecourses_dictionary_raw.keys())\n",
    "\n",
    "    if(ROI != \"V1\"):\n",
    "        # Loop through all trailer keys.\n",
    "        for id in range(len(trailer_list)): \n",
    "    \n",
    "            # Create new participant list with only participants that have the current trailer.\n",
    "            participants_with_trailer = [d for d in timecourses_dictionary_list if trailer_list[id] in timecourses_dictionary_list[d]] # Store participant id if they have the current trailer.\n",
    "        \n",
    "            # Make copy of dictionary with only participants that have the current trailer.\n",
    "            timecourses_dictionary_list_current_trailer = {k: timecourses_dictionary_list[k] for k in participants_with_trailer}\n",
    "        \n",
    "            # Loop through all participants and get the values for the current trailer.\n",
    "            # Note, a wrapper function just to return the values for the current trailer.\n",
    "            # if they exist, otherwise return None.\n",
    "            values = [access_trailer(timecourses_dictionary_list_current_trailer[d], trailer_list[id]) for d in timecourses_dictionary_list_current_trailer] \n",
    "        \n",
    "            values_Bilateral_ROI = [values[d][bROI] for d in range(len(values))]\n",
    "            #values_Right_ROI = [values[d][rROI] for d in range(len(values))]\n",
    "            #values_Left_ROI = [values[d][lROI] for d in range(len(values))]\n",
    "\n",
    "            # Compute the average for each roi.\n",
    "            averageBilateral_ROI = np.mean(values_Bilateral_ROI, axis=0)\n",
    "            #averageRight_ROI = np.mean(values_Right_ROI, axis=0)\n",
    "            #averageLeft_ROI = np.mean(values_Left_ROI, axis=0)\n",
    "\n",
    "            # Row are timepoints, columns are participants (15 x P).\n",
    "            values_Bilateral_ROI_array = np.array(values_Bilateral_ROI).T \n",
    "            #values_Right_ROI_array = np.array(values_Right_ROI).T\n",
    "            #values_Left_ROI_array = np.array(values_Left_ROI).T\n",
    "\n",
    "            # Create columns names for the dataframe.\n",
    "            TRs = np.arange(0, 15, 1)\n",
    "\n",
    "            # Creat empty dataframe for current trailer. \n",
    "            trailer_df = pd.DataFrame(columns=[\"Participant\", \"ROI\", \"Signal\", \"TR\"])   \n",
    "\n",
    "            current_participant_dic = {}\n",
    "\n",
    "            for current_participant in range(len(participants_with_trailer)):\n",
    "\n",
    "                # Create dictionary with all the timecourses for all trailers for each subject.\n",
    "                current_participant_dic[participants_with_trailer[current_participant]] = {bROI: values_Bilateral_ROI_array[:, current_participant]\n",
    "                                                                             \n",
    "                                                                                       }\n",
    " \n",
    "                participant_col = np.repeat(str(participants_with_trailer[current_participant]), 15)\n",
    "                label_bi = np.repeat(bROI, 15)\n",
    "\n",
    "                roi_labels = [label_bi]\n",
    "\n",
    "                # Store current participant values. \n",
    "                current_participant_bi = values_Bilateral_ROI_array[:, current_participant]\n",
    "                current_participant_values = [current_participant_bi]\n",
    "\n",
    "                for x in range(1):\n",
    "\n",
    "                    current_participant_roi_data = {\n",
    "                        \"Participant\": participant_col,\n",
    "                        \"ROI\": roi_labels[x],\n",
    "                        \"Signal\": current_participant_values[x],\n",
    "                        \"TR\": TRs\n",
    "                    }\n",
    "            \n",
    "                    participant_df = pd.DataFrame(data=current_participant_roi_data)\n",
    "\n",
    "                    trailer_df = pd.concat([trailer_df, participant_df], ignore_index=True)\n",
    "        \n",
    "            # store the all in the results dictionaries.\n",
    "            all_subjects_avg_ROI_timecourse[trailer_list[id]] = {bROI: averageBilateral_ROI}\n",
    "            all_subjects_ROI_timecourse[trailer_list[id]] = current_participant_dic\n",
    "            all_subjects_ROI_timecourse_df[trailer_list[id]] = trailer_df\n",
    "    \n",
    "    # Get average timecourse for all subjects in the current ROI. \n",
    "    Bilateral_ROI_raw_r1_list = [access_timecourse(timecourses_dictionary_raw[sub_id], bROI_r1) for sub_id in subjects_list] # if timecourses_dictionary_raw[sub_id][bROI_r1] is not None\n",
    "    Bilateral_ROI_raw_r2_list = [access_timecourse(timecourses_dictionary_raw[sub_id], bROI_r2) for sub_id in subjects_list]  \n",
    "\n",
    "    # Remove None values from the list.\n",
    "    Bilateral_ROI_raw_r1_list = [x for x in Bilateral_ROI_raw_r1_list if x is not None]\n",
    "    Bilateral_ROI_raw_r2_list = [x for x in Bilateral_ROI_raw_r2_list if x is not None]\n",
    "\n",
    "    # Get the average timecourse for all subjects in the current ROI.\n",
    "    Bilateral_ROI_raw_r1_average = np.mean(Bilateral_ROI_raw_r1_list, axis=0)\n",
    "    Bilateral_ROI_raw_r2_average = np.mean(Bilateral_ROI_raw_r2_list, axis=0)\n",
    "\n",
    "    # Create TR array.\n",
    "    # Adjust this to the number of TRs in the timecourse.\n",
    "    TRs_raw = np.arange(0, 378-4, 1)\n",
    "\n",
    "    # Put everything in a list \n",
    "    ROI_raw_list = [Bilateral_ROI_raw_r1_average, Bilateral_ROI_raw_r2_average]\n",
    "    ROI_raw_labels = [bROI_r1, bROI_r2]\n",
    "\n",
    "    # Creat empty dataframe. \n",
    "    all_subjects_avg_ROI_timecourse_raw_df = pd.DataFrame(columns=[\"ROI\", \"Signal\", \"TR\"])  \n",
    "\n",
    "    for ROI_id in range(2):\n",
    "\n",
    "        current_roi_data = {        \n",
    "            \"ROI\": ROI_raw_labels[ROI_id],\n",
    "            \"Signal\": ROI_raw_list[ROI_id],\n",
    "            \"TR\": TRs_raw}\n",
    "            \n",
    "        current_roi_df = pd.DataFrame(data=current_roi_data)\n",
    "        \n",
    "        all_subjects_avg_ROI_timecourse_raw_df = pd.concat([all_subjects_avg_ROI_timecourse_raw_df, current_roi_df], ignore_index=True)\n",
    "\n",
    "\n",
    "    return all_subjects_avg_ROI_timecourse, all_subjects_ROI_timecourse, all_subjects_ROI_timecourse_df, all_subjects_avg_ROI_timecourse_raw_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from: https://bioimagesuiteweb.github.io/webapp/mni2tal.html\n",
    "# Note, I am not transforming the coordinates, instead I am using the MNI coordinates \n",
    "# that the site suggests correspond to each region.\n",
    "mni_coords = {\n",
    "    \"NAcc_RL\": [(10, 10, -12), (-11, 9, -11)],\n",
    "    \"MPFC_RL\": [(12, 37, -19), (-11, 38, -18)],\n",
    "    \"AIns_RL\": [(36, 18, -4), (-36, 18, 4)], # Not sure about this one.\n",
    "}\n",
    "\n",
    "# Taken from (Genevsky et al., 2017). -> NAcc, MPFC, AIns. \n",
    "tal_coords = {\n",
    "    \"NAcc_RL\": [(10, 12, -2), (-10, 12, -2)], # Nucleus accumbes (NAcc) right, left\n",
    "    \"MPFC_RL\": [(4, 45, 0), (-4, 45, 0)], # Medial prefrontal cortex (MPFC) right, left\n",
    "    \"AIns_RL\": [(34, 24, -4), (-34, 24, -4)] # Anterior Insula right, left\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAcc is binary mask with unique values:  [0. 1.]\n",
      "AIns is binary mask with unique values:  [0. 1.]\n",
      "vmPFC is binary mask with unique values:  [0. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Load masks from Plassman & Albuquerque study.\n",
    "NAcc_path = \"/Users/la/Documents/GitHub/RM_Thesis_Neuroforecasting/ROI_masks/NAc_bilateral.nii\"\n",
    "AIns_path = \"/Users/la/Documents/GitHub/RM_Thesis_Neuroforecasting/ROI_masks/AI_bilateral.nii\"\n",
    "vmPFC_path = \"/Users/la/Documents/GitHub/RM_Thesis_Neuroforecasting/ROI_masks/vmPFC_neurosynth_binary_resl.nii\"\n",
    "\n",
    "print(\"NAcc is binary mask with unique values: \", np.unique(nib.load(NAcc_path).get_fdata()))\n",
    "print(\"AIns is binary mask with unique values: \", np.unique(nib.load(AIns_path).get_fdata()))\n",
    "print(\"vmPFC is binary mask with unique values: \", np.unique(nib.load(vmPFC_path).get_fdata()))\n",
    "\n",
    "# Create mask objects.\n",
    "NAcc_masker = NiftiMasker(mask_img=NAcc_path, standardize=True, verbose=False).fit()\n",
    "AIns_masker = NiftiMasker(mask_img=AIns_path, standardize=True, verbose=False).fit()\n",
    "vmPFC_masker = NiftiMasker(mask_img=vmPFC_path, standardize=True, verbose=False).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getROIs_timecourse(participant_num): \n",
    "\n",
    "    #r1_keys = ['rh6', 'rc2', 'uh1', 'rc9', 'rh2', 'uc4', 'rh1', 'rc6', 'rh9', 'rc11', 'uh2', 'uc1', 'rh3', 'rc12', 'rh11', 'rc5']\n",
    "\n",
    "    #r2_keys = ['rh12', 'rc3', 'rh7', 'rc1', 'rh10', 'rc8', 'rh4', 'uc3', 'rh5', 'rc4', 'rh8', 'uc2', 'uh3', 'rc10', 'uh4', 'rc7']\n",
    "    \n",
    "    # Get the labels of each trailer for each run. \n",
    "    run1_trailer_labels = [\"rh6\", \"uh1\", \"rc2\", \"rh2\", \"rh1\", \"rc9\", \"uc4\", \"rh9\", \"uh2\", \"rc6\", \"rh3\", \"rc11\", \"uc1\", \"rh11\", \"rc12\", \"rc5\"]\n",
    "    \n",
    "    run2_trailer_labels = [\"rc3\", \"rh12\", \"rc1\", 'rh7', \"rc8\", \"uc3\", 'rh10', \"rc4\", \"rh4\", \"uc2\", \"rh5\", \"rh8\", \"rc10\", 'uh3', \"rc7\", \"uh4\"]\n",
    "\n",
    "    ## 1) Load data.\n",
    "    # Load relevant files for participant\n",
    "    sub_run1_func_path = \"MovieData_BIDS_preproc/sub-\" + participant_num + \"/func/sub-\" + participant_num + \"_task-movie_run-01_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz\"\n",
    "    sub_run2_func_path = \"MovieData_BIDS_preproc/sub-\" + participant_num + \"/func/sub-\" + participant_num + \"_task-movie_run-02_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz\"\n",
    "\n",
    "    sub_run1_func1_path = \"MovieData_BIDS_preproc/sub-\" + participant_num + \"/func/sub-\" + participant_num + \"_task-movie_run-01_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz\"\n",
    "    sub_run2_func1_path = \"MovieData_BIDS_preproc/sub-\" + participant_num + \"/func/sub-\" + participant_num + \"_task-movie_run-02_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz\"\n",
    "\n",
    "    sub_run1_mask_path = \"MovieData_BIDS_preproc/sub-\" + participant_num + \"/func/sub-\" + participant_num + \"_task-movie_run-01_space-MNI152NLin2009cAsym_desc-brain_mask.nii.gz\"\n",
    "    sub_run2_mask_path = \"MovieData_BIDS_preproc/sub-\" + participant_num + \"/func/sub-\" + participant_num + \"_task-movie_run-02_space-MNI152NLin2009cAsym_desc-brain_mask.nii.gz\"\n",
    "\n",
    "    sub_events_path = \"/Users/la/Documents/GitHub/RM_Thesis_Neuroforecasting/ProcessedData/sub-\" + participant_num + \"/sub-\" + participant_num + \"_processed_events.csv\"\n",
    "\n",
    "    sub_run1_confounds_path = \"MovieData_BIDS_preproc/sub-\" + participant_num + \"/func/sub-\" + participant_num + \"_task-movie_run-01_desc-confounds_timeseries.tsv\"\n",
    "    sub_run2_confounds_path = \"MovieData_BIDS_preproc/sub-\" + participant_num + \"/func/sub-\" + participant_num + \"_task-movie_run-02_desc-confounds_timeseries.tsv\"\n",
    "\n",
    "    sub_run1_confounds_meta_path = \"MovieData_BIDS_preproc/sub-\" + participant_num + \"/func/sub-\" + participant_num + \"_task-movie_run-01_desc-confounds_timeseries.json\"\n",
    "    sub_run2_confounds_meta_path = \"MovieData_BIDS_preproc/sub-\" + participant_num + \"/func/sub-\" + participant_num + \"_task-movie_run-02_desc-confounds_timeseries.json\"\n",
    "\n",
    "    # Load processed events files. \n",
    "    sub_events_df = pd.read_csv(sub_events_path)\n",
    "\n",
    "    r1_vector = sub_events_df['Trailer'].isin(run1_trailer_labels)\n",
    "    sub_run1_p_events = sub_events_df[r1_vector]\n",
    "    sub_run1_p_events.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    r2_vector = sub_events_df['Trailer'].isin(run2_trailer_labels)\n",
    "    sub_run2_p_events = sub_events_df[r2_vector]\n",
    "    sub_run2_p_events.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Calculate relevant parameters for GLM and ROI time-course analysis.\n",
    "    func_file = nib.load(sub_run1_func_path)\n",
    "    func_data = func_file.get_fdata()\n",
    "    n_vols = func_data.shape[3]\n",
    "    TR = 2\n",
    "    n_timepoints = n_vols*TR\n",
    "\n",
    "    # Drop the first 4 TRs from the timecourse.\n",
    "    sub_run1_func_clean = nimg.load_img(sub_run1_func_path)\n",
    "    sub_run1_func_clean = sub_run1_func_clean.slicer[:,:,:,4:]\n",
    "\n",
    "    sub_run2_func_clean = nimg.load_img(sub_run2_func_path)\n",
    "    sub_run2_func_clean = sub_run2_func_clean.slicer[:,:,:,4:]\n",
    "\n",
    "    # Down-sample time onsets to get vol onsets. \n",
    "    # Create array from 0 to 'n_timepoints' in steps of 1.\n",
    "    time_scale = np.arange(0, n_timepoints, 1)  \n",
    "\n",
    "    # Create array from 0 to 'n_timepoints' in steps of 2.\n",
    "    vol_scale = np.arange(0, n_timepoints, TR)  \n",
    "\n",
    "    # Re-compute vol scale adjusting for the 4 TRs that were dropped.\n",
    "    vol_scale = np.arange(0, n_timepoints-8, TR)  \n",
    "\n",
    "    #run1_onsets_array = np.array([12, 60, 108, 152, 196, 244, 288, 332, 376, 420, 468, 512, 560, 608, 656, 700])\n",
    "    #run2_onsets_array = np.array([12, 60, 108, 152, 200, 244, 288, 332, 380, 428, 476, 520, 568, 616, 660, 704])\n",
    "    \n",
    "    # For this onsets, I re-calculated them using ITI info.\n",
    "    run1_onsets_array = np.array([12, 60, 108, 156, 200, 244, 292, 336, 380, 424, 468, 516, 560, 608, 656, 704])   \n",
    "    run2_onsets_array = np.array([12, 56, 104, 152, 196, 244, 288, 332, 376, 424, 472, 520, 564, 612, 660, 704])\n",
    "\n",
    "    # Create dictionary variable to store arrays with onset values for each trailer. \n",
    "    run1_onsets = {}\n",
    "    run1_watch_onsets_dic = {}\n",
    "    run1_feel_onsets_dic = {}\n",
    "    run1_arousal_onsets_dic = {}\n",
    "    run2_onsets = {}\n",
    "    run2_watch_onsets_dic = {}\n",
    "    run2_feel_onsets_dic = {}\n",
    "    run2_arousal_onsets_dic = {}\n",
    "\n",
    "    # Create a dictionary with all the onsets for each trailer in each run. \n",
    "    for trailer_id in range(16):\n",
    "\n",
    "        # Add onsets for each trailer in each run.\n",
    "        sub_run1_p_events[\"Onset\"] = run1_onsets_array[trailer_id]\n",
    "        sub_run2_p_events[\"Onset\"] = run2_onsets_array[trailer_id]\n",
    "\n",
    "        # Create array of zeros.\n",
    "        run1_trailer_onsets = np.zeros(n_timepoints)\n",
    "        run2_trailer_onsets = np.zeros(n_timepoints)\n",
    "        run1_watch_onsets = np.zeros(n_timepoints)\n",
    "        run2_watch_onsets = np.zeros(n_timepoints)\n",
    "        run1_feel_onsets = np.zeros(n_timepoints)\n",
    "        run2_feel_onsets = np.zeros(n_timepoints)\n",
    "        run1_arousal_onsets = np.zeros(n_timepoints)\n",
    "        run2_arousal_onsets = np.zeros(n_timepoints)\n",
    "\n",
    "        # Get onset time. \n",
    "        # Since I am dropping the first 4 TR, I need to subsctract 8 from the onset time.\n",
    "        #run1_current_trailer_onset = sub_run1_p_events[\"Onset\"][trailer_id] - 8\n",
    "        #run2_current_trailer_onset = sub_run2_p_events[\"Onset\"][trailer_id] - 8 \n",
    "        run1_current_trailer_onset = run1_onsets_array[trailer_id] - 8\n",
    "        run2_current_trailer_onset = run2_onsets_array[trailer_id] - 8\n",
    "        \n",
    "        # Add 30 sec for trailer duration.\n",
    "        # Add 4 sec for watch question duration.\n",
    "        # Add 4 sec for feel question duration.\n",
    "        run1_current_trailer_w_onset = run1_onsets_array[trailer_id] - 8 + 30\n",
    "        run2_current_trailer_w_onset = run2_onsets_array[trailer_id] - 8 + 30\n",
    "        run1_current_trailer_f_onset = run1_onsets_array[trailer_id] - 8 + 30 + 4\n",
    "        run2_current_trailer_f_onset = run2_onsets_array[trailer_id] - 8 + 30 + 4\n",
    "        run1_current_trailer_a_onset = run1_onsets_array[trailer_id] - 8 + 30 + 4 + 4\n",
    "        run2_current_trailer_a_onset = run2_onsets_array[trailer_id] - 8 + 30 + 4 + 4\n",
    "\n",
    "        # Assign 1 to such onset all the way til the end of the trailer (30 sec) in the array of zeros.\n",
    "        # Adjust for lag: add 4 seconds at the onset and offset\n",
    "        # Let's add 4 seconds to the onset and offset to account for the lag in the BOLD signal.\n",
    "        HMD_lag = 4\n",
    "        run1_trailer_onsets[int(run1_current_trailer_onset + HMD_lag):int(run1_current_trailer_onset)+ 30 + HMD_lag] = 1\n",
    "        run2_trailer_onsets[int(run2_current_trailer_onset + HMD_lag):int(run2_current_trailer_onset)+ 30 + HMD_lag] = 1\n",
    "        run1_watch_onsets[int(run1_current_trailer_w_onset + HMD_lag):int(run1_current_trailer_w_onset)+ 4 + HMD_lag] = 1\n",
    "        run2_watch_onsets[int(run2_current_trailer_w_onset + HMD_lag):int(run2_current_trailer_w_onset)+ 4 + HMD_lag] = 1\n",
    "        run1_feel_onsets[int(run1_current_trailer_f_onset + HMD_lag):int(run1_current_trailer_f_onset)+ 4 + HMD_lag] = 1\n",
    "        run2_feel_onsets[int(run2_current_trailer_f_onset + HMD_lag):int(run2_current_trailer_f_onset)+ 4 + HMD_lag] = 1\n",
    "        run1_arousal_onsets[int(run1_current_trailer_a_onset + HMD_lag):int(run1_current_trailer_a_onset)+ 4 + HMD_lag] = 1\n",
    "        run2_arousal_onsets[int(run2_current_trailer_a_onset + HMD_lag):int(run2_current_trailer_a_onset)+ 4 + HMD_lag] = 1\n",
    "\n",
    "        # Create resampler objects for each trailer/run of reward.\n",
    "        run1_resampler = interp1d(time_scale, run1_trailer_onsets)\n",
    "        run2_resampler = interp1d(time_scale, run2_trailer_onsets)\n",
    "        run1_w_resampler = interp1d(time_scale, run1_watch_onsets)\n",
    "        run2_w_resampler = interp1d(time_scale, run2_watch_onsets)\n",
    "        run1_f_resampler = interp1d(time_scale, run1_feel_onsets)\n",
    "        run2_f_resampler = interp1d(time_scale, run2_feel_onsets)\n",
    "        run1_a_resampler = interp1d(time_scale, run1_arousal_onsets)\n",
    "        run2_a_resampler = interp1d(time_scale, run2_arousal_onsets)\n",
    "\n",
    "        # Create downsampled arrays for each trailer. \n",
    "        # Note this vol arrays are half the length than the time arrays.\n",
    "        run1_trailer_vol_onsets = run1_resampler(vol_scale)\n",
    "        run2_trailer_vol_onsets = run2_resampler(vol_scale)\n",
    "        run1_watch_vol_onsets = run1_w_resampler(vol_scale)\n",
    "        run2_watch_vol_onsets = run2_w_resampler(vol_scale)\n",
    "        run1_feel_vol_onsets = run1_f_resampler(vol_scale)\n",
    "        run2_feel_vol_onsets = run2_f_resampler(vol_scale)\n",
    "        run1_arousal_vol_onsets = run1_a_resampler(vol_scale)\n",
    "        run2_arousal_vol_onsets = run2_a_resampler(vol_scale)\n",
    "\n",
    "        # Append/store the downsampled volumes arrays to each dictionary.\n",
    "        # I'm doing it this way, so the code is more interpretable\n",
    "        run1_onsets[run1_trailer_labels[trailer_id]] = run1_trailer_vol_onsets\n",
    "        run2_onsets[run2_trailer_labels[trailer_id]] = run2_trailer_vol_onsets\n",
    "        run1_watch_onsets_dic[run1_trailer_labels[trailer_id]] = run1_watch_vol_onsets\n",
    "        run2_watch_onsets_dic[run2_trailer_labels[trailer_id]] = run2_watch_vol_onsets\n",
    "        run1_feel_onsets_dic[run1_trailer_labels[trailer_id]] = run1_feel_vol_onsets\n",
    "        run2_feel_onsets_dic[run2_trailer_labels[trailer_id]] = run2_feel_vol_onsets\n",
    "        run1_arousal_onsets_dic[run1_trailer_labels[trailer_id]] = run1_arousal_vol_onsets\n",
    "        run2_arousal_onsets_dic[run2_trailer_labels[trailer_id]] = run2_arousal_vol_onsets\n",
    "\n",
    "    ## 3) Load confound data. \n",
    "    sub_run1_confounds_df = pd.read_csv(sub_run1_confounds_path, sep='\\t')\n",
    "    sub_run2_confounds_df = pd.read_csv(sub_run2_confounds_path, sep='\\t')\n",
    "\n",
    "    simple_confounds = [#'w_comp_cor_00', 'w_comp_cor_01', 'w_comp_cor_02', #'w_comp_cor_03', 'w_comp_cor_04',\n",
    "                        \"rot_x\", \"rot_y\", \"rot_z\", 'trans_x', \"trans_y\", \"trans_z\",\n",
    "                        #'trans_x_derivative1', 'trans_y_derivative1', 'trans_z_derivative1',\n",
    "                        #'rot_x_derivative1', 'rot_y_derivative1', 'rot_z_derivative1',\n",
    "\n",
    "                        #'trans_x_power2', 'trans_y_power2', 'trans_z_power2', \n",
    "                        #'rot_x_power2', 'rot_y_power2', 'rot_z_power2',\n",
    "                        #'w_comp_cor_00', 'w_comp_cor_01', 'w_comp_cor_02', 'w_comp_cor_03', 'w_comp_cor_04', \n",
    "                        #\"white_matter\", \"csf\",\n",
    "                        \"white_matter\", \"csf\", \"csf_wm\", \n",
    "                        #\"global_signal\", \n",
    "                        #'a_comp_cor_00', 'a_comp_cor_01', 'a_comp_cor_02', #'a_comp_cor_03', #'a_comp_cor_04', \n",
    "                        #'t_comp_cor_00', #'t_comp_cor_01',\n",
    "                        \"framewise_displacement\", \n",
    "                        #\"dvars\", \n",
    "                        #\"std_dvars\", \n",
    "                        #\"rmsd\", \n",
    "                        #\"tcompcor\", \n",
    "                        #'cosine00', 'cosine01', 'cosine02', 'cosine03', 'cosine04', 'cosine05', 'cosine06',\n",
    "                        #'cosine07', 'cosine08', 'cosine09'\n",
    "                        #\"dvars\", \"rmsd\", \n",
    "                        #'a_comp_cor_00', 'a_comp_cor_01', 'a_comp_cor_02', 'a_comp_cor_03', 'a_comp_cor_04', #'a_comp_cor_05', \n",
    "                        #'t_comp_cor_00',\n",
    "                        ]\n",
    "\n",
    "    # Get motion parameters.\n",
    "    sub_run1_motion_s_confounds = [i for i in sub_run1_confounds_df.columns if \"state\" in i] \n",
    "    sub_run2_motion_s_confounds = [i for i in sub_run2_confounds_df.columns if \"state\" in i] \n",
    "\n",
    "    sub_run1_motion_rot_confounds = [i for i in sub_run1_confounds_df.columns if \"rot\" in i] \n",
    "    sub_run2_motion_rot_confounds = [i for i in sub_run2_confounds_df.columns if \"rot\" in i] \n",
    "\n",
    "    sub_run1_motion_trans_confounds = [i for i in sub_run1_confounds_df.columns if \"trans\" in i] \n",
    "    sub_run2_motion_trans_confounds = [i for i in sub_run2_confounds_df.columns if \"trans\" in i] \n",
    "\n",
    "    sub_run1_motion_outliers_confounds = [i for i in sub_run1_confounds_df.columns if \"motion\" in i]\n",
    "    sub_run2_motion_outliers_confounds = [i for i in sub_run2_confounds_df.columns if \"motion\" in i]\n",
    "\n",
    "    sub_run1_cosines_confounds = [i for i in sub_run1_confounds_df.columns if \"cosine\" in i]\n",
    "    sub_run2_cosines_confounds = [i for i in sub_run2_confounds_df.columns if \"cosine\" in i]\n",
    "\n",
    "    ## Load sample_mask to censor high motion volumes. \n",
    "    confounds_run1, sample_mask_run1 = load_confounds(sub_run1_func1_path, strategy=[\"motion\", \"high_pass\", \"scrub\"], motion=\"power2\", fd_threshold=0.30, scrub=0) #std_dvars_threshold=2,\n",
    "    confounds_run2, sample_mask_run2 = load_confounds(sub_run2_func1_path, strategy=[\"motion\", \"high_pass\", \"scrub\"], motion=\"power2\", fd_threshold=0.30, scrub=0) #std_dvars_threshold=2,\n",
    "\n",
    "    # Adjust mask to match the new data range\n",
    "    sample_mask_run1 = sample_mask_run1[sample_mask_run1 >= 4] - 4\n",
    "    sample_mask_run2 = sample_mask_run2[sample_mask_run2 >= 4] - 4\n",
    "\n",
    "    # Index the confounds selection.\n",
    "    sub_run1_filtered_confounds_df = sub_run1_confounds_df[simple_confounds] \n",
    "    sub_run2_filtered_confounds_df = sub_run2_confounds_df[simple_confounds] \n",
    "\n",
    "    # Change NaNs to 0s. \n",
    "    sub_run1_filtered_confounds_df = sub_run1_filtered_confounds_df.fillna(0) \n",
    "    sub_run2_filtered_confounds_df = sub_run2_filtered_confounds_df.fillna(0) \n",
    "\n",
    "    # Reset index.\n",
    "    sub_run1_filtered_confounds_df.reset_index(drop=True, inplace=True)\n",
    "    sub_run2_filtered_confounds_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Drop the first 4 TRs from the confounds.\n",
    "    sub_run1_filtered_confounds_df = sub_run1_filtered_confounds_df.iloc[4:]\n",
    "    sub_run2_filtered_confounds_df = sub_run2_filtered_confounds_df.iloc[4:]\n",
    "\n",
    "    # Declare parameters for Sphere Masker objects. \n",
    "    detrend = False\n",
    "    standardize = \"psc\"\n",
    "    standardize_confounds = \"psc\" \n",
    "    sphere_radius = 8 \n",
    "    confounds1 = sub_run1_filtered_confounds_df\n",
    "    confounds2 = sub_run2_filtered_confounds_df\n",
    "    smoothing= 4 \n",
    "\n",
    "    ROI_raw_timecourses = {}\n",
    "    ROI_raw_timecourses_zscored = {}\n",
    "\n",
    "    # Test different masks. \n",
    "    NAcc_masker_resampled_r1 = resample_to_img(NAcc_path, sub_run1_func_clean, interpolation='nearest')\n",
    "    AIns_masker_resampled_r1 = resample_to_img(AIns_path, sub_run1_func_clean, interpolation='nearest')\n",
    "    vmPFC_masker_resampled_r1 = resample_to_img(vmPFC_path, sub_run1_func_clean, interpolation='nearest')\n",
    "\n",
    "    NAcc_masker_resampled_r2 = resample_to_img(NAcc_path, sub_run2_func_clean, interpolation='nearest')\n",
    "    AIns_masker_resampled_r2 = resample_to_img(AIns_path, sub_run2_func_clean, interpolation='nearest')\n",
    "    vmPFC_masker_resampled_r2 = resample_to_img(vmPFC_path, sub_run2_func_clean, interpolation='nearest')\n",
    "\n",
    "    NAcc_masker_r1 = NiftiMasker(mask_img=NAcc_masker_resampled_r1, t_r=2, standardize=standardize, smoothing_fwhm=smoothing, standardize_confounds=standardize_confounds, detrend=detrend, high_pass= 1/360, verbose=False).fit()\n",
    "    AIns_masker_r1 = NiftiMasker(mask_img=AIns_masker_resampled_r1, t_r=2, standardize=standardize, smoothing_fwhm=smoothing, standardize_confounds=standardize_confounds, detrend=detrend, high_pass= 1/360, verbose=False).fit()\n",
    "    vmPFC_masker_r1 = NiftiMasker(mask_img=vmPFC_masker_resampled_r1, t_r=2, standardize=standardize, smoothing_fwhm=smoothing, standardize_confounds=standardize_confounds, detrend=detrend, high_pass= 1/360, verbose=False).fit()\n",
    "\n",
    "    NAcc_masker_r2 = NiftiMasker(mask_img=NAcc_masker_resampled_r2, t_r=2, standardize=standardize, smoothing_fwhm=smoothing, standardize_confounds=standardize_confounds, detrend=detrend, high_pass= 1/360, verbose=False).fit()\n",
    "    AIns_masker_r2 = NiftiMasker(mask_img=AIns_masker_resampled_r2, t_r=2, standardize=standardize, smoothing_fwhm=smoothing, standardize_confounds=standardize_confounds, detrend=detrend, high_pass= 1/360, verbose=False).fit()\n",
    "    vmPFC_masker_r2 = NiftiMasker(mask_img=vmPFC_masker_resampled_r2, t_r=2, standardize=standardize, smoothing_fwhm=smoothing, standardize_confounds=standardize_confounds, detrend=detrend, high_pass= 1/360, verbose=False).fit()\n",
    "\n",
    "    # Mask the func data and get a time series for the ROI. \n",
    "    # Note this is similar to fitting the GLM, but without the event files.\n",
    "\n",
    "    # Apply function to get the percent signal change from each ROI timecourse. \n",
    "    sub_r1_NAcc_ROI = NAcc_masker_r1.fit_transform(sub_run1_func_clean, confounds=confounds1)   \n",
    "    sub_r1_AIns_ROI = AIns_masker_r1.fit_transform(sub_run1_func_clean, confounds=confounds1)\n",
    "    sub_r1_vmPFC_ROI = vmPFC_masker_r1.fit_transform(sub_run1_func_clean, confounds=confounds1) \n",
    "\n",
    "    sub_r1_NAcc_ROI = process_censored_vols2(sub_r1_NAcc_ROI, sample_mask_run1, 374) \n",
    "    sub_r1_AIns_ROI = process_censored_vols2(sub_r1_AIns_ROI, sample_mask_run1, 374)\n",
    "    sub_r1_vmPFC_ROI = process_censored_vols2(sub_r1_vmPFC_ROI, sample_mask_run1, 374)\n",
    "\n",
    "    sub_r1_AIM_ROI = np.concatenate((sub_r1_NAcc_ROI, sub_r1_AIns_ROI, sub_r1_vmPFC_ROI), axis=1)\n",
    "    sub_r1_AIM_ROI_zscored = zscore(sub_r1_AIM_ROI, axis=1, nan_policy='omit')\n",
    "\n",
    "    ## 5) Get the timecourses from each movie trailer. \n",
    "    # Create dictionary variable to store arrays with time series arrays for each trailer.\n",
    "    run1_timeseries = {}\n",
    "    run1_timeseries_zcored = {}\n",
    "        \n",
    "    # Get the trailers presented in each run. \n",
    "    r1_keys = list(run1_onsets.keys())\n",
    "\n",
    "    # Loop through each trailer and get its corresponding ROI timecourse\n",
    "    for id in range(len(r1_keys)):\n",
    "\n",
    "        run1_timeseries[r1_keys[id]] = {\n",
    "                \"Bilateral_NAcc\": sub_r1_AIM_ROI[run1_onsets[r1_keys[id]].astype(bool)][:, 0],\n",
    "                \"Bilateral_AIns\": sub_r1_AIM_ROI[run1_onsets[r1_keys[id]].astype(bool)][:, 1],\n",
    "                \"Bilateral_MPFC\": sub_r1_AIM_ROI[run1_onsets[r1_keys[id]].astype(bool)][:, 2], \n",
    "                \"Bilateral_NAcc_watch\": sub_r1_AIM_ROI[run1_watch_onsets_dic[r1_keys[id]].astype(bool)][:, 0], \n",
    "                \"Bilateral_AIns_watch\": sub_r1_AIM_ROI[run1_watch_onsets_dic[r1_keys[id]].astype(bool)][:, 1], \n",
    "                \"Bilateral_MPFC_watch\": sub_r1_AIM_ROI[run1_watch_onsets_dic[r1_keys[id]].astype(bool)][:, 2],\n",
    "                \"Bilateral_NAcc_feel\": sub_r1_AIM_ROI[run1_feel_onsets_dic[r1_keys[id]].astype(bool)][:, 0], \n",
    "                \"Bilateral_AIns_feel\": sub_r1_AIM_ROI[run1_feel_onsets_dic[r1_keys[id]].astype(bool)][:, 1], \n",
    "                \"Bilateral_MPFC_feel\": sub_r1_AIM_ROI[run1_feel_onsets_dic[r1_keys[id]].astype(bool)][:, 2], \n",
    "                \"Bilateral_NAcc_arousal\": sub_r1_AIM_ROI[run1_arousal_onsets_dic[r1_keys[id]].astype(bool)][:, 0],\n",
    "                \"Bilateral_AIns_arousal\": sub_r1_AIM_ROI[run1_arousal_onsets_dic[r1_keys[id]].astype(bool)][:, 1], \n",
    "                \"Bilateral_MPFC_arousal\": sub_r1_AIM_ROI[run1_arousal_onsets_dic[r1_keys[id]].astype(bool)][:, 2]}\n",
    "            \n",
    "        run1_timeseries_zcored[r1_keys[id]] = {\n",
    "                \"Bilateral_NAcc_zscored\": sub_r1_AIM_ROI_zscored[run1_onsets[r1_keys[id]].astype(bool)][:, 0],\n",
    "                \"Bilateral_AIns_zscored\": sub_r1_AIM_ROI_zscored[run1_onsets[r1_keys[id]].astype(bool)][:, 1], \n",
    "                \"Bilateral_MPFC_zscored\": sub_r1_AIM_ROI_zscored[run1_onsets[r1_keys[id]].astype(bool)][:, 2]\n",
    "               }\n",
    "        \n",
    "    # This code gives me the un-trimmed timecourses for each ROI. \n",
    "    ROI_raw_timecourses[\"Bilateral_NAcc_r1\"] = sub_r1_AIM_ROI[:, 0]\n",
    "    ROI_raw_timecourses[\"Bilateral_AIns_r1\"] = sub_r1_AIM_ROI[:, 1]\n",
    "    ROI_raw_timecourses[\"Bilateral_MPFC_r1\"] = sub_r1_AIM_ROI[:, 2]\n",
    "\n",
    "    # Get the same un-trimmed timecourses but z-scored.\n",
    "    ROI_raw_timecourses_zscored[\"Bilateral_NAcc_r1\"] = sub_r1_AIM_ROI_zscored[:, 0]\n",
    "    ROI_raw_timecourses_zscored[\"Bilateral_AIns_r1\"] = sub_r1_AIM_ROI_zscored[:, 1]\n",
    "    ROI_raw_timecourses_zscored[\"Bilateral_MPFC_r1\"] = sub_r1_AIM_ROI_zscored[:, 2]\n",
    "\n",
    "    # Apply for second run.\n",
    "    # Apply function to get the percent signal change from each ROI timecourse.\n",
    "    #sub_r2_AIM_ROI = masker_AIM_ROI_r2.fit_transform(sub_run2_func_clean, confounds=confounds2)\n",
    "    sub_r2_NAcc_ROI = NAcc_masker_r2.fit_transform(sub_run2_func_clean, confounds=confounds2)\n",
    "    sub_r2_AIns_ROI = AIns_masker_r2.fit_transform(sub_run2_func_clean, confounds=confounds2)\n",
    "    sub_r2_vmPFC_ROI = vmPFC_masker_r2.fit_transform(sub_run2_func_clean, confounds=confounds2)\n",
    "\n",
    "    sub_r2_NAcc_ROI = process_censored_vols2(sub_r2_NAcc_ROI, sample_mask_run2, 374)\n",
    "    sub_r2_AIns_ROI = process_censored_vols2(sub_r2_AIns_ROI, sample_mask_run2, 374)\n",
    "    sub_r2_vmPFC_ROI = process_censored_vols2(sub_r2_vmPFC_ROI, sample_mask_run2, 374)\n",
    "    \n",
    "    sub_r2_AIM_ROI = np.concatenate((sub_r2_NAcc_ROI, sub_r2_AIns_ROI, sub_r2_vmPFC_ROI), axis=1)\n",
    "    sub_r2_AIM_ROI_zscored = zscore(sub_r2_AIM_ROI, axis=1, nan_policy='omit')\n",
    "\n",
    "    # Create dictionary variable to store arrays with time series arrays for each trailer.\n",
    "    run2_timeseries = {}\n",
    "    run2_timeseries_zcored = {}\n",
    "\n",
    "    r2_keys = list(run2_onsets.keys())\n",
    "\n",
    "    # Loop through each traile and get its corresponding ROI timecourse\n",
    "    for id in range(len(r2_keys)):\n",
    "        run2_timeseries[r2_keys[id]] = {\n",
    "                \"Bilateral_NAcc\": sub_r2_AIM_ROI[run2_onsets[r2_keys[id]].astype(bool)][:, 0],\n",
    "                \"Bilateral_AIns\": sub_r2_AIM_ROI[run2_onsets[r2_keys[id]].astype(bool)][:, 1], \n",
    "                \"Bilateral_MPFC\": sub_r2_AIM_ROI[run2_onsets[r2_keys[id]].astype(bool)][:, 2], \n",
    "                \"Bilateral_NAcc_watch\": sub_r2_AIM_ROI[run2_watch_onsets_dic[r2_keys[id]].astype(bool)][:, 0], \n",
    "                \"Bilateral_AIns_watch\": sub_r2_AIM_ROI[run2_watch_onsets_dic[r2_keys[id]].astype(bool)][:, 1], \n",
    "                \"Bilateral_MPFC_watch\": sub_r2_AIM_ROI[run2_watch_onsets_dic[r2_keys[id]].astype(bool)][:, 2], \n",
    "                \"Bilateral_NAcc_feel\": sub_r2_AIM_ROI[run2_feel_onsets_dic[r2_keys[id]].astype(bool)][:, 0], \n",
    "                \"Bilateral_AIns_feel\": sub_r2_AIM_ROI[run2_feel_onsets_dic[r2_keys[id]].astype(bool)][:, 1],\n",
    "                \"Bilateral_MPFC_feel\": sub_r2_AIM_ROI[run2_feel_onsets_dic[r2_keys[id]].astype(bool)][:, 2], \n",
    "                \"Bilateral_NAcc_arousal\": sub_r2_AIM_ROI[run2_arousal_onsets_dic[r2_keys[id]].astype(bool)][:, 0], \n",
    "                \"Bilateral_AIns_arousal\": sub_r2_AIM_ROI[run2_arousal_onsets_dic[r2_keys[id]].astype(bool)][:, 1], \n",
    "                \"Bilateral_MPFC_arousal\": sub_r2_AIM_ROI[run2_arousal_onsets_dic[r2_keys[id]].astype(bool)][:, 2]}\n",
    "\n",
    "        run2_timeseries_zcored[r2_keys[id]] = {\n",
    "                \"Bilateral_NAcc_zscored\": sub_r2_AIM_ROI_zscored[run2_onsets[r2_keys[id]].astype(bool)][:, 0],\n",
    "                \"Bilateral_AIns_zscored\": sub_r2_AIM_ROI_zscored[run2_onsets[r2_keys[id]].astype(bool)][:, 1],\n",
    "                \"Bilateral_MPFC_zscored\": sub_r2_AIM_ROI_zscored[run2_onsets[r2_keys[id]].astype(bool)][:, 2], \n",
    "                }\n",
    "        \n",
    "    # This code gives me the un-trimmed timecourses for each ROI.\n",
    "    ROI_raw_timecourses[\"Bilateral_NAcc_r2\"] = sub_r2_AIM_ROI[:, 0]\n",
    "    ROI_raw_timecourses[\"Bilateral_AIns_r2\"] = sub_r2_AIM_ROI[:, 1]\n",
    "    ROI_raw_timecourses[\"Bilateral_MPFC_r2\"] = sub_r2_AIM_ROI[:, 2]\n",
    "\n",
    "    # Get the same un-trimmed timecourses but z-scored.\n",
    "    ROI_raw_timecourses_zscored[\"Bilateral_NAcc_r2\"] = sub_r2_AIM_ROI_zscored[:, 0]\n",
    "    ROI_raw_timecourses_zscored[\"Bilateral_AIns_r2\"] = sub_r2_AIM_ROI_zscored[:, 1]\n",
    "    ROI_raw_timecourses_zscored[\"Bilateral_MPFC_r2\"] = sub_r2_AIM_ROI_zscored[:, 2]\n",
    "\n",
    "    ## 6) Merge timecourses and events data. \n",
    "    # Merge the timecourses for both runs.\n",
    "    all_timeseries = merge_dictionaries(run1_timeseries, run2_timeseries) \n",
    "    all_timeseries_zcored = merge_dictionaries(run1_timeseries_zcored, run2_timeseries_zcored)  \n",
    "    sub_all_p_events = pd.concat([sub_run1_p_events, sub_run2_p_events], ignore_index=True)\n",
    "    \n",
    "    # Create dictionary to store all the relevant output. \n",
    "    Output_dic = { \n",
    "        \"ROI_trimmed_timeseries\": all_timeseries, \n",
    "        \"ROI_trimmed_timeseries_zcored\": all_timeseries_zcored,\n",
    "        \"Processes_events\": sub_all_p_events, \n",
    "        \"ROI_raw_timeseries\": ROI_raw_timecourses,\n",
    "        \"ROI_raw_timeseries_zscored\": ROI_raw_timecourses_zscored\n",
    "    }\n",
    "\n",
    "    return Output_dic\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get neural activation time courses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working dir : /Users/la/Documents/Datasets\n"
     ]
    }
   ],
   "source": [
    "# Open a datasets directory. \n",
    "fd = os.open(\"/Users/la/Documents/Datasets\", os.O_RDONLY)\n",
    "\n",
    "# Use os.fchdir() method to change the current dir/folder.\n",
    "os.fchdir(fd)\n",
    "\n",
    "# Safe check- Print current working directory\n",
    "print(\"Current working dir : %s\" % os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define each subject motion parameters from fMRIprep.\n",
    "# The first value is the max framewise displacement, the second is the mean framewise displacement.\n",
    "# Two values per run. \n",
    "sub01_motion = [0.911, 0.095, 0.571, 0.077]\n",
    "sub02_motion = [0.663, 0.073, 0.734, 0.108]\n",
    "sub03_motion = [0.456, 0.048, 0.300, 0.049]\n",
    "sub04_motion = [0.607, 0.080, 0.519, 0.076]\n",
    "#sub05_motion = [1.612, 0.138, 1.403, 0.208] # out both\n",
    "sub05_motion = [2.0, 2.0, 2.0, 2.0] # out both\n",
    "#sub06_motion = [3.008, 0.248, 4.225, 0.311] # out both\n",
    "sub06_motion = [2.0, 2.0, 2.0, 2.0]\n",
    "#sub07_motion = [8.275, 0.156, 0.531, 0.072] # out r1\n",
    "sub07_motion = [2.0, 2.0, 2.0, 2.0]\n",
    "sub08_motion = [0.597, 0.074, 0.482, 0.072] \n",
    "sub09_motion = [0.367, 0.069, 0.475, 0.104]\n",
    "sub10_motion = [0.597, 0.074, 0.679, 0.103]\n",
    "sub11_motion = [0.540, 0.059, 0.394, 0.055]\n",
    "sub12_motion = [0.649, 0.154, 0.529, 0.162]\n",
    "sub13_motion = [0.468, 0.091, 0.459, 0.102]\n",
    "#sub14_motion = [2.538, 0.164, 1.245, 0.091] # out both\n",
    "sub14_motion = [2.0, 2.0, 2.0, 2.0]\n",
    "sub15_motion = [0.844, 0.066, 1.163, 0.062] # out r2\n",
    "#sub16_motion = [0.688, 0.111, 3.026, 0.218] # out r2\n",
    "sub16_motion = [2.0, 2.0, 2.0, 2.0]\n",
    "sub17_motion = [0.626, 0.091, 0.376, 0.072]\n",
    "sub18_motion = [0.777, 0.054, 0.699, 0.048]\n",
    "sub19_motion = [1.593, 0.180, 0.789, 0.189] # out r1\n",
    "#sub20_motion = [1.476, 0.091, 5.681, 0.167] # out both\n",
    "sub20_motion = [2.0, 2.0, 2.0, 2.0]\n",
    "sub21_motion = [0.935, 0.088, 0.856, 0.110]\n",
    "sub22_motion = [0.729, 0.084, 1.021, 0.105] # out r2\n",
    "sub23_motion = [0.582, 0.084, 0.496, 0.073] # head looks a bit weird\n",
    "#sub24_motion = [1.406, 0.109, 1.550, 0.088] # out r1\n",
    "sub24_motion = [2.0, 2.0, 2.0, 2.0]\n",
    "sub25_motion = [0.649, 0.092, 0.351, 0.059]\n",
    "sub26_motion = [0.894, 0.105, 0.965, 0.127]\n",
    "sub27_motion = [0.525, 0.081, 0.494, 0.070]\n",
    "sub28_motion = [1.001, 0.103, 0.494, 0.094] # out r1\n",
    "sub29_motion = [0.300, 0.052, 0.330, 0.052]\n",
    "sub30_motion = [0.269, 0.063, 0.335, 0.068]\n",
    "sub31_motion = [0.398, 0.066, 0.377, 0.075]\n",
    "sub32_motion = [0.817, 0.113, 1.551, 0.153] # out r2\n",
    "sub33_motion = [0.959, 0.120, 0.627, 0.107]\n",
    "sub34_motion = [0.344, 0.058, 0.309, 0.051]\n",
    "sub35_motion = [0.656, 0.064, 0.704, 0.071]\n",
    "sub36_motion = [0.790, 0.107, 0.746, 0.105]\n",
    "sub37_motion = [0.560, 0.071, 0.585, 0.086]\n",
    "\n",
    "QA_afni_list =['sub-01', 'sub-02', 'sub-03', 'sub-04', 'sub-08', 'sub-09', 'sub-10',\n",
    "               'sub-11', 'sub-12', 'sub-13', 'sub-14', 'sub-15', 'sub-16', 'sub-17', 'sub-18',\n",
    "               'sub-19', 'sub-21', 'sub-22', 'sub-23', 'sub-25', 'sub-26', 'sub-27', 'sub-28', 'sub-29',\n",
    "               'sub-30', 'sub-31', 'sub-32', 'sub-33', 'sub-34', 'sub-35', 'sub-36', 'sub-37']\n",
    "\n",
    "# QA_afni_list =['sub-01', 'sub-02', 'sub-03', 'sub-04', 'sub-08', 'sub-09', 'sub-10',\n",
    "#               'sub-11', 'sub-12', 'sub-13', 'sub-14',  'sub-15', 'sub-16', 'sub-17', 'sub-18',\n",
    "#               'sub-19', 'sub-21', 'sub-22', 'sub-23', 'sub-25', 'sub-26', 'sub-27', 'sub-28', 'sub-29',\n",
    "#               'sub-30', 'sub-31', 'sub-32', 'sub-33', 'sub-34', 'sub-35', 'sub-36', 'sub-37']\n",
    "\n",
    "\n",
    "# by231212, cg231116, hh231209, jz231215, kp231213, aw231111\n",
    "\n",
    "# by231212, cg231116, jz231215, kp231213, aw231111\n",
    "\n",
    "# sub-05: aw231111\n",
    "# sub-06: by231212\n",
    "# sub-07: cg231116\n",
    "# sub-20: jz231215 \n",
    "# sub-16: hh231209\n",
    "# sub-24: kp231213 \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "participants_motion_list = [sub01_motion, sub02_motion, sub03_motion, sub04_motion, sub05_motion, sub06_motion, sub07_motion, sub08_motion, sub09_motion, sub10_motion, sub11_motion, sub12_motion, sub13_motion, sub14_motion, sub15_motion, sub16_motion, sub17_motion, sub18_motion, sub19_motion, sub20_motion, sub21_motion, sub22_motion, sub23_motion, sub24_motion, sub25_motion, sub26_motion, sub27_motion, sub28_motion, sub29_motion, sub30_motion, sub31_motion, sub32_motion, sub33_motion, sub34_motion, sub35_motion, sub36_motion, sub37_motion]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe first six and last four volume acquisitions constituted lead-in and leadout periods, and were omitted from analysis.\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "The first six and last four volume acquisitions constituted lead-in and leadout periods, and were omitted from analysis.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Participant sub-01 has acceptable data for BOTH runs.\n",
      "Participant sub-02 has acceptable data for BOTH runs.\n",
      "Participant sub-03 has acceptable data for BOTH runs.\n",
      "Participant sub-04 has acceptable data for BOTH runs.\n",
      "Participant sub-08 has acceptable data for BOTH runs.\n",
      "Participant sub-09 has acceptable data for BOTH runs.\n",
      "Participant sub-10 has acceptable data for BOTH runs.\n",
      "Participant sub-11 has acceptable data for BOTH runs.\n",
      "Participant sub-12 has acceptable data for BOTH runs.\n",
      "Participant sub-13 has acceptable data for BOTH runs.\n",
      "Participant sub-14 has acceptable data for BOTH runs.\n",
      "Participant sub-15 has acceptable data for BOTH runs.\n",
      "Participant sub-16 has acceptable data for BOTH runs.\n",
      "Participant sub-17 has acceptable data for BOTH runs.\n",
      "Participant sub-18 has acceptable data for BOTH runs.\n",
      "Participant sub-19 has acceptable data for BOTH runs.\n",
      "Participant sub-21 has acceptable data for BOTH runs.\n",
      "Participant sub-22 has acceptable data for BOTH runs.\n",
      "Participant sub-23 has acceptable data for BOTH runs.\n",
      "Participant sub-25 has acceptable data for BOTH runs.\n",
      "Participant sub-26 has acceptable data for BOTH runs.\n",
      "Participant sub-27 has acceptable data for BOTH runs.\n",
      "Participant sub-28 has acceptable data for BOTH runs.\n",
      "Participant sub-29 has acceptable data for BOTH runs.\n",
      "Participant sub-30 has acceptable data for BOTH runs.\n",
      "Participant sub-31 has acceptable data for BOTH runs.\n",
      "Participant sub-32 has acceptable data for BOTH runs.\n",
      "Participant sub-33 has acceptable data for BOTH runs.\n",
      "Participant sub-34 has acceptable data for BOTH runs.\n",
      "Participant sub-35 has acceptable data for BOTH runs.\n",
      "Participant sub-36 has acceptable data for BOTH runs.\n",
      "Participant sub-37 has acceptable data for BOTH runs.\n",
      "A total of 32 participants have usable data.\n"
     ]
    }
   ],
   "source": [
    "# Record the number with good data for each run. \n",
    "run1_subs_list = []\n",
    "run2_subs_list = []\n",
    "particpants_list = []\n",
    "\n",
    "# Create relevant dictionaries. \n",
    "participant_dictionaries = {}\n",
    "participant_dictionaries_zscored = {}\n",
    "participant_dictionaries_raw = {}\n",
    "participant_dictionaries_raw_z = {}\n",
    "participants_events = {}\n",
    "\n",
    "for sub_id in range(37):\n",
    "    \n",
    "    # Format participant number.\n",
    "    current_sub = \"sub-\" + str(sub_id + 1).zfill(2)\n",
    "    current_num = str(sub_id + 1).zfill(2)\n",
    "\n",
    "    # Append data to relevant dictionaries. \n",
    "    # Append the participant number to the list of participants.\n",
    "    if current_sub in QA_afni_list:\n",
    "        \n",
    "        current_sub_dic = getROIs_timecourse(current_num)\n",
    "\n",
    "        participant_dictionaries[current_sub] = current_sub_dic[\"ROI_trimmed_timeseries\"]\n",
    "        participant_dictionaries_zscored[current_sub] = current_sub_dic[\"ROI_trimmed_timeseries_zcored\"]\n",
    "        participant_dictionaries_raw[current_sub] = current_sub_dic[\"ROI_raw_timeseries\"]\n",
    "        participant_dictionaries_raw_z[current_sub] = current_sub_dic[\"ROI_raw_timeseries_zscored\"]\n",
    "        participants_events[current_sub] = current_sub_dic[\"Processes_events\"]\n",
    "        particpants_list.append(current_num)\n",
    "        print(\"Participant \" + current_sub + \" has acceptable data for BOTH runs.\")\n",
    "\n",
    "print(\"A total of \" + str(len(particpants_list)) + \" participants have usable data.\")\n",
    "\n",
    "# Remove 05(aw231111), 06(by231212), 07(cg231116), 14(hb231201), 16(hh231209), 20(jz231215), 24(kp231213)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1_keys_sorted = ['rh6', 'rc2',\n",
    "                  'uh1', 'rc9',\n",
    "                  'rh2', 'uc4',\n",
    "                  'rh1', 'rc6',\n",
    "                  'rh9', 'rc11',\n",
    "                  'uh2', 'uc1',\n",
    "                  'rh3', 'rc12',\n",
    "                  'rh11', 'rc5']\n",
    "\n",
    "r2_keys_sorted = ['rh12', 'rc3',\n",
    "                  'rh7', 'rc1', \n",
    "                  'rh10', 'rc8', \n",
    "                  'rh4', 'uc3', \n",
    "                  'rh5', 'rc4', \n",
    "                  'rh8', 'uc2',\n",
    "                  'uh3', 'rc10',\n",
    "                  'uh4', 'rc7']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim each timecourse per trailer per ROI. \n",
    "all_subjects_avg_NAcc_timecourse, all_subjects_NAcc_timecourse, all_subjects_NAcc_timecourse_df, all_subjects_NAcc_raw_df = trim_timecourse_per_roi(participant_dictionaries, participant_dictionaries_raw, \"NAcc\")\n",
    "all_subjects_avg_AIns_timecourse, all_subjects_AIns_timecourse, all_subjects_AIns_timecourse_df, all_subjects_AIns_raw_df = trim_timecourse_per_roi(participant_dictionaries, participant_dictionaries_raw, \"AIns\")\n",
    "all_subjects_avg_MPFC_timecourse, all_subjects_MPFC_timecourse, all_subjects_MPFC_timecourse_df, all_subjects_MPFC_raw_df = trim_timecourse_per_roi(participant_dictionaries, participant_dictionaries_raw, \"MPFC\")\n",
    "#all_subjects_avg_V1_timecourse, all_subjects_V1_timecourse, all_subjects_V1_timecourse_df, all_subjects_V1_raw_df = trim_timecourse_per_roi(participant_dictionaries, participant_dictionaries_raw, \"V1\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing and saving files for : sub-01\n",
      "Processing and saving files for : sub-02\n",
      "Processing and saving files for : sub-03\n",
      "Processing and saving files for : sub-04\n",
      "Processing and saving files for : sub-08\n",
      "Processing and saving files for : sub-09\n",
      "Processing and saving files for : sub-10\n",
      "Processing and saving files for : sub-11\n",
      "Processing and saving files for : sub-12\n",
      "Processing and saving files for : sub-13\n",
      "Processing and saving files for : sub-14\n",
      "Processing and saving files for : sub-15\n",
      "Processing and saving files for : sub-16\n",
      "Processing and saving files for : sub-17\n",
      "Processing and saving files for : sub-18\n",
      "Processing and saving files for : sub-19\n",
      "Processing and saving files for : sub-21\n",
      "Processing and saving files for : sub-22\n",
      "Processing and saving files for : sub-23\n",
      "Processing and saving files for : sub-25\n",
      "Processing and saving files for : sub-26\n",
      "Processing and saving files for : sub-27\n",
      "Processing and saving files for : sub-28\n",
      "Processing and saving files for : sub-29\n",
      "Processing and saving files for : sub-30\n",
      "Processing and saving files for : sub-31\n",
      "Processing and saving files for : sub-32\n",
      "Processing and saving files for : sub-33\n",
      "Processing and saving files for : sub-34\n",
      "Processing and saving files for : sub-35\n",
      "Processing and saving files for : sub-36\n",
      "Processing and saving files for : sub-37\n"
     ]
    }
   ],
   "source": [
    "# Save each participants timecourse data and processed events to a csv files.\n",
    "\n",
    "save_path = \"/Users/la/Documents/GitHub/RM_Thesis_Neuroforecasting/ProcessedData/\"\n",
    "\n",
    "for id_participant in range(len(particpants_list)):\n",
    "    \n",
    "    current_sub = \"sub-\" + particpants_list[id_participant]\n",
    "    print(\"Processing and saving files for : \"  + current_sub)\n",
    "\n",
    "    participants_events[current_sub].to_csv(save_path + current_sub + \"/\" + current_sub + \"_processed_events.csv\", index=False)\n",
    "\n",
    "    # Create empty dataframe for current participant.\n",
    "    current_participant_df = pd.DataFrame(columns=[\"Participant\"], data=np.repeat(current_sub, 15))\n",
    "\n",
    "    # Current participant trailer keys.\n",
    "    current_sub_trailer_keys = participants_events[current_sub][\"Trailer\"].unique()\n",
    "\n",
    "    for id_trailer in current_sub_trailer_keys:\n",
    "\n",
    "        col_label_NAcc = id_trailer + \"_bNAcc\"\n",
    "        col_lable_AIns = id_trailer + \"_bAIns\"\n",
    "        col_lable_MPFC = id_trailer + \"_bMPFC\"\n",
    "            \n",
    "        current_participant_df[col_label_NAcc] = participant_dictionaries[current_sub][id_trailer][\"Bilateral_NAcc\"]\n",
    "        current_participant_df[col_lable_AIns] = participant_dictionaries[current_sub][id_trailer][\"Bilateral_AIns\"]\n",
    "        current_participant_df[col_lable_MPFC] = participant_dictionaries[current_sub][id_trailer][\"Bilateral_MPFC\"]\n",
    "    \n",
    "    #current_participant_df.to_csv(save_path + current_sub + \"/\" + current_sub + \"_AIM_ROI_NeuralActivation.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process all participants data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working dir : /Users/la/Documents/GitHub/RM_Thesis_Neuroforecasting/ProcessedData\n"
     ]
    }
   ],
   "source": [
    "# Open a datasets directory. \n",
    "fd = os.open(\"/Users/la/Documents/GitHub/RM_Thesis_Neuroforecasting/ProcessedData\", os.O_RDONLY)\n",
    "\n",
    "# Use os.fchdir() method to change the current dir/folder.\n",
    "os.fchdir(fd)\n",
    "\n",
    "# Safe check- Print current working directory\n",
    "print(\"Current working dir : %s\" % os.getcwd())\n",
    "\n",
    "root_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_SegData(participant_list, participant_dictionaries_zscored, participant_dictionaries):\n",
    "\n",
    "    all_cols = ['Participant', 'Trailer', 'Type', 'W_score', 'A_score', 'F_score']\n",
    "\n",
    "    all_subs_events_df = pd.DataFrame(columns = all_cols)\n",
    "\n",
    "\n",
    "    # Loop through each participant's data.\n",
    "    for sub_num in particpants_list:\n",
    "\n",
    "        current_sub = str(\"sub-\" + sub_num)\n",
    "        current_sub_events_path = root_path + \"/\" + current_sub + \"/\" + current_sub + \"_processed_events.csv\"\n",
    "        current_sub_AIM_path = root_path + \"/\" + current_sub + \"/\" + current_sub + \"_AIM_ROI_NeuralActivation.csv\"\n",
    "\n",
    "        current_sub_events_df = pd.read_csv(current_sub_events_path)\n",
    "        current_sub_AIM_df = pd.read_csv(current_sub_AIM_path)\n",
    "        current_sub_zscored_dict = participant_dictionaries_zscored[current_sub]\n",
    "        current_sub_dic = participant_dictionaries[current_sub]\n",
    "\n",
    "        current_sub_clean_df = pd.DataFrame() \n",
    "\n",
    "        trailer_keys = current_sub_events_df[\"Trailer\"].unique().tolist()\n",
    "\n",
    "        # Create empty lists to store the mean onset, middle, offset, and whole activation for each ROI.\n",
    "        current_sub_clean_df[\"NAcc_onset\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"AIns_onset\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"MPFC_onset\"] = np.zeros(len(trailer_keys))\n",
    "\n",
    "        current_sub_clean_df[\"NAcc_middle\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"AIns_middle\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"MPFC_middle\"] = np.zeros(len(trailer_keys))\n",
    "\n",
    "        current_sub_clean_df[\"NAcc_offset\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"AIns_offset\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"MPFC_offset\"] = np.zeros(len(trailer_keys))\n",
    "\n",
    "        current_sub_clean_df[\"NAcc_whole\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"AIns_whole\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"MPFC_whole\"] = np.zeros(len(trailer_keys))\n",
    "\n",
    "        # Create empty lists to store the mean onset, middle, offset, and whole activation for each ROI.\n",
    "        current_sub_clean_df[\"NAcc_onset_z\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"AIns_onset_z\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"MPFC_onset_z\"] = np.zeros(len(trailer_keys))\n",
    "\n",
    "        current_sub_clean_df[\"NAcc_middle_z\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"AIns_middle_z\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"MPFC_middle_z\"] = np.zeros(len(trailer_keys))\n",
    "\n",
    "        current_sub_clean_df[\"NAcc_offset_z\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"AIns_offset_z\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"MPFC_offset_z\"] = np.zeros(len(trailer_keys))\n",
    "\n",
    "        current_sub_clean_df[\"NAcc_whole_z\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"AIns_whole_z\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"MPFC_whole_z\"] = np.zeros(len(trailer_keys))\n",
    "\n",
    "        # Create empty list to store 5 segments of the timecourse for each ROI.\n",
    "        current_sub_clean_df[\"NAcc_seg1\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"AIns_seg1\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"MPFC_seg1\"] = np.zeros(len(trailer_keys))\n",
    "\n",
    "        current_sub_clean_df[\"NAcc_seg2\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"AIns_seg2\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"MPFC_seg2\"] = np.zeros(len(trailer_keys))\n",
    "\n",
    "        current_sub_clean_df[\"NAcc_seg3\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"AIns_seg3\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"MPFC_seg3\"] = np.zeros(len(trailer_keys))\n",
    "\n",
    "        current_sub_clean_df[\"NAcc_seg4\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"AIns_seg4\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"MPFC_seg4\"] = np.zeros(len(trailer_keys))\n",
    "\n",
    "        current_sub_clean_df[\"NAcc_seg5\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"AIns_seg5\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"MPFC_seg5\"] = np.zeros(len(trailer_keys))\n",
    "\n",
    "        current_sub_clean_df[\"NAcc_seg1_z\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"AIns_seg1_z\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"MPFC_seg1_z\"] = np.zeros(len(trailer_keys))\n",
    "\n",
    "        current_sub_clean_df[\"NAcc_seg2_z\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"AIns_seg2_z\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"MPFC_seg2_z\"] = np.zeros(len(trailer_keys))\n",
    "\n",
    "        current_sub_clean_df[\"NAcc_seg3_z\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"AIns_seg3_z\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"MPFC_seg3_z\"] = np.zeros(len(trailer_keys))\n",
    "\n",
    "        current_sub_clean_df[\"NAcc_seg4_z\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"AIns_seg4_z\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"MPFC_seg4_z\"] = np.zeros(len(trailer_keys))\n",
    "\n",
    "        current_sub_clean_df[\"NAcc_seg5_z\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"AIns_seg5_z\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"MPFC_seg5_z\"] = np.zeros(len(trailer_keys))\n",
    "\n",
    "        # Create empty lists to store the ratings segments and whole activation for each ROI.\n",
    "        current_sub_clean_df[\"NAcc_watch\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"AIns_watch\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"MPFC_watch\"] = np.zeros(len(trailer_keys))\n",
    "\n",
    "        current_sub_clean_df[\"NAcc_feel\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"AIns_feel\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"MPFC_feel\"] = np.zeros(len(trailer_keys))\n",
    "\n",
    "        current_sub_clean_df[\"NAcc_arousal\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"AIns_arousal\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"MPFC_arousal\"] = np.zeros(len(trailer_keys))\n",
    "\n",
    "        # Create empty lists to store max signal for each ROI.\n",
    "\n",
    "        current_sub_clean_df[\"NAcc_ind_peaks_pos\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"NAcc_ind_peaks_pos_id\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"NAcc_ind_peaks_pos_z\"] = np.zeros(len(trailer_keys))\n",
    "\n",
    "        current_sub_clean_df[\"NAcc_ind_peaks_pos2\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"NAcc_ind_peaks_pos_id2\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"NAcc_ind_peaks_pos_2z\"] = np.zeros(len(trailer_keys))\n",
    "\n",
    "        current_sub_clean_df[\"NAcc_ind_peaks_pos3\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"NAcc_ind_peaks_pos_id3\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"NAcc_ind_peaks_pos_3z\"] = np.zeros(len(trailer_keys))\n",
    "\n",
    "        current_sub_clean_df[\"AIns_ind_peaks_pos\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"AIns_ind_peaks_pos_id\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"AIns_ind_peaks_pos_z\"] = np.zeros(len(trailer_keys))\n",
    "\n",
    "        current_sub_clean_df[\"AIns_ind_peaks_pos2\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"AIns_ind_peaks_pos_id2\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"AIns_ind_peaks_pos_2z\"] = np.zeros(len(trailer_keys))\n",
    "\n",
    "        current_sub_clean_df[\"AIns_ind_peaks_pos3\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"AIns_ind_peaks_pos_id3\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"AIns_ind_peaks_pos_3z\"] = np.zeros(len(trailer_keys))\n",
    "\n",
    "        current_sub_clean_df[\"MPFC_ind_peaks_pos\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"MPFC_ind_peaks_pos_id\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"MPFC_ind_peaks_pos_z\"] = np.zeros(len(trailer_keys))\n",
    "\n",
    "        current_sub_clean_df[\"MPFC_ind_peaks_pos2\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"MPFC_ind_peaks_pos_id2\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"MPFC_ind_peaks_pos_2z\"] = np.zeros(len(trailer_keys))\n",
    "\n",
    "        current_sub_clean_df[\"MPFC_ind_peaks_pos3\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"MPFC_ind_peaks_pos_id3\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"MPFC_ind_peaks_pos_3z\"] = np.zeros(len(trailer_keys))\n",
    "\n",
    "        # Create empty lists to store individual TRs for each ROI.\n",
    "        current_sub_clean_df[\"NAcc_TR1\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"NAcc_TR2\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"NAcc_TR3\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"NAcc_TR4\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"NAcc_TR5\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"NAcc_TR6\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"NAcc_TR7\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"NAcc_TR8\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"NAcc_TR9\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"NAcc_TR10\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"NAcc_TR11\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"NAcc_TR12\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"NAcc_TR13\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"NAcc_TR14\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"NAcc_TR15\"] = np.zeros(len(trailer_keys))\n",
    "\n",
    "        current_sub_clean_df[\"AIns_TR1\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"AIns_TR2\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"AIns_TR3\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"AIns_TR4\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"AIns_TR5\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"AIns_TR6\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"AIns_TR7\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"AIns_TR8\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"AIns_TR9\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"AIns_TR10\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"AIns_TR11\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"AIns_TR12\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"AIns_TR13\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"AIns_TR14\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"AIns_TR15\"] = np.zeros(len(trailer_keys))\n",
    "\n",
    "        current_sub_clean_df[\"MPFC_TR1\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"MPFC_TR2\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"MPFC_TR3\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"MPFC_TR4\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"MPFC_TR5\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"MPFC_TR6\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"MPFC_TR7\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"MPFC_TR8\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"MPFC_TR9\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"MPFC_TR10\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"MPFC_TR11\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"MPFC_TR12\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"MPFC_TR13\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"MPFC_TR14\"] = np.zeros(len(trailer_keys))\n",
    "        current_sub_clean_df[\"MPFC_TR15\"] = np.zeros(len(trailer_keys))\n",
    "\n",
    "        # Loop through each trailer. \n",
    "        for id_trailer in range(len(trailer_keys)):\n",
    "\n",
    "            trailer_key = trailer_keys[id_trailer]\n",
    "\n",
    "            # Create keys to access AIM_df columns.\n",
    "            NAcc_timeseries = participant_dictionaries[current_sub][trailer_key][\"Bilateral_NAcc\"]\n",
    "            AIns_timeseries = participant_dictionaries[current_sub][trailer_key][\"Bilateral_AIns\"]\n",
    "            MPFC_timeseries = participant_dictionaries[current_sub][trailer_key][\"Bilateral_MPFC\"]\n",
    "\n",
    "            NAcc_timeseries_z = participant_dictionaries_zscored[current_sub][trailer_key][\"Bilateral_NAcc_zscored\"]\n",
    "            AIns_timeseries_z = participant_dictionaries_zscored[current_sub][trailer_key][\"Bilateral_AIns_zscored\"]\n",
    "            MPFC_timeseries_z = participant_dictionaries_zscored[current_sub][trailer_key][\"Bilateral_MPFC_zscored\"]\n",
    "\n",
    "            # Calculate ROI mean onset (1-5 TRs) activation for each trailer. \n",
    "            # And append to the corresponding list.\n",
    "            current_sub_clean_df[\"NAcc_onset\"][id_trailer] = np.nanmean(NAcc_timeseries[0:5])\n",
    "            current_sub_clean_df[\"AIns_onset\"][id_trailer] = np.nanmean(AIns_timeseries[0:5])\n",
    "            current_sub_clean_df[\"MPFC_onset\"][id_trailer] = np.nanmean(MPFC_timeseries[0:5])\n",
    "\n",
    "            current_sub_clean_df[\"NAcc_onset_z\"][id_trailer] = np.nanmean(NAcc_timeseries_z[0:5])\n",
    "            current_sub_clean_df[\"AIns_onset_z\"][id_trailer] = np.nanmean(AIns_timeseries_z[0:5])\n",
    "            current_sub_clean_df[\"MPFC_onset_z\"][id_trailer] = np.nanmean(MPFC_timeseries_z[0:5])\n",
    "\n",
    "            # Calculate ROI mean middle (6-10 TRs) activation for each trailer.\n",
    "            current_sub_clean_df[\"NAcc_middle\"][id_trailer] = np.nanmean(NAcc_timeseries[5:10])\n",
    "            current_sub_clean_df[\"AIns_middle\"][id_trailer] = np.nanmean(AIns_timeseries[5:10])\n",
    "            current_sub_clean_df[\"MPFC_middle\"][id_trailer] = np.nanmean(MPFC_timeseries[5:10])\n",
    "\n",
    "            current_sub_clean_df[\"NAcc_middle_z\"][id_trailer] = np.nanmean(NAcc_timeseries_z[5:10])\n",
    "            current_sub_clean_df[\"AIns_middle_z\"][id_trailer] = np.nanmean(AIns_timeseries_z[5:10])\n",
    "            current_sub_clean_df[\"MPFC_middle_z\"][id_trailer] = np.nanmean(MPFC_timeseries_z[5:10])\n",
    "\n",
    "            # Calculate ROI mean offset (11-15 TRs) activation for each trailer.\n",
    "            current_sub_clean_df[\"NAcc_offset\"][id_trailer] = np.nanmean(NAcc_timeseries[10:])\n",
    "            current_sub_clean_df[\"AIns_offset\"][id_trailer] = np.nanmean(AIns_timeseries[10:])\n",
    "            current_sub_clean_df[\"MPFC_offset\"][id_trailer] = np.nanmean(MPFC_timeseries[10:])\n",
    "\n",
    "            current_sub_clean_df[\"NAcc_offset_z\"][id_trailer] = np.nanmean(NAcc_timeseries_z[10:])\n",
    "            current_sub_clean_df[\"AIns_offset_z\"][id_trailer] = np.nanmean(AIns_timeseries_z[10:])\n",
    "            current_sub_clean_df[\"MPFC_offset_z\"][id_trailer] = np.nanmean(MPFC_timeseries_z[10:])\n",
    "\n",
    "            # Calculate ROI mean whole activation for each trailer.\n",
    "            current_sub_clean_df[\"NAcc_whole\"][id_trailer] = np.nanmean(NAcc_timeseries)\n",
    "            current_sub_clean_df[\"AIns_whole\"][id_trailer] = np.nanmean(AIns_timeseries)\n",
    "            current_sub_clean_df[\"MPFC_whole\"][id_trailer] = np.nanmean(MPFC_timeseries)\n",
    "\n",
    "            current_sub_clean_df[\"NAcc_whole_z\"][id_trailer] = np.nanmean(NAcc_timeseries_z)\n",
    "            current_sub_clean_df[\"AIns_whole_z\"][id_trailer] = np.nanmean(AIns_timeseries_z)\n",
    "            current_sub_clean_df[\"MPFC_whole_z\"][id_trailer] = np.nanmean(MPFC_timeseries_z)\n",
    "\n",
    "            # Calculate segments for each ROI.\n",
    "            current_sub_clean_df[\"NAcc_seg1\"][id_trailer] = np.nanmean(NAcc_timeseries[0:3])\n",
    "            current_sub_clean_df[\"AIns_seg1\"][id_trailer] = np.nanmean(AIns_timeseries[0:3])\n",
    "            current_sub_clean_df[\"MPFC_seg1\"][id_trailer] = np.nanmean(MPFC_timeseries[0:3])\n",
    "\n",
    "            current_sub_clean_df[\"NAcc_seg2\"][id_trailer] = np.nanmean(NAcc_timeseries[3:6])\n",
    "            current_sub_clean_df[\"AIns_seg2\"][id_trailer] = np.nanmean(AIns_timeseries[3:6])\n",
    "            current_sub_clean_df[\"MPFC_seg2\"][id_trailer] = np.nanmean(MPFC_timeseries[3:6])\n",
    "\n",
    "            current_sub_clean_df[\"NAcc_seg3\"][id_trailer] = np.nanmean(NAcc_timeseries[6:9])\n",
    "            current_sub_clean_df[\"AIns_seg3\"][id_trailer] = np.nanmean(AIns_timeseries[6:9])\n",
    "            current_sub_clean_df[\"MPFC_seg3\"][id_trailer] = np.nanmean(MPFC_timeseries[6:9])\n",
    "\n",
    "            current_sub_clean_df[\"NAcc_seg4\"][id_trailer] = np.nanmean(NAcc_timeseries[9:12])\n",
    "            current_sub_clean_df[\"AIns_seg4\"][id_trailer] = np.nanmean(AIns_timeseries[9:12])\n",
    "            current_sub_clean_df[\"MPFC_seg4\"][id_trailer] = np.nanmean(MPFC_timeseries[9:12])\n",
    "\n",
    "            current_sub_clean_df[\"NAcc_seg5\"][id_trailer] = np.nanmean(NAcc_timeseries[12:15])\n",
    "            current_sub_clean_df[\"AIns_seg5\"][id_trailer] = np.nanmean(AIns_timeseries[12:15])\n",
    "            current_sub_clean_df[\"MPFC_seg5\"][id_trailer] = np.nanmean(MPFC_timeseries[12:15])\n",
    "\n",
    "            current_sub_clean_df[\"NAcc_seg1_z\"][id_trailer] = np.nanmean(NAcc_timeseries_z[0:3])\n",
    "            current_sub_clean_df[\"AIns_seg1_z\"][id_trailer] = np.nanmean(AIns_timeseries_z[0:3])\n",
    "            current_sub_clean_df[\"MPFC_seg1_z\"][id_trailer] = np.nanmean(MPFC_timeseries_z[0:3])\n",
    "\n",
    "            current_sub_clean_df[\"NAcc_seg2_z\"][id_trailer] = np.nanmean(NAcc_timeseries_z[3:6])\n",
    "            current_sub_clean_df[\"AIns_seg2_z\"][id_trailer] = np.nanmean(AIns_timeseries_z[3:6])\n",
    "            current_sub_clean_df[\"MPFC_seg2_z\"][id_trailer] = np.nanmean(MPFC_timeseries_z[3:6])\n",
    "\n",
    "            current_sub_clean_df[\"NAcc_seg3_z\"][id_trailer] = np.nanmean(NAcc_timeseries_z[6:9])\n",
    "            current_sub_clean_df[\"AIns_seg3_z\"][id_trailer] = np.nanmean(AIns_timeseries_z[6:9])\n",
    "            current_sub_clean_df[\"MPFC_seg3_z\"][id_trailer] = np.nanmean(MPFC_timeseries_z[6:9])\n",
    "\n",
    "            current_sub_clean_df[\"NAcc_seg4_z\"][id_trailer] = np.nanmean(NAcc_timeseries_z[9:12])\n",
    "            current_sub_clean_df[\"AIns_seg4_z\"][id_trailer] = np.nanmean(AIns_timeseries_z[9:12])\n",
    "            current_sub_clean_df[\"MPFC_seg4_z\"][id_trailer] = np.nanmean(MPFC_timeseries_z[9:12])\n",
    "\n",
    "            current_sub_clean_df[\"NAcc_seg5_z\"][id_trailer] = np.nanmean(NAcc_timeseries_z[12:15])\n",
    "            current_sub_clean_df[\"AIns_seg5_z\"][id_trailer] = np.nanmean(AIns_timeseries_z[12:15])\n",
    "            current_sub_clean_df[\"MPFC_seg5_z\"][id_trailer] = np.nanmean(MPFC_timeseries_z[12:15])\n",
    "\n",
    "            # Calculate ratings segments and whole activation for each ROI.\n",
    "            current_sub_clean_df[\"NAcc_watch\"][id_trailer] = np.mean(current_sub_dic[trailer_keys[id_trailer]]['Bilateral_NAcc_watch'])\n",
    "            current_sub_clean_df[\"AIns_watch\"][id_trailer] = np.mean(current_sub_dic[trailer_keys[id_trailer]]['Bilateral_AIns_watch'])\n",
    "            current_sub_clean_df[\"MPFC_watch\"][id_trailer] = np.mean(current_sub_dic[trailer_keys[id_trailer]]['Bilateral_MPFC_watch'])\n",
    "\n",
    "            current_sub_clean_df[\"NAcc_feel\"][id_trailer] = np.mean(current_sub_dic[trailer_keys[id_trailer]]['Bilateral_NAcc_feel'])\n",
    "            current_sub_clean_df[\"AIns_feel\"][id_trailer] = np.mean(current_sub_dic[trailer_keys[id_trailer]]['Bilateral_AIns_feel'])\n",
    "            current_sub_clean_df[\"MPFC_feel\"][id_trailer] = np.mean(current_sub_dic[trailer_keys[id_trailer]]['Bilateral_MPFC_feel'])\n",
    "\n",
    "            current_sub_clean_df[\"NAcc_arousal\"][id_trailer] = np.mean(current_sub_dic[trailer_keys[id_trailer]]['Bilateral_NAcc_arousal'])\n",
    "            current_sub_clean_df[\"AIns_arousal\"][id_trailer] = np.mean(current_sub_dic[trailer_keys[id_trailer]]['Bilateral_AIns_arousal'])\n",
    "            current_sub_clean_df[\"MPFC_arousal\"][id_trailer] = np.mean(current_sub_dic[trailer_keys[id_trailer]]['Bilateral_MPFC_arousal'])\n",
    "\n",
    "            # Replace NaN values with 0 for locating peak in time series. \n",
    "            NAcc_timeseries_nans2zero = np.nan_to_num(NAcc_timeseries)\n",
    "            AIns_timeseries_nans2zero = np.nan_to_num(AIns_timeseries)\n",
    "            MPFC_timeseries_nans2zero = np.nan_to_num(MPFC_timeseries)\n",
    "\n",
    "            # Sanity-check.\n",
    "            #print(\"For \" + current_sub + \" (\" + trailer_key  + \") NAcc timeseries: \" + str(np.round(NAcc_timeseries_nans2zero, 3).tolist()) + \"; \\033[1m peak(\" + str(np.round( np.nanmax(NAcc_timeseries_nans2zero), 3) ) + \") & id(\" + str(np.argsort(NAcc_timeseries_nans2zero)[-1]) + \")\\033[0;0m\")  \n",
    "\n",
    "            # Calculate individual peaks for each ROI.\n",
    "            current_sub_clean_df[\"NAcc_ind_peaks_pos\"][id_trailer] = np.nanmax(NAcc_timeseries_nans2zero) \n",
    "            current_sub_clean_df[\"NAcc_ind_peaks_pos_id\"][id_trailer] = np.argsort(NAcc_timeseries_nans2zero)[-1]\n",
    "            current_sub_clean_df[\"NAcc_ind_peaks_pos_z\"][id_trailer] = NAcc_timeseries_z[np.argsort(NAcc_timeseries)[-1]]\n",
    "\n",
    "            current_sub_clean_df[\"NAcc_ind_peaks_pos2\"][id_trailer] = np.sort(NAcc_timeseries_nans2zero)[-2]\n",
    "            current_sub_clean_df[\"NAcc_ind_peaks_pos_id2\"][id_trailer] = np.argsort(NAcc_timeseries_nans2zero)[-2]\n",
    "            current_sub_clean_df[\"NAcc_ind_peaks_pos_2z\"][id_trailer] = NAcc_timeseries_z[np.argsort(NAcc_timeseries)[-2]]\n",
    "\n",
    "            current_sub_clean_df[\"NAcc_ind_peaks_pos3\"][id_trailer] = np.sort(NAcc_timeseries_nans2zero)[-3]\n",
    "            current_sub_clean_df[\"NAcc_ind_peaks_pos_id3\"][id_trailer] = np.argsort(NAcc_timeseries_nans2zero)[-3]\n",
    "            current_sub_clean_df[\"NAcc_ind_peaks_pos_3z\"][id_trailer] = NAcc_timeseries_z[np.argsort(NAcc_timeseries)[-3]]\n",
    "\n",
    "            current_sub_clean_df[\"AIns_ind_peaks_pos\"][id_trailer] = np.nanmax(AIns_timeseries_nans2zero)\n",
    "            current_sub_clean_df[\"AIns_ind_peaks_pos_id\"][id_trailer] = np.argsort(AIns_timeseries_nans2zero)[-1]\n",
    "            current_sub_clean_df[\"AIns_ind_peaks_pos_z\"][id_trailer] = AIns_timeseries_z[np.argsort(AIns_timeseries)[-1]]\n",
    "\n",
    "            current_sub_clean_df[\"AIns_ind_peaks_pos2\"][id_trailer] = np.sort(AIns_timeseries_nans2zero)[-2]\n",
    "            current_sub_clean_df[\"AIns_ind_peaks_pos_id2\"][id_trailer] = np.argsort(AIns_timeseries_nans2zero)[-2]\n",
    "            current_sub_clean_df[\"AIns_ind_peaks_pos_2z\"][id_trailer] = AIns_timeseries_z[np.argsort(AIns_timeseries)[-2]]\n",
    "\n",
    "            current_sub_clean_df[\"AIns_ind_peaks_pos3\"][id_trailer] = np.sort(AIns_timeseries_nans2zero)[-3]\n",
    "            current_sub_clean_df[\"AIns_ind_peaks_pos_id3\"][id_trailer] = np.argsort(AIns_timeseries_nans2zero)[-3]\n",
    "            current_sub_clean_df[\"AIns_ind_peaks_pos_3z\"][id_trailer] = AIns_timeseries_z[np.argsort(AIns_timeseries)[-3]]\n",
    "\n",
    "            current_sub_clean_df[\"MPFC_ind_peaks_pos\"][id_trailer] = np.nanmax(MPFC_timeseries_nans2zero)\n",
    "            current_sub_clean_df[\"MPFC_ind_peaks_pos_id\"][id_trailer] = np.argsort(MPFC_timeseries_nans2zero)[-1]\n",
    "            current_sub_clean_df[\"MPFC_ind_peaks_pos_z\"][id_trailer] = MPFC_timeseries_z[np.argsort(MPFC_timeseries)[-1]]\n",
    "\n",
    "            current_sub_clean_df[\"MPFC_ind_peaks_pos2\"][id_trailer] = np.sort(MPFC_timeseries_nans2zero)[-2]\n",
    "            current_sub_clean_df[\"MPFC_ind_peaks_pos_id2\"][id_trailer] = np.argsort(MPFC_timeseries_nans2zero)[-2]\n",
    "            current_sub_clean_df[\"MPFC_ind_peaks_pos_2z\"][id_trailer] = MPFC_timeseries_z[np.argsort(MPFC_timeseries)[-2]]\n",
    "\n",
    "            current_sub_clean_df[\"MPFC_ind_peaks_pos3\"][id_trailer] = np.sort(MPFC_timeseries_nans2zero)[-3]\n",
    "            current_sub_clean_df[\"MPFC_ind_peaks_pos_id3\"][id_trailer] = np.argsort(MPFC_timeseries_nans2zero)[-3]\n",
    "            current_sub_clean_df[\"MPFC_ind_peaks_pos_3z\"][id_trailer] = MPFC_timeseries_z[np.argsort(MPFC_timeseries)[-3]]\n",
    "\n",
    "            # If there's a nan in the ROI time series, do not store the peak id for this trial. \n",
    "            # it could be the case that the peak occurs when the subject moves but we have no way of sepataing th peak signal from the motion.\n",
    "\n",
    "            if np.isnan(NAcc_timeseries).sum() > 2:\n",
    "                current_sub_clean_df[\"NAcc_ind_peaks_pos_id\"][id_trailer] = np.nan\n",
    "                current_sub_clean_df[\"NAcc_ind_peaks_pos_id2\"][id_trailer] = np.nan\n",
    "                current_sub_clean_df[\"NAcc_ind_peaks_pos_id3\"][id_trailer] = np.nan\n",
    "\n",
    "            if np.isnan(AIns_timeseries).sum() > 2:\n",
    "                current_sub_clean_df[\"AIns_ind_peaks_pos_id\"][id_trailer] = np.nan\n",
    "                current_sub_clean_df[\"AIns_ind_peaks_pos_id2\"][id_trailer] = np.nan\n",
    "                current_sub_clean_df[\"AIns_ind_peaks_pos_id3\"][id_trailer] = np.nan\n",
    "\n",
    "            if np.isnan(MPFC_timeseries).sum() > 2:\n",
    "                current_sub_clean_df[\"MPFC_ind_peaks_pos_id\"][id_trailer] = np.nan\n",
    "                current_sub_clean_df[\"MPFC_ind_peaks_pos_id2\"][id_trailer] = np.nan\n",
    "                current_sub_clean_df[\"MPFC_ind_peaks_pos_id3\"][id_trailer] = np.nan\n",
    "\n",
    "            # Calculate individual TRs for each ROI.\n",
    "            current_sub_clean_df[\"NAcc_TR1\"][id_trailer] = NAcc_timeseries[0]\n",
    "            current_sub_clean_df[\"NAcc_TR2\"][id_trailer] = NAcc_timeseries[1]\n",
    "            current_sub_clean_df[\"NAcc_TR3\"][id_trailer] = NAcc_timeseries[2]\n",
    "            current_sub_clean_df[\"NAcc_TR4\"][id_trailer] = NAcc_timeseries[3]\n",
    "            current_sub_clean_df[\"NAcc_TR5\"][id_trailer] = NAcc_timeseries[4]\n",
    "            current_sub_clean_df[\"NAcc_TR6\"][id_trailer] = NAcc_timeseries[5]\n",
    "            current_sub_clean_df[\"NAcc_TR7\"][id_trailer] = NAcc_timeseries[6]\n",
    "            current_sub_clean_df[\"NAcc_TR8\"][id_trailer] = NAcc_timeseries[7]\n",
    "            current_sub_clean_df[\"NAcc_TR9\"][id_trailer] = NAcc_timeseries[8]\n",
    "            current_sub_clean_df[\"NAcc_TR10\"][id_trailer] = NAcc_timeseries[9]\n",
    "            current_sub_clean_df[\"NAcc_TR11\"][id_trailer] = NAcc_timeseries[10]\n",
    "            current_sub_clean_df[\"NAcc_TR12\"][id_trailer] = NAcc_timeseries[11]\n",
    "            current_sub_clean_df[\"NAcc_TR13\"][id_trailer] = NAcc_timeseries[12]\n",
    "            current_sub_clean_df[\"NAcc_TR14\"][id_trailer] = NAcc_timeseries[13]\n",
    "            current_sub_clean_df[\"NAcc_TR15\"][id_trailer] = NAcc_timeseries[14]\n",
    "\n",
    "            current_sub_clean_df[\"AIns_TR1\"][id_trailer] = AIns_timeseries[0]\n",
    "            current_sub_clean_df[\"AIns_TR2\"][id_trailer] = AIns_timeseries[1]\n",
    "            current_sub_clean_df[\"AIns_TR3\"][id_trailer] = AIns_timeseries[2]\n",
    "            current_sub_clean_df[\"AIns_TR4\"][id_trailer] = AIns_timeseries[3]\n",
    "            current_sub_clean_df[\"AIns_TR5\"][id_trailer] = AIns_timeseries[4]\n",
    "            current_sub_clean_df[\"AIns_TR6\"][id_trailer] = AIns_timeseries[5]\n",
    "            current_sub_clean_df[\"AIns_TR7\"][id_trailer] = AIns_timeseries[6]\n",
    "            current_sub_clean_df[\"AIns_TR8\"][id_trailer] = AIns_timeseries[7]\n",
    "            current_sub_clean_df[\"AIns_TR9\"][id_trailer] = AIns_timeseries[8]\n",
    "            current_sub_clean_df[\"AIns_TR10\"][id_trailer] = AIns_timeseries[9]\n",
    "            current_sub_clean_df[\"AIns_TR11\"][id_trailer] = AIns_timeseries[10]\n",
    "            current_sub_clean_df[\"AIns_TR12\"][id_trailer] = AIns_timeseries[11]\n",
    "            current_sub_clean_df[\"AIns_TR13\"][id_trailer] = AIns_timeseries[12]\n",
    "            current_sub_clean_df[\"AIns_TR14\"][id_trailer] = AIns_timeseries[13]\n",
    "            current_sub_clean_df[\"AIns_TR15\"][id_trailer] = AIns_timeseries[14]\n",
    "\n",
    "            current_sub_clean_df[\"MPFC_TR1\"][id_trailer] = MPFC_timeseries[0]\n",
    "            current_sub_clean_df[\"MPFC_TR2\"][id_trailer] = MPFC_timeseries[1]\n",
    "            current_sub_clean_df[\"MPFC_TR3\"][id_trailer] = MPFC_timeseries[2]\n",
    "            current_sub_clean_df[\"MPFC_TR4\"][id_trailer] = MPFC_timeseries[3]\n",
    "            current_sub_clean_df[\"MPFC_TR5\"][id_trailer] = MPFC_timeseries[4]\n",
    "            current_sub_clean_df[\"MPFC_TR6\"][id_trailer] = MPFC_timeseries[5]\n",
    "            current_sub_clean_df[\"MPFC_TR7\"][id_trailer] = MPFC_timeseries[6]\n",
    "            current_sub_clean_df[\"MPFC_TR8\"][id_trailer] = MPFC_timeseries[7]\n",
    "            current_sub_clean_df[\"MPFC_TR9\"][id_trailer] = MPFC_timeseries[8]\n",
    "            current_sub_clean_df[\"MPFC_TR10\"][id_trailer] = MPFC_timeseries[9]\n",
    "            current_sub_clean_df[\"MPFC_TR11\"][id_trailer] = MPFC_timeseries[10]\n",
    "            current_sub_clean_df[\"MPFC_TR12\"][id_trailer] = MPFC_timeseries[11]\n",
    "            current_sub_clean_df[\"MPFC_TR13\"][id_trailer] = MPFC_timeseries[12]\n",
    "            current_sub_clean_df[\"MPFC_TR14\"][id_trailer] = MPFC_timeseries[13]\n",
    "            current_sub_clean_df[\"MPFC_TR15\"][id_trailer] = MPFC_timeseries[14]\n",
    "\n",
    "        # Calculate individual peaks for each ROI.\n",
    "\n",
    "        # Add id and rating info.\n",
    "        current_sub_clean_df[\"Participant\"] = np.repeat(current_sub_AIM_df[\"Participant\"][0], len(trailer_keys))\n",
    "        current_sub_clean_df[\"Trailer\"] = trailer_keys\n",
    "        current_sub_clean_df[\"Type\"] = \"Horror\" if 'h' in current_sub_events_df[\"Trailer\"] else \"Comedy\"\n",
    "        current_sub_clean_df[\"W_score\"] = current_sub_events_df[\"W_score\"]\n",
    "        current_sub_clean_df[\"A_score\"] = current_sub_events_df[\"A_score\"]\n",
    "        current_sub_clean_df[\"F_score\"] = current_sub_events_df[\"F_score\"]\n",
    "        current_sub_clean_df[\"W_score_scaled\"] = current_sub_events_df[\"W_score_scaled\"]\n",
    "        current_sub_clean_df[\"A_score_scaled\"] = current_sub_events_df[\"A_score\"] - current_sub_events_df[\"A_score\"].mean()\n",
    "        current_sub_clean_df[\"F_score_scaled\"] = current_sub_events_df[\"F_score\"] - current_sub_events_df[\"F_score\"].mean()\n",
    "        current_sub_clean_df[\"Pos_arousal\"] = current_sub_events_df[\"Pos_arousal\"]\n",
    "        current_sub_clean_df[\"Neg_arousal\"] = current_sub_events_df[\"Neg_arousal\"]\n",
    "        current_sub_clean_df[\"Pos_arousal_scaled\"] = current_sub_events_df[\"Pos_arousal_scaled\"]\n",
    "        current_sub_clean_df[\"Neg_arousal_scaled\"] = current_sub_events_df[\"Neg_arousal_scaled\"]\n",
    "        \n",
    "        # Append the current_sub_events_df to the all_subs_events_df.\n",
    "        all_subs_events_df = pd.concat([all_subs_events_df, current_sub_clean_df], ignore_index=True)\n",
    "\n",
    "    # Drop columns with scores missing. \n",
    "    drop_id = all_subs_events_df[(all_subs_events_df[\"W_score\"] == 0) | (all_subs_events_df[\"A_score\"] == 0) | (all_subs_events_df[\"F_score\"] == 0)]\n",
    "    all_subs_events_df.drop(drop_id.index, inplace=True)\n",
    "    all_subs_events_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return all_subs_events_df\n",
    "\n",
    "def get_PeakInformation(all_subs_events_df, r1_keys_sorted, r2_keys_sorted): \n",
    "\n",
    "     # Create column for binary choice to watch the trailer.\n",
    "     all_subs_events_df[\"Watch_choice\"] = \"No\"\n",
    "     all_subs_events_df.loc[all_subs_events_df[\"W_score\"] > 2, \"Watch_choice\"] = \"Yes\"\n",
    "\n",
    "     all_subs_events_watch_yes_df = all_subs_events_df[all_subs_events_df[\"Watch_choice\"] == \"Yes\"]\n",
    "     all_subs_events_watch_no_df = all_subs_events_df[all_subs_events_df[\"Watch_choice\"] == \"No\"]\n",
    "\n",
    "     # Calculate collective peaks. \n",
    "     # Create empty dictionaries to store the peak location for each trailer.\n",
    "     r1_peaks_id = {}\n",
    "     r2_peaks_id = {}\n",
    "\n",
    "     # Loop through each trailer and get the location for where (for most participants) the peak occurs.\n",
    "     for trailer_id in range(len(r1_keys_sorted)):\n",
    "\n",
    "          NAcc_peak_pos_label_r1 = r1_keys_sorted[trailer_id] + \"_NAcc_dem_pos_loc\"    \n",
    "          AIns_peak_pos_label_r1 = r1_keys_sorted[trailer_id] + \"_AIns_dem_pos_loc\"\n",
    "          MPFC_peak_pos_label_r1 = r1_keys_sorted[trailer_id] + \"_MPFC_dem_pos_loc\"\n",
    "\n",
    "          # Using both peak id1 and id2. \n",
    "          NAcc_array_ids_r1 = np.array([all_subs_events_df[all_subs_events_df[\"Trailer\"] == r1_keys_sorted[trailer_id]][\"NAcc_ind_peaks_pos_id\"].to_numpy(), all_subs_events_df[all_subs_events_df[\"Trailer\"] == r1_keys_sorted[trailer_id]][\"NAcc_ind_peaks_pos_id2\"].to_numpy()]).flatten()\n",
    "          AIns_array_ids_r1 = np.array([all_subs_events_df[all_subs_events_df[\"Trailer\"] == r1_keys_sorted[trailer_id]][\"AIns_ind_peaks_pos_id\"].to_numpy(), all_subs_events_df[all_subs_events_df[\"Trailer\"] == r1_keys_sorted[trailer_id]][\"AIns_ind_peaks_pos_id2\"].to_numpy()]).flatten()\n",
    "          MPFC_array_ids_r1 = np.array([all_subs_events_df[all_subs_events_df[\"Trailer\"] == r1_keys_sorted[trailer_id]][\"MPFC_ind_peaks_pos_id\"].to_numpy(), all_subs_events_df[all_subs_events_df[\"Trailer\"] == r1_keys_sorted[trailer_id]][\"MPFC_ind_peaks_pos_id2\"].to_numpy()]).flatten()\n",
    "\n",
    "          # Drop nans from the arrays.\n",
    "          NAcc_array_ids_r1 = NAcc_array_ids_r1[~np.isnan(NAcc_array_ids_r1)]\n",
    "          AIns_array_ids_r1 = AIns_array_ids_r1[~np.isnan(AIns_array_ids_r1)]\n",
    "          MPFC_array_ids_r1 = MPFC_array_ids_r1[~np.isnan(MPFC_array_ids_r1)]\n",
    "\n",
    "          r1_peaks_id[NAcc_peak_pos_label_r1] = st.mode(NAcc_array_ids_r1)[0]\n",
    "          r1_peaks_id[AIns_peak_pos_label_r1] = st.mode(AIns_array_ids_r1)[0]\n",
    "          r1_peaks_id[MPFC_peak_pos_label_r1] = st.mode(MPFC_array_ids_r1)[0]\n",
    "\n",
    "          # Now for RUN2...\n",
    "          NAcc_peak_pos_label_r2 = r2_keys_sorted[trailer_id] + \"_NAcc_dem_pos_loc\"\n",
    "          AIns_peak_pos_label_r2 = r2_keys_sorted[trailer_id] + \"_AIns_dem_pos_loc\"\n",
    "          MPFC_peak_pos_label_r2 = r2_keys_sorted[trailer_id] + \"_MPFC_dem_pos_loc\"\n",
    "\n",
    "          # Using both peak id1 and id2. \n",
    "          NAcc_array_ids_r2 = np.array([all_subs_events_df[all_subs_events_df[\"Trailer\"] == r2_keys_sorted[trailer_id]][\"NAcc_ind_peaks_pos_id\"].to_numpy(), all_subs_events_df[all_subs_events_df[\"Trailer\"] == r2_keys_sorted[trailer_id]][\"NAcc_ind_peaks_pos_id2\"].to_numpy()]).flatten()\n",
    "          AIns_array_ids_r2 = np.array([all_subs_events_df[all_subs_events_df[\"Trailer\"] == r2_keys_sorted[trailer_id]][\"AIns_ind_peaks_pos_id\"].to_numpy(), all_subs_events_df[all_subs_events_df[\"Trailer\"] == r2_keys_sorted[trailer_id]][\"AIns_ind_peaks_pos_id2\"].to_numpy()]).flatten()\n",
    "          MPFC_array_ids_r2 = np.array([all_subs_events_df[all_subs_events_df[\"Trailer\"] == r2_keys_sorted[trailer_id]][\"MPFC_ind_peaks_pos_id\"].to_numpy(), all_subs_events_df[all_subs_events_df[\"Trailer\"] == r2_keys_sorted[trailer_id]][\"MPFC_ind_peaks_pos_id2\"].to_numpy()]).flatten()\n",
    "\n",
    "          # Drop nans from the arrays.\n",
    "          NAcc_array_ids_r2 = NAcc_array_ids_r2[~np.isnan(NAcc_array_ids_r2)]\n",
    "          AIns_array_ids_r2 = AIns_array_ids_r2[~np.isnan(AIns_array_ids_r2)]\n",
    "          MPFC_array_ids_r2 = MPFC_array_ids_r2[~np.isnan(MPFC_array_ids_r2)]\n",
    "\n",
    "          r2_peaks_id[NAcc_peak_pos_label_r2] = st.mode(NAcc_array_ids_r2)[0]\n",
    "          r2_peaks_id[AIns_peak_pos_label_r2] = st.mode(AIns_array_ids_r2)[0]\n",
    "          r2_peaks_id[MPFC_peak_pos_label_r2] = st.mode(MPFC_array_ids_r2)[0]\n",
    "\n",
    "          # Now, lets get the positive peaks from people that said watch yes.\n",
    "          NAcc_peak_pos_label_r1_yes = r1_keys_sorted[trailer_id] + \"_NAcc_dem_pos_loc_yes\"\n",
    "          AIns_peak_pos_label_r1_yes = r1_keys_sorted[trailer_id] + \"_AIns_dem_pos_loc_yes\"\n",
    "          MPFC_peak_pos_label_r1_yes = r1_keys_sorted[trailer_id] + \"_MPFC_dem_pos_loc_yes\"\n",
    "\n",
    "          NAcc_YES_array_ids_r1 = np.array([all_subs_events_watch_yes_df[all_subs_events_watch_yes_df[\"Trailer\"] == r1_keys_sorted[trailer_id]][\"NAcc_ind_peaks_pos_id\"].to_numpy(), all_subs_events_watch_yes_df[all_subs_events_watch_yes_df[\"Trailer\"] == r1_keys_sorted[trailer_id]][\"NAcc_ind_peaks_pos_id2\"].to_numpy()]).flatten()\n",
    "          AIns_YES_array_ids_r1 = np.array([all_subs_events_watch_yes_df[all_subs_events_watch_yes_df[\"Trailer\"] == r1_keys_sorted[trailer_id]][\"AIns_ind_peaks_pos_id\"].to_numpy(), all_subs_events_watch_yes_df[all_subs_events_watch_yes_df[\"Trailer\"] == r1_keys_sorted[trailer_id]][\"AIns_ind_peaks_pos_id2\"].to_numpy()]).flatten()\n",
    "          MPFC_YES_array_ids_r1 = np.array([all_subs_events_watch_yes_df[all_subs_events_watch_yes_df[\"Trailer\"] == r1_keys_sorted[trailer_id]][\"MPFC_ind_peaks_pos_id\"].to_numpy(), all_subs_events_watch_yes_df[all_subs_events_watch_yes_df[\"Trailer\"] == r1_keys_sorted[trailer_id]][\"MPFC_ind_peaks_pos_id2\"].to_numpy()]).flatten()\n",
    "\n",
    "          NAcc_YES_array_ids_r1 = NAcc_YES_array_ids_r1[~np.isnan(NAcc_YES_array_ids_r1)]\n",
    "          AIns_YES_array_ids_r1 = AIns_YES_array_ids_r1[~np.isnan(AIns_YES_array_ids_r1)]\n",
    "          MPFC_YES_array_ids_r1 = MPFC_YES_array_ids_r1[~np.isnan(MPFC_YES_array_ids_r1)]\n",
    "\n",
    "          r1_peaks_id[NAcc_peak_pos_label_r1_yes] = st.mode(NAcc_YES_array_ids_r1)[0]\n",
    "          r1_peaks_id[AIns_peak_pos_label_r1_yes] = st.mode(AIns_YES_array_ids_r1)[0]\n",
    "          r1_peaks_id[MPFC_peak_pos_label_r1_yes] = st.mode(MPFC_YES_array_ids_r1)[0]\n",
    "\n",
    "          NAcc_peak_pos_label_r2_yes = r2_keys_sorted[trailer_id] + \"_NAcc_dem_pos_loc_yes\"\n",
    "          AIns_peak_pos_label_r2_yes = r2_keys_sorted[trailer_id] + \"_AIns_dem_pos_loc_yes\"\n",
    "          MPFC_peak_pos_label_r2_yes = r2_keys_sorted[trailer_id] + \"_MPFC_dem_pos_loc_yes\"\n",
    "\n",
    "          NAcc_YES_array_ids_r2 = np.array([all_subs_events_watch_yes_df[all_subs_events_watch_yes_df[\"Trailer\"] == r2_keys_sorted[trailer_id]][\"NAcc_ind_peaks_pos_id\"].to_numpy(), all_subs_events_watch_yes_df[all_subs_events_watch_yes_df[\"Trailer\"] == r2_keys_sorted[trailer_id]][\"NAcc_ind_peaks_pos_id2\"].to_numpy()]).flatten()\n",
    "          AIns_YES_array_ids_r2 = np.array([all_subs_events_watch_yes_df[all_subs_events_watch_yes_df[\"Trailer\"] == r2_keys_sorted[trailer_id]][\"AIns_ind_peaks_pos_id\"].to_numpy(), all_subs_events_watch_yes_df[all_subs_events_watch_yes_df[\"Trailer\"] == r2_keys_sorted[trailer_id]][\"AIns_ind_peaks_pos_id2\"].to_numpy()]).flatten()\n",
    "          MPFC_YES_array_ids_r2 = np.array([all_subs_events_watch_yes_df[all_subs_events_watch_yes_df[\"Trailer\"] == r2_keys_sorted[trailer_id]][\"MPFC_ind_peaks_pos_id\"].to_numpy(), all_subs_events_watch_yes_df[all_subs_events_watch_yes_df[\"Trailer\"] == r2_keys_sorted[trailer_id]][\"MPFC_ind_peaks_pos_id2\"].to_numpy()]).flatten()\n",
    "\n",
    "          NAcc_YES_array_ids_r2 = NAcc_YES_array_ids_r2[~np.isnan(NAcc_YES_array_ids_r2)]\n",
    "          AIns_YES_array_ids_r2 = AIns_YES_array_ids_r2[~np.isnan(AIns_YES_array_ids_r2)]\n",
    "          MPFC_YES_array_ids_r2 = MPFC_YES_array_ids_r2[~np.isnan(MPFC_YES_array_ids_r2)]\n",
    "\n",
    "          r2_peaks_id[NAcc_peak_pos_label_r2_yes] = st.mode(NAcc_YES_array_ids_r2)[0]\n",
    "          r2_peaks_id[AIns_peak_pos_label_r2_yes] = st.mode(AIns_YES_array_ids_r2)[0]\n",
    "          r2_peaks_id[MPFC_peak_pos_label_r2_yes] = st.mode(MPFC_YES_array_ids_r2)[0]\n",
    "\n",
    "          # Now, lets get the positive peaks from people that said watch no.\n",
    "          NAcc_peak_pos_label_r1_no = r1_keys_sorted[trailer_id] + \"_NAcc_dem_pos_loc_no\"\n",
    "          AIns_peak_pos_label_r1_no = r1_keys_sorted[trailer_id] + \"_AIns_dem_pos_loc_no\"\n",
    "          MPFC_peak_pos_label_r1_no = r1_keys_sorted[trailer_id] + \"_MPFC_dem_pos_loc_no\"\n",
    "\n",
    "          r1_peaks_id[NAcc_peak_pos_label_r1_no] = all_subs_events_watch_no_df[all_subs_events_watch_no_df[\"Trailer\"] == r1_keys_sorted[trailer_id]][\"NAcc_ind_peaks_pos_id\"].value_counts(ascending=False).index[0]\n",
    "          r1_peaks_id[AIns_peak_pos_label_r1_no] = all_subs_events_watch_no_df[all_subs_events_watch_no_df[\"Trailer\"] == r1_keys_sorted[trailer_id]][\"AIns_ind_peaks_pos_id\"].value_counts(ascending=False).index[0]\n",
    "          r1_peaks_id[MPFC_peak_pos_label_r1_no] = all_subs_events_watch_no_df[all_subs_events_watch_no_df[\"Trailer\"] == r1_keys_sorted[trailer_id]][\"MPFC_ind_peaks_pos_id\"].value_counts(ascending=False).index[0]\n",
    "\n",
    "          NAcc_peak_pos_label_r2_no = r2_keys_sorted[trailer_id] + \"_NAcc_dem_pos_loc_no\"\n",
    "          AIns_peak_pos_label_r2_no = r2_keys_sorted[trailer_id] + \"_AIns_dem_pos_loc_no\"\n",
    "          MPFC_peak_pos_label_r2_no = r2_keys_sorted[trailer_id] + \"_MPFC_dem_pos_loc_no\"\n",
    "\n",
    "          r2_peaks_id[NAcc_peak_pos_label_r2_no] = all_subs_events_watch_no_df[all_subs_events_watch_no_df[\"Trailer\"] == r2_keys_sorted[trailer_id]][\"NAcc_ind_peaks_pos_id\"].value_counts(ascending=False).index[0]\n",
    "          r2_peaks_id[AIns_peak_pos_label_r2_no] = all_subs_events_watch_no_df[all_subs_events_watch_no_df[\"Trailer\"] == r2_keys_sorted[trailer_id]][\"AIns_ind_peaks_pos_id\"].value_counts(ascending=False).index[0]\n",
    "          r2_peaks_id[MPFC_peak_pos_label_r2_no] = all_subs_events_watch_no_df[all_subs_events_watch_no_df[\"Trailer\"] == r2_keys_sorted[trailer_id]][\"MPFC_ind_peaks_pos_id\"].value_counts(ascending=False).index[0]\n",
    "\n",
    "     # Merge the dictionaries.\n",
    "     all_peaks_id = {**r1_peaks_id, **r2_peaks_id}\n",
    "\n",
    "     return all_peaks_id\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_PeakInformation2(all_subs_events_df, all_peaks_id):\n",
    "\n",
    "        # Create column for binary choice to watch the trailer.\n",
    "        all_subs_events_df[\"Watch_choice\"] = \"No\"\n",
    "        all_subs_events_df.loc[all_subs_events_df[\"W_score\"] > 2, \"Watch_choice\"] = \"Yes\"\n",
    "\n",
    "        all_subs_events_watch_yes_df = all_subs_events_df[all_subs_events_df[\"Watch_choice\"] == \"Yes\"]\n",
    "        all_subs_events_watch_no_df = all_subs_events_df[all_subs_events_df[\"Watch_choice\"] == \"No\"]\n",
    "\n",
    "        numeric_cols = all_subs_events_watch_yes_df.select_dtypes(include=['number']).columns\n",
    "\n",
    "        all_subs_events_watch_yes_df.groupby(\"Trailer\")[numeric_cols].mean()\n",
    "\n",
    "        trailer_avg_peaks_dic = {}\n",
    "\n",
    "        # Loop through each trailer and get the location for where (for most participants) the peak occurs.\n",
    "        for trailer_id in all_subjects_avg_NAcc_timecourse.keys(): \n",
    "\n",
    "                # Get the average timecourse for each ROI.\n",
    "                NAcc_TR1 = all_subs_events_df.loc[(all_subs_events_df[\"Trailer\"] == trailer_id), \"NAcc_TR1\"]\n",
    "                NAcc_TR2 = all_subs_events_df.loc[(all_subs_events_df[\"Trailer\"] == trailer_id), \"NAcc_TR2\"]\n",
    "                NAcc_TR3 = all_subs_events_df.loc[(all_subs_events_df[\"Trailer\"] == trailer_id), \"NAcc_TR3\"]\n",
    "                NAcc_TR4 = all_subs_events_df.loc[(all_subs_events_df[\"Trailer\"] == trailer_id), \"NAcc_TR4\"]\n",
    "                NAcc_TR5 = all_subs_events_df.loc[(all_subs_events_df[\"Trailer\"] == trailer_id), \"NAcc_TR5\"]\n",
    "                NAcc_TR6 = all_subs_events_df.loc[(all_subs_events_df[\"Trailer\"] == trailer_id), \"NAcc_TR6\"]\n",
    "                NAcc_TR7 = all_subs_events_df.loc[(all_subs_events_df[\"Trailer\"] == trailer_id), \"NAcc_TR7\"]\n",
    "                NAcc_TR8 = all_subs_events_df.loc[(all_subs_events_df[\"Trailer\"] == trailer_id), \"NAcc_TR8\"]\n",
    "                NAcc_TR9 = all_subs_events_df.loc[(all_subs_events_df[\"Trailer\"] == trailer_id), \"NAcc_TR9\"]\n",
    "                NAcc_TR10 = all_subs_events_df.loc[(all_subs_events_df[\"Trailer\"] == trailer_id), \"NAcc_TR10\"]\n",
    "                NAcc_TR11 = all_subs_events_df.loc[(all_subs_events_df[\"Trailer\"] == trailer_id), \"NAcc_TR11\"]\n",
    "                NAcc_TR12 = all_subs_events_df.loc[(all_subs_events_df[\"Trailer\"] == trailer_id), \"NAcc_TR12\"]\n",
    "                NAcc_TR13 = all_subs_events_df.loc[(all_subs_events_df[\"Trailer\"] == trailer_id), \"NAcc_TR13\"]\n",
    "                NAcc_TR14 = all_subs_events_df.loc[(all_subs_events_df[\"Trailer\"] == trailer_id), \"NAcc_TR14\"]\n",
    "                NAcc_TR15 = all_subs_events_df.loc[(all_subs_events_df[\"Trailer\"] == trailer_id), \"NAcc_TR15\"]\n",
    "\n",
    "                AIns_TR1 = all_subs_events_df.loc[(all_subs_events_df[\"Trailer\"] == trailer_id), \"AIns_TR1\"]\n",
    "                AIns_TR2 = all_subs_events_df.loc[(all_subs_events_df[\"Trailer\"] == trailer_id), \"AIns_TR2\"]\n",
    "                AIns_TR3 = all_subs_events_df.loc[(all_subs_events_df[\"Trailer\"] == trailer_id), \"AIns_TR3\"]\n",
    "                AIns_TR4 = all_subs_events_df.loc[(all_subs_events_df[\"Trailer\"] == trailer_id), \"AIns_TR4\"]\n",
    "                AIns_TR5 = all_subs_events_df.loc[(all_subs_events_df[\"Trailer\"] == trailer_id), \"AIns_TR5\"]\n",
    "                AIns_TR6 = all_subs_events_df.loc[(all_subs_events_df[\"Trailer\"] == trailer_id), \"AIns_TR6\"]\n",
    "                AIns_TR7 = all_subs_events_df.loc[(all_subs_events_df[\"Trailer\"] == trailer_id), \"AIns_TR7\"]\n",
    "                AIns_TR8 = all_subs_events_df.loc[(all_subs_events_df[\"Trailer\"] == trailer_id), \"AIns_TR8\"]\n",
    "                AIns_TR9 = all_subs_events_df.loc[(all_subs_events_df[\"Trailer\"] == trailer_id), \"AIns_TR9\"]\n",
    "                AIns_TR10 = all_subs_events_df.loc[(all_subs_events_df[\"Trailer\"] == trailer_id), \"AIns_TR10\"]\n",
    "                AIns_TR11 = all_subs_events_df.loc[(all_subs_events_df[\"Trailer\"] == trailer_id), \"AIns_TR11\"]\n",
    "                AIns_TR12 = all_subs_events_df.loc[(all_subs_events_df[\"Trailer\"] == trailer_id), \"AIns_TR12\"]\n",
    "                AIns_TR13 = all_subs_events_df.loc[(all_subs_events_df[\"Trailer\"] == trailer_id), \"AIns_TR13\"]\n",
    "                AIns_TR14 = all_subs_events_df.loc[(all_subs_events_df[\"Trailer\"] == trailer_id), \"AIns_TR14\"]\n",
    "                AIns_TR15 = all_subs_events_df.loc[(all_subs_events_df[\"Trailer\"] == trailer_id), \"AIns_TR15\"]\n",
    "\n",
    "                MPFC_TR1 = all_subs_events_df.loc[(all_subs_events_df[\"Trailer\"] == trailer_id), \"MPFC_TR1\"]\n",
    "                MPFC_TR2 = all_subs_events_df.loc[(all_subs_events_df[\"Trailer\"] == trailer_id), \"MPFC_TR2\"]\n",
    "                MPFC_TR3 = all_subs_events_df.loc[(all_subs_events_df[\"Trailer\"] == trailer_id), \"MPFC_TR3\"]\n",
    "                MPFC_TR4 = all_subs_events_df.loc[(all_subs_events_df[\"Trailer\"] == trailer_id), \"MPFC_TR4\"]\n",
    "                MPFC_TR5 = all_subs_events_df.loc[(all_subs_events_df[\"Trailer\"] == trailer_id), \"MPFC_TR5\"]\n",
    "                MPFC_TR6 = all_subs_events_df.loc[(all_subs_events_df[\"Trailer\"] == trailer_id), \"MPFC_TR6\"]\n",
    "                MPFC_TR7 = all_subs_events_df.loc[(all_subs_events_df[\"Trailer\"] == trailer_id), \"MPFC_TR7\"]\n",
    "                MPFC_TR8 = all_subs_events_df.loc[(all_subs_events_df[\"Trailer\"] == trailer_id), \"MPFC_TR8\"]\n",
    "                MPFC_TR9 = all_subs_events_df.loc[(all_subs_events_df[\"Trailer\"] == trailer_id), \"MPFC_TR9\"]\n",
    "                MPFC_TR10 = all_subs_events_df.loc[(all_subs_events_df[\"Trailer\"] == trailer_id), \"MPFC_TR10\"]\n",
    "                MPFC_TR11 = all_subs_events_df.loc[(all_subs_events_df[\"Trailer\"] == trailer_id), \"MPFC_TR11\"]\n",
    "                MPFC_TR12 = all_subs_events_df.loc[(all_subs_events_df[\"Trailer\"] == trailer_id), \"MPFC_TR12\"]\n",
    "                MPFC_TR13 = all_subs_events_df.loc[(all_subs_events_df[\"Trailer\"] == trailer_id), \"MPFC_TR13\"]\n",
    "                MPFC_TR14 = all_subs_events_df.loc[(all_subs_events_df[\"Trailer\"] == trailer_id), \"MPFC_TR14\"]\n",
    "                MPFC_TR15 = all_subs_events_df.loc[(all_subs_events_df[\"Trailer\"] == trailer_id), \"MPFC_TR15\"]\n",
    "\n",
    "                # Get timecourse for each ROI.\n",
    "                NAcc_timecourse = np.array([NAcc_TR1, NAcc_TR2, NAcc_TR3, NAcc_TR4, NAcc_TR5, NAcc_TR6, NAcc_TR7, NAcc_TR8, NAcc_TR9, NAcc_TR10, NAcc_TR11, NAcc_TR12, NAcc_TR13, NAcc_TR14, NAcc_TR15])\n",
    "                AIns_timecourse = np.array([AIns_TR1, AIns_TR2, AIns_TR3, AIns_TR4, AIns_TR5, AIns_TR6, AIns_TR7, AIns_TR8, AIns_TR9, AIns_TR10, AIns_TR11, AIns_TR12, AIns_TR13, AIns_TR14, AIns_TR15])\n",
    "                MPFC_timecourse = np.array([MPFC_TR1, MPFC_TR2, MPFC_TR3, MPFC_TR4, MPFC_TR5, MPFC_TR6, MPFC_TR7, MPFC_TR8, MPFC_TR9, MPFC_TR10, MPFC_TR11, MPFC_TR12, MPFC_TR13, MPFC_TR14, MPFC_TR15])\n",
    "\n",
    "                # Get the average timecourse for each ROI.\n",
    "                NAcc_avg_timecourse = np.nanmean(NAcc_timecourse, axis=1)        \n",
    "                AIns_avg_timecourse = np.nanmean(AIns_timecourse, axis=1)\n",
    "                MPFC_avg_timecourse = np.nanmean(MPFC_timecourse, axis=1)\n",
    "\n",
    "                # Get the std timecourse for each ROI.\n",
    "                NAcc_err_timecourse = np.nanstd(NAcc_timecourse, axis =1)/np.sqrt(NAcc_timecourse.shape[1])\n",
    "                AIns_err_timecourse = np.nanstd(AIns_timecourse, axis=1)/np.sqrt(AIns_timecourse.shape[1])\n",
    "                MPFC_err_timecourse = np.nanstd(MPFC_timecourse, axis=1)/np.sqrt(MPFC_timecourse.shape[1])\n",
    "\n",
    "                # Get the average peak location for each ROI. \n",
    "                NAcc_avg_peak_pos = np.argmax(NAcc_avg_timecourse)\n",
    "                AIns_avg_peak_pos = np.argmax(AIns_avg_timecourse)\n",
    "                MPFC_avg_peak_pos = np.argmax(MPFC_avg_timecourse)\n",
    "\n",
    "                NAcc_avg_peak_neg = np.argmin(NAcc_avg_timecourse)\n",
    "                AIns_avg_peak_neg = np.argmin(AIns_avg_timecourse)\n",
    "                MPFC_avg_peak_neg = np.argmin(MPFC_avg_timecourse)\n",
    "        \n",
    "                trailer_avg_peaks_dic[trailer_id + \"_NAcc_avg_peak_pos\"] = NAcc_avg_peak_pos\n",
    "                trailer_avg_peaks_dic[trailer_id + \"_AIns_avg_peak_pos\"] = AIns_avg_peak_pos\n",
    "                trailer_avg_peaks_dic[trailer_id + \"_MPFC_avg_peak_pos\"] = MPFC_avg_peak_pos\n",
    "        \n",
    "                trailer_avg_peaks_dic[trailer_id + \"_NAcc_avg_peak_neg\"] = NAcc_avg_peak_neg\n",
    "                trailer_avg_peaks_dic[trailer_id + \"_AIns_avg_peak_neg\"] = AIns_avg_peak_neg\n",
    "                trailer_avg_peaks_dic[trailer_id + \"_MPFC_avg_peak_neg\"] = MPFC_avg_peak_neg\n",
    "\n",
    "                # Get the Inter-subject sim peak location for each ROI.\n",
    "                NAcc_ISS_timecourse = NAcc_avg_timecourse/NAcc_err_timecourse\n",
    "                AIns_ISS_timecourse = AIns_avg_timecourse/AIns_err_timecourse\n",
    "                MPFC_ISS_timecourse = MPFC_avg_timecourse/MPFC_err_timecourse\n",
    "\n",
    "                NAcc_ISS_avg_peak_pos = np.argmax(NAcc_ISS_timecourse)\n",
    "                AIns_ISS_avg_peak_pos = np.argmax(AIns_ISS_timecourse)\n",
    "                MPFC_ISS_avg_peak_pos = np.argmax(MPFC_ISS_timecourse)\n",
    "\n",
    "                NAcc_ISS_avg_peak_neg = np.argmin(NAcc_ISS_timecourse)\n",
    "                AIns_ISS_avg_peak_neg = np.argmin(AIns_ISS_timecourse)\n",
    "                MPFC_ISS_avg_peak_neg = np.argmin(MPFC_ISS_timecourse)\n",
    "\n",
    "                trailer_avg_peaks_dic[trailer_id + \"_NAcc_ISS_avg_peak_pos\"] = NAcc_ISS_avg_peak_pos    \n",
    "                trailer_avg_peaks_dic[trailer_id + \"_AIns_ISS_avg_peak_pos\"] = AIns_ISS_avg_peak_pos\n",
    "                trailer_avg_peaks_dic[trailer_id + \"_MPFC_ISS_avg_peak_pos\"] = MPFC_ISS_avg_peak_pos\n",
    "\n",
    "                trailer_avg_peaks_dic[trailer_id + \"_NAcc_ISS_avg_peak_neg\"] = NAcc_ISS_avg_peak_neg\n",
    "                trailer_avg_peaks_dic[trailer_id + \"_AIns_ISS_avg_peak_neg\"] = AIns_ISS_avg_peak_neg\n",
    "                trailer_avg_peaks_dic[trailer_id + \"_MPFC_ISS_avg_peak_neg\"] = MPFC_ISS_avg_peak_neg\n",
    "\n",
    "        # Get avg across trailers. \n",
    "        numeric_cols = all_subs_events_watch_yes_df.select_dtypes(include=['number']).columns\n",
    "        all_subs_events_watch_yes_avg_df = all_subs_events_watch_yes_df.groupby(\"Trailer\")[numeric_cols].mean()\n",
    "        all_subs_events_watch_yes_avg_df.reset_index(inplace=True)\n",
    "\n",
    "        # Loop through each trailer and get the location for where (for most participants) the peak occurs.\n",
    "        for trailer_id in all_subjects_avg_NAcc_timecourse.keys(): \n",
    "\n",
    "                NAcc_TR1 = all_subs_events_watch_yes_avg_df.loc[(all_subs_events_watch_yes_avg_df[\"Trailer\"] == trailer_id), \"NAcc_TR1\"]\n",
    "                NAcc_TR2 = all_subs_events_watch_yes_avg_df.loc[(all_subs_events_watch_yes_avg_df[\"Trailer\"] == trailer_id), \"NAcc_TR2\"]\n",
    "                NAcc_TR3 = all_subs_events_watch_yes_avg_df.loc[(all_subs_events_watch_yes_avg_df[\"Trailer\"] == trailer_id), \"NAcc_TR3\"]\n",
    "                NAcc_TR4 = all_subs_events_watch_yes_avg_df.loc[(all_subs_events_watch_yes_avg_df[\"Trailer\"] == trailer_id), \"NAcc_TR4\"]\n",
    "                NAcc_TR5 = all_subs_events_watch_yes_avg_df.loc[(all_subs_events_watch_yes_avg_df[\"Trailer\"] == trailer_id), \"NAcc_TR5\"]\n",
    "                NAcc_TR6 = all_subs_events_watch_yes_avg_df.loc[(all_subs_events_watch_yes_avg_df[\"Trailer\"] == trailer_id), \"NAcc_TR6\"]\n",
    "                NAcc_TR7 = all_subs_events_watch_yes_avg_df.loc[(all_subs_events_watch_yes_avg_df[\"Trailer\"] == trailer_id), \"NAcc_TR7\"]\n",
    "                NAcc_TR8 = all_subs_events_watch_yes_avg_df.loc[(all_subs_events_watch_yes_avg_df[\"Trailer\"] == trailer_id), \"NAcc_TR8\"]\n",
    "                NAcc_TR9 = all_subs_events_watch_yes_avg_df.loc[(all_subs_events_watch_yes_avg_df[\"Trailer\"] == trailer_id), \"NAcc_TR9\"]\n",
    "                NAcc_TR10 = all_subs_events_watch_yes_avg_df.loc[(all_subs_events_watch_yes_avg_df[\"Trailer\"] == trailer_id), \"NAcc_TR10\"]\n",
    "                NAcc_TR11 = all_subs_events_watch_yes_avg_df.loc[(all_subs_events_watch_yes_avg_df[\"Trailer\"] == trailer_id), \"NAcc_TR11\"]\n",
    "                NAcc_TR12 = all_subs_events_watch_yes_avg_df.loc[(all_subs_events_watch_yes_avg_df[\"Trailer\"] == trailer_id), \"NAcc_TR12\"]\n",
    "                NAcc_TR13 = all_subs_events_watch_yes_avg_df.loc[(all_subs_events_watch_yes_avg_df[\"Trailer\"] == trailer_id), \"NAcc_TR13\"]\n",
    "                NAcc_TR14 = all_subs_events_watch_yes_avg_df.loc[(all_subs_events_watch_yes_avg_df[\"Trailer\"] == trailer_id), \"NAcc_TR14\"]\n",
    "                NAcc_TR15 = all_subs_events_watch_yes_avg_df.loc[(all_subs_events_watch_yes_avg_df[\"Trailer\"] == trailer_id), \"NAcc_TR15\"]\n",
    "\n",
    "                AIns_TR1 = all_subs_events_watch_yes_avg_df.loc[(all_subs_events_watch_yes_avg_df[\"Trailer\"] == trailer_id), \"AIns_TR1\"]\n",
    "                AIns_TR2 = all_subs_events_watch_yes_avg_df.loc[(all_subs_events_watch_yes_avg_df[\"Trailer\"] == trailer_id), \"AIns_TR2\"]\n",
    "                AIns_TR3 = all_subs_events_watch_yes_avg_df.loc[(all_subs_events_watch_yes_avg_df[\"Trailer\"] == trailer_id), \"AIns_TR3\"]\n",
    "                AIns_TR4 = all_subs_events_watch_yes_avg_df.loc[(all_subs_events_watch_yes_avg_df[\"Trailer\"] == trailer_id), \"AIns_TR4\"]\n",
    "                AIns_TR5 = all_subs_events_watch_yes_avg_df.loc[(all_subs_events_watch_yes_avg_df[\"Trailer\"] == trailer_id), \"AIns_TR5\"]\n",
    "                AIns_TR6 = all_subs_events_watch_yes_avg_df.loc[(all_subs_events_watch_yes_avg_df[\"Trailer\"] == trailer_id), \"AIns_TR6\"]\n",
    "                AIns_TR7 = all_subs_events_watch_yes_avg_df.loc[(all_subs_events_watch_yes_avg_df[\"Trailer\"] == trailer_id), \"AIns_TR7\"]\n",
    "                AIns_TR8 = all_subs_events_watch_yes_avg_df.loc[(all_subs_events_watch_yes_avg_df[\"Trailer\"] == trailer_id), \"AIns_TR8\"]\n",
    "                AIns_TR9 = all_subs_events_watch_yes_avg_df.loc[(all_subs_events_watch_yes_avg_df[\"Trailer\"] == trailer_id), \"AIns_TR9\"]\n",
    "                AIns_TR10 = all_subs_events_watch_yes_avg_df.loc[(all_subs_events_watch_yes_avg_df[\"Trailer\"] == trailer_id), \"AIns_TR10\"]\n",
    "                AIns_TR11 = all_subs_events_watch_yes_avg_df.loc[(all_subs_events_watch_yes_avg_df[\"Trailer\"] == trailer_id), \"AIns_TR11\"]\n",
    "                AIns_TR12 = all_subs_events_watch_yes_avg_df.loc[(all_subs_events_watch_yes_avg_df[\"Trailer\"] == trailer_id), \"AIns_TR12\"]\n",
    "                AIns_TR13 = all_subs_events_watch_yes_avg_df.loc[(all_subs_events_watch_yes_avg_df[\"Trailer\"] == trailer_id), \"AIns_TR13\"]\n",
    "                AIns_TR14 = all_subs_events_watch_yes_avg_df.loc[(all_subs_events_watch_yes_avg_df[\"Trailer\"] == trailer_id), \"AIns_TR14\"]\n",
    "                AIns_TR15 = all_subs_events_watch_yes_avg_df.loc[(all_subs_events_watch_yes_avg_df[\"Trailer\"] == trailer_id), \"AIns_TR15\"]\n",
    "\n",
    "                MPFC_TR1 = all_subs_events_watch_yes_avg_df.loc[(all_subs_events_watch_yes_avg_df[\"Trailer\"] == trailer_id), \"MPFC_TR1\"]\n",
    "                MPFC_TR2 = all_subs_events_watch_yes_avg_df.loc[(all_subs_events_watch_yes_avg_df[\"Trailer\"] == trailer_id), \"MPFC_TR2\"]\n",
    "                MPFC_TR3 = all_subs_events_watch_yes_avg_df.loc[(all_subs_events_watch_yes_avg_df[\"Trailer\"] == trailer_id), \"MPFC_TR3\"]\n",
    "                MPFC_TR4 = all_subs_events_watch_yes_avg_df.loc[(all_subs_events_watch_yes_avg_df[\"Trailer\"] == trailer_id), \"MPFC_TR4\"]\n",
    "                MPFC_TR5 = all_subs_events_watch_yes_avg_df.loc[(all_subs_events_watch_yes_avg_df[\"Trailer\"] == trailer_id), \"MPFC_TR5\"]\n",
    "                MPFC_TR6 = all_subs_events_watch_yes_avg_df.loc[(all_subs_events_watch_yes_avg_df[\"Trailer\"] == trailer_id), \"MPFC_TR6\"]\n",
    "                MPFC_TR7 = all_subs_events_watch_yes_avg_df.loc[(all_subs_events_watch_yes_avg_df[\"Trailer\"] == trailer_id), \"MPFC_TR7\"]\n",
    "                MPFC_TR8 = all_subs_events_watch_yes_avg_df.loc[(all_subs_events_watch_yes_avg_df[\"Trailer\"] == trailer_id), \"MPFC_TR8\"]\n",
    "                MPFC_TR9 = all_subs_events_watch_yes_avg_df.loc[(all_subs_events_watch_yes_avg_df[\"Trailer\"] == trailer_id), \"MPFC_TR9\"]\n",
    "                MPFC_TR10 = all_subs_events_watch_yes_avg_df.loc[(all_subs_events_watch_yes_avg_df[\"Trailer\"] == trailer_id), \"MPFC_TR10\"]\n",
    "                MPFC_TR11 = all_subs_events_watch_yes_avg_df.loc[(all_subs_events_watch_yes_avg_df[\"Trailer\"] == trailer_id), \"MPFC_TR11\"]\n",
    "                MPFC_TR12 = all_subs_events_watch_yes_avg_df.loc[(all_subs_events_watch_yes_avg_df[\"Trailer\"] == trailer_id), \"MPFC_TR12\"]\n",
    "                MPFC_TR13 = all_subs_events_watch_yes_avg_df.loc[(all_subs_events_watch_yes_avg_df[\"Trailer\"] == trailer_id), \"MPFC_TR13\"]\n",
    "                MPFC_TR14 = all_subs_events_watch_yes_avg_df.loc[(all_subs_events_watch_yes_avg_df[\"Trailer\"] == trailer_id), \"MPFC_TR14\"]\n",
    "                MPFC_TR15 = all_subs_events_watch_yes_avg_df.loc[(all_subs_events_watch_yes_avg_df[\"Trailer\"] == trailer_id), \"MPFC_TR15\"]\n",
    "\n",
    "                # Create timecourse array. \n",
    "                NAcc_current_trailer_timecourse = np.array([NAcc_TR1, NAcc_TR2, NAcc_TR3, NAcc_TR4, NAcc_TR5, NAcc_TR6, NAcc_TR7, NAcc_TR8, NAcc_TR9, NAcc_TR10, NAcc_TR11, NAcc_TR12, NAcc_TR13, NAcc_TR14, NAcc_TR15])\n",
    "                AIns_current_trailer_timecourse = np.array([AIns_TR1, AIns_TR2, AIns_TR3, AIns_TR4, AIns_TR5, AIns_TR6, AIns_TR7, AIns_TR8, AIns_TR9, AIns_TR10, AIns_TR11, AIns_TR12, AIns_TR13, AIns_TR14, AIns_TR15])\n",
    "                MPFC_current_trailer_timecourse = np.array([MPFC_TR1, MPFC_TR2, MPFC_TR3, MPFC_TR4, MPFC_TR5, MPFC_TR6, MPFC_TR7, MPFC_TR8, MPFC_TR9, MPFC_TR10, MPFC_TR11, MPFC_TR12, MPFC_TR13, MPFC_TR14, MPFC_TR15])\n",
    "\n",
    "                # Get the peak location for each ROI.\n",
    "                NAcc_avg_peak_pos = np.argmax(NAcc_current_trailer_timecourse, axis=0)\n",
    "                AIns_avg_peak_pos = np.argmax(AIns_current_trailer_timecourse, axis=0)\n",
    "                MPFC_avg_peak_pos = np.argmax(MPFC_current_trailer_timecourse, axis=0)\n",
    "\n",
    "                # Store the peaks location in dictionary. \n",
    "                trailer_avg_peaks_dic[trailer_id + \"_NAcc_avg_peak_yes_pos\"] = NAcc_avg_peak_pos\n",
    "                trailer_avg_peaks_dic[trailer_id + \"_AIns_avg_peak_yes_pos\"] = AIns_avg_peak_pos\n",
    "                trailer_avg_peaks_dic[trailer_id + \"_MPFC_avg_peak_yes_pos\"] = MPFC_avg_peak_pos\n",
    "        \n",
    "        # Merge the dictionaries.\n",
    "        all_peaks_id = {**all_peaks_id, **trailer_avg_peaks_dic}\n",
    "\n",
    "        return all_peaks_id\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_PeakInformation3(all_subs_events_df, all_peaks_id):\n",
    "\n",
    "    # Add type information to each trial.\n",
    "    for trial_id in range(all_subs_events_df.shape[0]):\n",
    "\n",
    "        #all_subs_events_df[\"Type\"] = \"Horror\" if 'h' in all_subs_events_df.loc[trial_id, \"Trailer\"] else \"Comedy\"\n",
    "        \n",
    "        # Add Type information to each trial. \n",
    "        if 'h' in all_subs_events_df.loc[trial_id, \"Trailer\"]:\n",
    "\n",
    "            all_subs_events_df.loc[trial_id, \"Type\"] = \"Horror\"\n",
    "        \n",
    "        elif 'c' in all_subs_events_df.loc[trial_id, \"Trailer\"]:\n",
    "                \n",
    "                all_subs_events_df.loc[trial_id, \"Type\"] = \"Comedy\"\n",
    "\n",
    "    \n",
    "\n",
    "    # Get sub-groups based on Comedy and Horror preferences.  \n",
    "    # Get list of processed participants.\n",
    "    participants_list = all_subs_events_df['Participant'].unique()\n",
    "\n",
    "    # Separate dataframes by Genre\n",
    "    neural_activation_horror_df = all_subs_events_df[all_subs_events_df[\"Type\"] == \"Horror\"]\n",
    "    neural_activation_horror_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    neural_activation_comedy_df = all_subs_events_df[all_subs_events_df[\"Type\"] == \"Comedy\"]\n",
    "    neural_activation_comedy_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Get individuals mean preference for a genre. \n",
    "    numeric_cols = neural_activation_comedy_df.select_dtypes(include=['number']).columns\n",
    "    Individuals_mean_comedy_df = neural_activation_comedy_df.groupby(\"Participant\", as_index=False, dropna=False)[numeric_cols].mean()\n",
    "    Individuals_mean_horror_df = neural_activation_horror_df.groupby(\"Participant\", as_index=False, dropna=False)[numeric_cols].mean()\n",
    "    Individuals_mean_comedy_df[\"Type\"] = \"Comedy\"\n",
    "    Individuals_mean_horror_df[\"Type\"] = \"Horror\"\n",
    "\n",
    "    # set threshold to categorize participants as preferring a genre.\n",
    "    Horror_w_threshold = Individuals_mean_horror_df[\"W_score_scaled\"].describe()[\"50%\"]\n",
    "    Comedy_w_threshold = Individuals_mean_comedy_df[\"W_score_scaled\"].describe()[\"50%\"]\n",
    "\n",
    "    Dislike_horror_list = []\n",
    "    Like_horror_list = []\n",
    "\n",
    "    Dislike_comedy_list = []\n",
    "    Like_comedy_list = []\n",
    "\n",
    "    # Loop through participants and categorize them based on their preference for a genre.\n",
    "    for participant_id in range(len(participants_list)):\n",
    "\n",
    "        if Individuals_mean_horror_df.loc[participant_id, \"W_score_scaled\"] < Horror_w_threshold:\n",
    "            Dislike_horror_list.append(participants_list[participant_id])\n",
    "        else: \n",
    "            Like_horror_list.append(participants_list[participant_id])\n",
    "\n",
    "        if Individuals_mean_comedy_df.loc[participant_id, \"W_score_scaled\"] < Comedy_w_threshold: #0\n",
    "            Dislike_comedy_list.append(participants_list[participant_id])\n",
    "        else:\n",
    "            Like_comedy_list.append(participants_list[participant_id])\n",
    "\n",
    "\n",
    "    # Get the location of the peaks based on sub-groups average. \n",
    "\n",
    "    trailer_peaks_subgroup = {}\n",
    "\n",
    "    # Separate dataframes by sub-group preferences. \n",
    "    all_subs_events_LH_df = all_subs_events_df[all_subs_events_df[\"Participant\"].isin(Like_horror_list)]\n",
    "    all_subs_events_LH_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    all_subs_events_LC_df = all_subs_events_df[all_subs_events_df[\"Participant\"].isin(Like_comedy_list)]\n",
    "    all_subs_events_LC_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "    trailer_list = [x for x in all_subjects_avg_NAcc_timecourse.keys()]\n",
    "    comedy_trailers = all_subs_events_df[all_subs_events_df[\"Type\"] == \"Comedy\"][\"Trailer\"].unique()\n",
    "    horror_trailers = all_subs_events_df[all_subs_events_df[\"Type\"] == \"Horror\"][\"Trailer\"].unique()\n",
    "\n",
    "    # Get democratic peaks. \n",
    "    for trailer_id in trailer_list:\n",
    "        \n",
    "        # Get df for current trailer. \n",
    "        # Here, depending on the type of trailer I am selecting the correct dataframe.\n",
    "        # So, if it's a comedy, I am getting the dataframe for people that like comedy.\n",
    "        if trailer_id in comedy_trailers:\n",
    "            current_sub_events_df = all_subs_events_LC_df[all_subs_events_LC_df[\"Trailer\"] == trailer_id]\n",
    "            \n",
    "        elif trailer_id in horror_trailers:\n",
    "            current_sub_events_df = all_subs_events_LH_df[all_subs_events_LH_df[\"Trailer\"] == trailer_id]\n",
    "        else:\n",
    "            print(\"Error: Trailer type not recognized.\")\n",
    "\n",
    "        # Get the locationg of individual peaks for current trailer.\n",
    "        NAcc_ind_peaks_ids = np.array(current_sub_events_df[\"NAcc_ind_peaks_pos_id\"]).astype(int)\n",
    "        AIns_ind_peaks_ids = np.array(current_sub_events_df[\"AIns_ind_peaks_pos_id\"]).astype(int)\n",
    "        MPFC_ind_peaks_ids = np.array(current_sub_events_df[\"MPFC_ind_peaks_pos_id\"]).astype(int)\n",
    "\n",
    "        # Drop negative values from the arrays.\n",
    "        NAcc_ind_peaks_ids = NAcc_ind_peaks_ids[NAcc_ind_peaks_ids > 0]\n",
    "        AIns_ind_peaks_ids = AIns_ind_peaks_ids[AIns_ind_peaks_ids > 0]\n",
    "        MPFC_ind_peaks_ids = MPFC_ind_peaks_ids[MPFC_ind_peaks_ids > 0]\n",
    "\n",
    "        # Get the location of the peak based on the most common location.\n",
    "        NAcc_peak_pos = np.bincount(NAcc_ind_peaks_ids).argmax(axis=0)\n",
    "        AIns_peak_pos = np.bincount(AIns_ind_peaks_ids).argmax(axis=0)\n",
    "        MPFC_peak_pos = np.bincount(MPFC_ind_peaks_ids).argmax(axis=0)\n",
    "\n",
    "        # Store the peak location in dictionary.\n",
    "        trailer_peaks_subgroup[trailer_id + \"_NAcc_dem_peak_subgroup\"] = NAcc_peak_pos\n",
    "        trailer_peaks_subgroup[trailer_id + \"_AIns_dem_peak_subgroup\"] = AIns_peak_pos\n",
    "        trailer_peaks_subgroup[trailer_id + \"_MPFC_dem_peak_subgroup\"] = MPFC_peak_pos\n",
    "\n",
    "    # Now get avg peaks. \n",
    "    for trailer_id in trailer_list:\n",
    "\n",
    "        if trailer_id in comedy_trailers:\n",
    "\n",
    "            current_sub_events_df = all_subs_events_df[(all_subs_events_df[\"Trailer\"] == trailer_id) & (all_subs_events_df[\"Participant\"].isin(Like_comedy_list))]\n",
    "\n",
    "        elif trailer_id in horror_trailers:\n",
    "\n",
    "            current_sub_events_df = all_subs_events_df[(all_subs_events_df[\"Trailer\"] == trailer_id) & (all_subs_events_df[\"Participant\"].isin(Like_horror_list))]\n",
    "\n",
    "        else:\n",
    "            print(\"Error: Trailer type not recognized.\")\n",
    "\n",
    "        NAcc_current_timecourse = current_sub_events_df[[\"NAcc_TR1\", \"NAcc_TR2\", \"NAcc_TR3\", \"NAcc_TR4\", \"NAcc_TR5\", \"NAcc_TR6\", \"NAcc_TR7\", \"NAcc_TR8\", \"NAcc_TR9\", \"NAcc_TR10\", \"NAcc_TR11\", \"NAcc_TR12\", \"NAcc_TR13\", \"NAcc_TR14\", \"NAcc_TR15\"]].to_numpy()\n",
    "        AIns_current_timecourse = current_sub_events_df[[\"AIns_TR1\", \"AIns_TR2\", \"AIns_TR3\", \"AIns_TR4\", \"AIns_TR5\", \"AIns_TR6\", \"AIns_TR7\", \"AIns_TR8\", \"AIns_TR9\", \"AIns_TR10\", \"AIns_TR11\", \"AIns_TR12\", \"AIns_TR13\", \"AIns_TR14\", \"AIns_TR15\"]].to_numpy()\n",
    "        MPFC_current_timecourse = current_sub_events_df[[\"MPFC_TR1\", \"MPFC_TR2\", \"MPFC_TR3\", \"MPFC_TR4\", \"MPFC_TR5\", \"MPFC_TR6\", \"MPFC_TR7\", \"MPFC_TR8\", \"MPFC_TR9\", \"MPFC_TR10\", \"MPFC_TR11\", \"MPFC_TR12\", \"MPFC_TR13\", \"MPFC_TR14\", \"MPFC_TR15\"]].to_numpy()\n",
    "\n",
    "        # Get the average timecourse for the current trailer.\n",
    "        NAcc_current_trailer_timecourse = np.mean(NAcc_current_timecourse, axis=0)\n",
    "        AIns_current_trailer_timecourse = np.mean(AIns_current_timecourse, axis=0)\n",
    "        MPFC_current_trailer_timecourse = np.mean(MPFC_current_timecourse, axis=0)\n",
    "\n",
    "        # Get the peak location for each ROI.\n",
    "        NAcc_avg_peak_pos = np.argmax(NAcc_current_trailer_timecourse, axis=0)\n",
    "        AIns_avg_peak_pos = np.argmax(AIns_current_trailer_timecourse, axis=0)\n",
    "        MPFC_avg_peak_pos = np.argmax(MPFC_current_trailer_timecourse, axis=0)\n",
    "\n",
    "        # Store the peaks location in dictionary.\n",
    "        trailer_peaks_subgroup[trailer_id + \"_NAcc_avg_peak_subgroup\"] = NAcc_avg_peak_pos\n",
    "        trailer_peaks_subgroup[trailer_id + \"_AIns_avg_peak_subgroup\"] = AIns_avg_peak_pos\n",
    "        trailer_peaks_subgroup[trailer_id + \"_MPFC_avg_peak_subgroup\"] = MPFC_avg_peak_pos\n",
    "\n",
    "    # Merge the dictionaries.\n",
    "    all_peaks_id = {**all_peaks_id, **trailer_peaks_subgroup}\n",
    "\n",
    "    return all_peaks_id, Like_horror_list, Like_comedy_list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FinalIndividualProcessing(all_subs_events_df, all_peaks_id):\n",
    "\n",
    "    # Add columns for individual peaks.\n",
    "    all_subs_events_df[\"NAcc_IP\"] = 0\n",
    "    all_subs_events_df[\"AIns_IP\"] = 0\n",
    "    all_subs_events_df[\"MPFC_IP\"] = 0\n",
    "\n",
    "    all_subs_events_df[\"NAcc_IP1\"] = 0\n",
    "    all_subs_events_df[\"NAcc_IP2\"] = 0\n",
    "    all_subs_events_df[\"NAcc_IP3\"] = 0\n",
    "\n",
    "    all_subs_events_df[\"AIns_IP1\"] = 0\n",
    "    all_subs_events_df[\"AIns_IP2\"] = 0\n",
    "    all_subs_events_df[\"AIns_IP3\"] = 0\n",
    "\n",
    "    all_subs_events_df[\"MPFC_IP1\"] = 0\n",
    "    all_subs_events_df[\"MPFC_IP2\"] = 0\n",
    "    all_subs_events_df[\"MPFC_IP3\"] = 0\n",
    "\n",
    "    # Add columns for Modal peak.\n",
    "    all_subs_events_df[\"NAcc_DP\"] = 0\n",
    "    all_subs_events_df[\"AIns_DP\"] = 0\n",
    "    all_subs_events_df[\"MPFC_DP\"] = 0\n",
    "\n",
    "    all_subs_events_df[\"NAcc_DP1\"] = 0\n",
    "    all_subs_events_df[\"NAcc_DP2\"] = 0\n",
    "    all_subs_events_df[\"NAcc_DP3\"] = 0\n",
    "\n",
    "    # Declare extended peaks.\n",
    "    all_subs_events_df[\"AIns_DP4\"] = 0\n",
    "    all_subs_events_df[\"AIns_DP5\"] = 0\n",
    "    all_subs_events_df[\"AIns_DP6\"] = 0\n",
    "    all_subs_events_df[\"AIns_DP7\"] = 0\n",
    "    all_subs_events_df[\"AIns_DP8\"] = 0\n",
    "\n",
    "    all_subs_events_df[\"MPFC_DP2\"] = 0\n",
    "    all_subs_events_df[\"MPFC_DP3\"] = 0\n",
    "    all_subs_events_df[\"MPFC_DP4\"] = 0\n",
    "\n",
    "    # Declare DPy peaks.\n",
    "    all_subs_events_df[\"NAcc_DPy\"] = 0\n",
    "    all_subs_events_df[\"AIns_DPy\"] = 0\n",
    "    all_subs_events_df[\"MPFC_DPy\"] = 0\n",
    "\n",
    "    # Declare AVG peaks.\n",
    "    all_subs_events_df[\"NAcc_AP\"] = 0\n",
    "    all_subs_events_df[\"AIns_AP\"] = 0\n",
    "    all_subs_events_df[\"MPFC_AP\"] = 0\n",
    "\n",
    "    all_subs_events_df[\"NAcc_AP1\"] = 0\n",
    "    all_subs_events_df[\"NAcc_AP2\"] = 0\n",
    "    all_subs_events_df[\"NAcc_AP3\"] = 0\n",
    "\n",
    "    all_subs_events_df[\"AIns_AP4\"] = 0\n",
    "    all_subs_events_df[\"AIns_AP5\"] = 0\n",
    "    all_subs_events_df[\"AIns_AP6\"] = 0\n",
    "    all_subs_events_df[\"AIns_AP7\"] = 0\n",
    "    all_subs_events_df[\"AIns_AP8\"] = 0\n",
    "\n",
    "    all_subs_events_df[\"NAcc_APy\"] = 0\n",
    "    all_subs_events_df[\"AIns_APy\"] = 0\n",
    "    all_subs_events_df[\"MPFC_APys\"] = 0\n",
    "\n",
    "    all_subs_events_df[\"NAcc_AP2\"] = 0\n",
    "    all_subs_events_df[\"AIns_AP2\"] = 0\n",
    "    all_subs_events_df[\"MPFC_AP2\"] = 0\n",
    "\n",
    "    all_subs_events_df[\"NAcc_APy2\"] = 0\n",
    "    all_subs_events_df[\"AIns_APy2\"] = 0\n",
    "    all_subs_events_df[\"MPFC_APy2\"] = 0\n",
    "\n",
    "    all_subs_events_df[\"NAcc_seg1\"] = 0\n",
    "    all_subs_events_df[\"AIns_seg1\"] = 0\n",
    "    all_subs_events_df[\"MPFC_seg1\"] = 0\n",
    "\n",
    "    all_subs_events_df[\"NAcc_seg2\"] = 0\n",
    "    all_subs_events_df[\"AIns_seg2\"] = 0\n",
    "    all_subs_events_df[\"MPFC_seg2\"] = 0\n",
    "\n",
    "    all_subs_events_df[\"NAcc_seg3\"] = 0\n",
    "    all_subs_events_df[\"AIns_seg3\"] = 0\n",
    "    all_subs_events_df[\"MPFC_seg3\"] = 0\n",
    "\n",
    "    all_subs_events_df[\"NAcc_seg4\"] = 0\n",
    "    all_subs_events_df[\"AIns_seg4\"] = 0\n",
    "    all_subs_events_df[\"MPFC_seg4\"] = 0\n",
    "\n",
    "    all_subs_events_df[\"NAcc_seg5\"] = 0\n",
    "    all_subs_events_df[\"AIns_seg5\"] = 0\n",
    "    all_subs_events_df[\"MPFC_seg5\"] = 0\n",
    "\n",
    "    all_subs_events_df[\"NAcc_long_avg\"] = 0\n",
    "    all_subs_events_df[\"AIns_long_avg\"] = 0\n",
    "    all_subs_events_df[\"MPFC_long_avg\"] = 0\n",
    "\n",
    "    all_subs_events_df[\"NAcc_DP_subgroup\"] = 0\n",
    "    all_subs_events_df[\"AIns_DP_subgroup\"] = 0\n",
    "    all_subs_events_df[\"MPFC_DP_subgroup\"] = 0\n",
    "\n",
    "    all_subs_events_df[\"NAcc_AP_subgroup\"] = 0\n",
    "    all_subs_events_df[\"AIns_AP_subgroup\"] = 0\n",
    "    all_subs_events_df[\"MPFC_AP_subgroup\"] = 0\n",
    "\n",
    "    # Define ISS peaks.\n",
    "    all_subs_events_df[\"NAcc_ISSP\"] = 0\n",
    "    all_subs_events_df[\"AIns_ISSP\"] = 0\n",
    "    all_subs_events_df[\"MPFC_ISSP\"] = 0\n",
    "\n",
    "    all_subs_events_df[\"NAcc_ISSP1\"] = 0\n",
    "    all_subs_events_df[\"NAcc_ISSP2\"] = 0\n",
    "    all_subs_events_df[\"NAcc_ISSP3\"] = 0\n",
    "\n",
    "    all_subs_events_df[\"AIns_ISSP1\"] = 0\n",
    "    all_subs_events_df[\"AIns_ISSP2\"] = 0\n",
    "    all_subs_events_df[\"AIns_ISSP3\"] = 0\n",
    "\n",
    "    all_subs_events_df[\"MPFC_ISSP1\"] = 0\n",
    "    all_subs_events_df[\"MPFC_ISSP2\"] = 0\n",
    "    all_subs_events_df[\"MPFC_ISSP3\"] = 0\n",
    "\n",
    "    # Calculate the average peak location (for collective peak) for each trailer.\n",
    "\n",
    "    # Loop through each participant. \n",
    "    for sub_num in particpants_list:\n",
    "\n",
    "        current_sub = str(\"sub-\" + sub_num)\n",
    "\n",
    "        current_sub_dic = participant_dictionaries[current_sub]\n",
    "\n",
    "        current_sub_AIM_path = root_path + \"/\" + current_sub + \"/\" + current_sub + \"_AIM_ROI_NeuralActivation.csv\"\n",
    "\n",
    "        current_sub_AIM_df = pd.read_csv(current_sub_AIM_path)\n",
    "\n",
    "        trailer_keys = list(current_sub_dic.keys())\n",
    "\n",
    "        # Loop through each trailer.\n",
    "        for trailer_id in range(len(trailer_keys)):\n",
    "\n",
    "            # Create keys to access AIM_df columns.\n",
    "            NAcc_key = trailer_keys[id_trailer] + \"_bNAcc\" \n",
    "            AIns_key = trailer_keys[id_trailer] + \"_bAIns\"\n",
    "            MPFC_key = trailer_keys[id_trailer] + \"_bMPFC\"\n",
    "\n",
    "            # Get the timecourse for each ROI.\n",
    "            NAcc_timeseries = participant_dictionaries[current_sub][trailer_keys[trailer_id]][\"Bilateral_NAcc\"]\n",
    "            AIns_timeseries = participant_dictionaries[current_sub][trailer_keys[trailer_id]][\"Bilateral_AIns\"]\n",
    "            MPFC_timeseries = participant_dictionaries[current_sub][trailer_keys[trailer_id]][\"Bilateral_MPFC\"]\n",
    "\n",
    "            # Get location of modal peak for current trailer.\n",
    "            NAcc_pos_peak_label = trailer_keys[trailer_id] + \"_NAcc_dem_pos_loc\"   \n",
    "            AIns_pos_peak_label = trailer_keys[trailer_id] + \"_AIns_dem_pos_loc\"  \n",
    "            MPFC_pos_peak_label = trailer_keys[trailer_id] + \"_MPFC_dem_pos_loc\"  \n",
    "\n",
    "            # Get modal peak for the given location for each ROI (for watch yes).\n",
    "            NAcc_pos_peak_label_yes = trailer_keys[trailer_id] + \"_NAcc_dem_pos_loc_yes\"\n",
    "            AIns_pos_peak_label_yes = trailer_keys[trailer_id] + \"_AIns_dem_pos_loc_yes\"\n",
    "            MPFC_pos_peak_label_yes = trailer_keys[trailer_id] + \"_MPFC_dem_pos_loc_yes\"\n",
    "\n",
    "            # Get the average peak location for the current trailer.\n",
    "            NAcc_avg_peak_pos_key = int(all_peaks_id[trailer_keys[trailer_id] + \"_NAcc_avg_peak_pos\"])\n",
    "            AIns_avg_peak_pos_key = int(all_peaks_id[trailer_keys[trailer_id] + \"_AIns_avg_peak_pos\"])\n",
    "            MPFC_avg_peak_pos_key = int(all_peaks_id[trailer_keys[trailer_id] + \"_MPFC_avg_peak_pos\"])\n",
    "\n",
    "            NAcc_avg_peak_pos_yes_key = int(all_peaks_id[trailer_keys[trailer_id] + \"_NAcc_avg_peak_yes_pos\"])\n",
    "            AIns_avg_peak_pos_yes_key = int(all_peaks_id[trailer_keys[trailer_id] + \"_AIns_avg_peak_yes_pos\"])\n",
    "            MPFC_avg_peak_pos_yes_key = int(all_peaks_id[trailer_keys[trailer_id] + \"_MPFC_avg_peak_yes_pos\"])\n",
    "\n",
    "            # Get the location of the ISS peak for current trailer.\n",
    "            NAcc_ISSP_label = trailer_keys[trailer_id] + \"_NAcc_ISS_avg_peak_pos\"   \n",
    "            AIns_ISSP_label = trailer_keys[trailer_id] + \"_AIns_ISS_avg_peak_pos\"  \n",
    "            MPFC_ISSP_label = trailer_keys[trailer_id] + \"_MPFC_ISS_avg_peak_pos\"  \n",
    "\n",
    "            current_NAcc_ISSP_peak_id = all_peaks_id[NAcc_ISSP_label]\n",
    "            current_AIns_ISSP_peak_id = all_peaks_id[AIns_ISSP_label]\n",
    "            current_MPFC_ISSP_peak_id = all_peaks_id[MPFC_ISSP_label]\n",
    "\n",
    "            # Get the location of the individual participant peak for current trailer.\n",
    "            current_individual_NAcc_peak_id = np.argsort(NAcc_timeseries)[-1]\n",
    "            current_individual_AIns_peak_id = np.argsort(AIns_timeseries)[-1]\n",
    "            current_individual_MPFC_peak_id = np.argsort(MPFC_timeseries)[-1]\n",
    "\n",
    "            # Get modal peak for the given location for each ROI.\n",
    "            NAcc_dem_peak_pos = NAcc_timeseries[int(all_peaks_id[NAcc_pos_peak_label]) ]\n",
    "            AIns_dem_peak_pos = AIns_timeseries[int(all_peaks_id[AIns_pos_peak_label]) ]\n",
    "            MPFC_dem_peak_pos = MPFC_timeseries[int(all_peaks_id[MPFC_pos_peak_label]) ]\n",
    "\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"NAcc_DP\"] = NAcc_dem_peak_pos\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"AIns_DP\"] = AIns_dem_peak_pos\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"MPFC_DP\"] = MPFC_dem_peak_pos\n",
    "\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"NAcc_DP1\"] = getPeak(NAcc_timeseries, all_peaks_id[NAcc_pos_peak_label], 1)\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"NAcc_DP2\"] = getPeak(NAcc_timeseries, all_peaks_id[NAcc_pos_peak_label], 2)\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"NAcc_DP3\"] = getPeak(NAcc_timeseries, all_peaks_id[NAcc_pos_peak_label], 3)\n",
    "\n",
    "            # Get extended DP for AIns.\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"AIns_DP1\"] = getPeak(AIns_timeseries, all_peaks_id[AIns_pos_peak_label_yes], 1)\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"AIns_DP2\"] = getPeak(AIns_timeseries, all_peaks_id[AIns_pos_peak_label_yes], 2)   \n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"AIns_DP3\"] = getPeak(AIns_timeseries, all_peaks_id[AIns_pos_peak_label_yes], 3)\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"AIns_DP4\"] = getPeak(AIns_timeseries, all_peaks_id[AIns_pos_peak_label_yes], 4)\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"AIns_DP5\"] = getPeak(AIns_timeseries, all_peaks_id[AIns_pos_peak_label_yes], 5)\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"AIns_DP6\"] = getPeak(AIns_timeseries, all_peaks_id[AIns_pos_peak_label_yes], 6)\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"AIns_DP7\"] = getPeak(AIns_timeseries, all_peaks_id[AIns_pos_peak_label_yes], 7)\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"AIns_DP8\"] = getPeak(AIns_timeseries, all_peaks_id[AIns_pos_peak_label_yes], 8)\n",
    "\n",
    "            # Get extended DP for MPFC.\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"MPFC_DP2\"] = getPeak(MPFC_timeseries, all_peaks_id[MPFC_pos_peak_label], 2)\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"MPFC_DP3\"] = getPeak(MPFC_timeseries, all_peaks_id[MPFC_pos_peak_label], 3)\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"MPFC_DP4\"] = getPeak(MPFC_timeseries, all_peaks_id[MPFC_pos_peak_label], 4)\n",
    "\n",
    "            # Get DPy.\n",
    "            NAcc_dem_peak_pos_yes = NAcc_timeseries[int(all_peaks_id[NAcc_pos_peak_label_yes])]\n",
    "            AIns_dem_peak_pos_yes = AIns_timeseries[int(all_peaks_id[AIns_pos_peak_label_yes])]\n",
    "            MPFC_dem_peak_pos_yes = MPFC_timeseries[int(all_peaks_id[MPFC_pos_peak_label_yes])]\n",
    "\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"NAcc_DPy\"] = np.nanmean(NAcc_dem_peak_pos_yes)\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"AIns_DPy\"] = np.nanmean(AIns_dem_peak_pos_yes)\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"MPFC_DPy\"] = np.nanmean(MPFC_dem_peak_pos_yes)\n",
    "\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"NAcc_DPy1\"] = getPeak(NAcc_timeseries, all_peaks_id[NAcc_pos_peak_label_yes], 1)\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"NAcc_DPy2\"] = getPeak(NAcc_timeseries, all_peaks_id[NAcc_pos_peak_label_yes], 2)\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"NAcc_DPy3\"] = getPeak(NAcc_timeseries, all_peaks_id[NAcc_pos_peak_label_yes], 3)\n",
    "\n",
    "            # Get the individual peaks for the current trailer. \n",
    "            NAcc_ind_peak = NAcc_timeseries[current_individual_NAcc_peak_id]\n",
    "            AIns_ind_peak = AIns_timeseries[current_individual_AIns_peak_id]\n",
    "            MPFC_ind_peak = MPFC_timeseries[current_individual_MPFC_peak_id]\n",
    "\n",
    "            # Store the individual peaks in the dataframe.\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"NAcc_IP\"] = NAcc_ind_peak\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"AIns_IP\"] = AIns_ind_peak\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"MPFC_IP\"] = MPFC_ind_peak\n",
    "\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"NAcc_IP1\"] = getPeakB(NAcc_timeseries, current_individual_NAcc_peak_id, 1)\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"NAcc_IP2\"] = getPeakB(NAcc_timeseries, current_individual_NAcc_peak_id, 2)\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"NAcc_IP3\"] = getPeakB(NAcc_timeseries, current_individual_NAcc_peak_id, 3)\n",
    "\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"AIns_IP1\"] = getPeakB(AIns_timeseries, current_individual_AIns_peak_id, 1)\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"AIns_IP2\"] = getPeakB(AIns_timeseries, current_individual_AIns_peak_id, 2)\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"AIns_IP3\"] = getPeakB(AIns_timeseries, current_individual_AIns_peak_id, 3)\n",
    "\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"MPFC_IP1\"] = getPeakB(MPFC_timeseries, current_individual_MPFC_peak_id, 1)\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"MPFC_IP2\"] = getPeakB(MPFC_timeseries, current_individual_MPFC_peak_id, 2)\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"MPFC_IP3\"] = getPeakB(MPFC_timeseries, current_individual_MPFC_peak_id, 3)\n",
    "\n",
    "            # Get the average peak for the given location for each ROI.\n",
    "            NAcc_avg_peak_pos = NAcc_timeseries[int(all_peaks_id[trailer_keys[trailer_id] + \"_NAcc_avg_peak_pos\"]): int(all_peaks_id[trailer_keys[trailer_id] + \"_NAcc_avg_peak_pos\"]) + 1]\n",
    "            AIns_avg_peak_pos = AIns_timeseries[int(all_peaks_id[trailer_keys[trailer_id] + \"_AIns_avg_peak_pos\"]): int(all_peaks_id[trailer_keys[trailer_id] + \"_AIns_avg_peak_pos\"]) + 1]\n",
    "            MPFC_avg_peak_pos = MPFC_timeseries[int(all_peaks_id[trailer_keys[trailer_id] + \"_MPFC_avg_peak_pos\"]): int(all_peaks_id[trailer_keys[trailer_id] + \"_MPFC_avg_peak_pos\"]) + 1]\n",
    "\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"NAcc_AP\"] = np.nanmean(NAcc_avg_peak_pos)\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"AIns_AP\"] = np.nanmean(AIns_avg_peak_pos)\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"MPFC_AP\"] = np.nanmean(MPFC_avg_peak_pos)\n",
    "\n",
    "            # Get NA_AP peaks. \n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"NAcc_AP1\"] = getPeak(NAcc_timeseries, NAcc_avg_peak_pos_key, 1)\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"NAcc_AP2\"] = getPeak(NAcc_timeseries, NAcc_avg_peak_pos_key, 2)\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"NAcc_AP3\"] = getPeak(NAcc_timeseries, NAcc_avg_peak_pos_key, 3)\n",
    "\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"NAcc_APy1\"] = getPeak(NAcc_timeseries, NAcc_avg_peak_pos_yes_key, 1)\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"NAcc_APy2\"] = getPeak(NAcc_timeseries, NAcc_avg_peak_pos_yes_key, 2)\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"NAcc_APy3\"] = getPeak(NAcc_timeseries, NAcc_avg_peak_pos_yes_key, 3)\n",
    "\n",
    "            # Get the avg peak for the given location for each ROI (for watch yes).\n",
    "            NAcc_avg_peak_pos_yes = NAcc_timeseries[int(all_peaks_id[trailer_keys[trailer_id] + \"_NAcc_avg_peak_yes_pos\"]) : int(all_peaks_id[trailer_keys[trailer_id] + \"_NAcc_avg_peak_yes_pos\"]) + 2]\n",
    "            AIns_avg_peak_pos_yes = AIns_timeseries[int(all_peaks_id[trailer_keys[trailer_id] + \"_AIns_avg_peak_yes_pos\"]) : int(all_peaks_id[trailer_keys[trailer_id] + \"_AIns_avg_peak_yes_pos\"]) + 2]\n",
    "            MPFC_avg_peak_pos_yes = MPFC_timeseries[int(all_peaks_id[trailer_keys[trailer_id] + \"_MPFC_avg_peak_yes_pos\"]) : int(all_peaks_id[trailer_keys[trailer_id] + \"_MPFC_avg_peak_yes_pos\"]) + 2]\n",
    "\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"NAcc_APy\"] = getPeak(NAcc_timeseries, all_peaks_id[trailer_keys[trailer_id] + \"_NAcc_avg_peak_yes_pos\"], 2)\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"AIns_APy\"] = getPeak(AIns_timeseries, all_peaks_id[trailer_keys[trailer_id] + \"_AIns_avg_peak_yes_pos\"], 2)\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"MPFC_APy\"] = getPeak(MPFC_timeseries, all_peaks_id[trailer_keys[trailer_id] + \"_MPFC_avg_peak_yes_pos\"], 2)\n",
    "\n",
    "            # Get the extended peaks for AIns.\n",
    "            AIns_avg_peak_pos4 = AIns_timeseries[int(all_peaks_id[trailer_keys[trailer_id] + \"_AIns_avg_peak_pos\"]): int(all_peaks_id[trailer_keys[trailer_id] + \"_AIns_avg_peak_pos\"]) + 4]\n",
    "            AIns_avg_peak_pos5 = AIns_timeseries[int(all_peaks_id[trailer_keys[trailer_id] + \"_AIns_avg_peak_pos\"]): int(all_peaks_id[trailer_keys[trailer_id] + \"_AIns_avg_peak_pos\"]) + 5]\n",
    "            AIns_avg_peak_pos6 = AIns_timeseries[int(all_peaks_id[trailer_keys[trailer_id] + \"_AIns_avg_peak_pos\"]): int(all_peaks_id[trailer_keys[trailer_id] + \"_AIns_avg_peak_pos\"]) + 6]\n",
    "            AIns_avg_peak_pos7 = AIns_timeseries[int(all_peaks_id[trailer_keys[trailer_id] + \"_AIns_avg_peak_pos\"]): int(all_peaks_id[trailer_keys[trailer_id] + \"_AIns_avg_peak_pos\"]) + 7]\n",
    "            AIns_avg_peak_pos8 = AIns_timeseries[int(all_peaks_id[trailer_keys[trailer_id] + \"_AIns_avg_peak_pos\"]): int(all_peaks_id[trailer_keys[trailer_id] + \"_AIns_avg_peak_pos\"]) + 8]\n",
    "\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"AIns_AP4\"] = np.nanmean(AIns_avg_peak_pos4)\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"AIns_AP5\"] = np.nanmean(AIns_avg_peak_pos5)\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"AIns_AP6\"] = np.nanmean(AIns_avg_peak_pos6)\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"AIns_AP7\"] = np.nanmean(AIns_avg_peak_pos7)\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"AIns_AP8\"] = np.nanmean(AIns_avg_peak_pos8)\n",
    "\n",
    "            # Calculate the avg peak pos for each ROI.\n",
    "            NAcc_avg_peak_pos2 = NAcc_timeseries[int(all_peaks_id[trailer_keys[trailer_id] + \"_NAcc_avg_peak_pos\"]): int(all_peaks_id[trailer_keys[trailer_id] + \"_NAcc_avg_peak_pos\"]) + 2]\n",
    "            AIns_avg_peak_pos2 = AIns_timeseries[int(all_peaks_id[trailer_keys[trailer_id] + \"_AIns_avg_peak_pos\"]): int(all_peaks_id[trailer_keys[trailer_id] + \"_AIns_avg_peak_pos\"]) + 2]\n",
    "            MPFC_avg_peak_pos2 = MPFC_timeseries[int(all_peaks_id[trailer_keys[trailer_id] + \"_MPFC_avg_peak_pos\"]): int(all_peaks_id[trailer_keys[trailer_id] + \"_MPFC_avg_peak_pos\"]) + 2]\n",
    "\n",
    "            # Append the collective peak to the all_subs_events_df.\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"NAcc_AP2\"] = np.nanmean(NAcc_avg_peak_pos2)\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"AIns_AP2\"] = np.nanmean(AIns_avg_peak_pos2)\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"MPFC_AP2\"] = np.nanmean(MPFC_avg_peak_pos2)\n",
    "\n",
    "            # Get the avg peak for the given location for each ROI (for watch yes).\n",
    "            NAcc_avg_peak_pos_yes2 = NAcc_timeseries[int(all_peaks_id[trailer_keys[trailer_id] + \"_NAcc_avg_peak_yes_pos\"]): int(all_peaks_id[trailer_keys[trailer_id] + \"_NAcc_avg_peak_yes_pos\"]) + 2]\n",
    "            AIns_avg_peak_pos_yes2 = AIns_timeseries[int(all_peaks_id[trailer_keys[trailer_id] + \"_AIns_avg_peak_yes_pos\"]): int(all_peaks_id[trailer_keys[trailer_id] + \"_AIns_avg_peak_yes_pos\"]) + 2]\n",
    "            MPFC_avg_peak_pos_yes2 = MPFC_timeseries[int(all_peaks_id[trailer_keys[trailer_id] + \"_MPFC_avg_peak_yes_pos\"]): int(all_peaks_id[trailer_keys[trailer_id] + \"_MPFC_avg_peak_yes_pos\"]) + 2]\n",
    "\n",
    "            # Append the collective peak to the all_subs_events_df.\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"NAcc_APy2\"] = np.nanmean(NAcc_avg_peak_pos_yes2)\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"AIns_APy2\"] = np.nanmean(AIns_avg_peak_pos_yes2)\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"MPFC_APy2\"] = np.nanmean(MPFC_avg_peak_pos_yes2)\n",
    "\n",
    "            # Get the ISSP for each ROI.\n",
    "            NAcc_ISSP = NAcc_timeseries[int(all_peaks_id[NAcc_ISSP_label])]\n",
    "            AIns_ISSP = AIns_timeseries[int(all_peaks_id[AIns_ISSP_label])]\n",
    "            MPFC_ISSP = MPFC_timeseries[int(all_peaks_id[MPFC_ISSP_label])]\n",
    "\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"NAcc_ISSP\"] = NAcc_ISSP\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"AIns_ISSP\"] = AIns_ISSP\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"MPFC_ISSP\"] = MPFC_ISSP\n",
    "\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"NAcc_ISSP1\"] = getPeakB(NAcc_timeseries, current_NAcc_ISSP_peak_id, 1)\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"NAcc_ISSP2\"] = getPeakB(NAcc_timeseries, current_NAcc_ISSP_peak_id, 2)\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"NAcc_ISSP3\"] = getPeakB(NAcc_timeseries, current_NAcc_ISSP_peak_id, 3)\n",
    "\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"AIns_ISSP1\"] = getPeakB(AIns_timeseries, current_AIns_ISSP_peak_id, 1)\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"AIns_ISSP2\"] = getPeakB(AIns_timeseries, current_AIns_ISSP_peak_id, 2)\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"AIns_ISSP3\"] = getPeakB(AIns_timeseries, current_AIns_ISSP_peak_id, 3)\n",
    "\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"MPFC_ISSP1\"] = getPeakB(MPFC_timeseries, current_MPFC_ISSP_peak_id, 1)\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"MPFC_ISSP2\"] = getPeakB(MPFC_timeseries, current_MPFC_ISSP_peak_id, 2)\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"MPFC_ISSP3\"] = getPeakB(MPFC_timeseries, current_MPFC_ISSP_peak_id, 3)\n",
    "\n",
    "            # Get segment activity for each roi.\n",
    "            NAcc_seg1 = current_sub_dic[trailer_keys[trailer_id]][\"Bilateral_NAcc\"][0:3]\n",
    "            AIns_seg1 = current_sub_dic[trailer_keys[trailer_id]][\"Bilateral_AIns\"][0:3]\n",
    "            MPFC_seg1 = current_sub_dic[trailer_keys[trailer_id]][\"Bilateral_MPFC\"][0:3]\n",
    "\n",
    "            NAcc_seg2 = current_sub_dic[trailer_keys[trailer_id]][\"Bilateral_NAcc\"][3:6]\n",
    "            AIns_seg2 = current_sub_dic[trailer_keys[trailer_id]][\"Bilateral_AIns\"][3:6]\n",
    "            MPFC_seg2 = current_sub_dic[trailer_keys[trailer_id]][\"Bilateral_MPFC\"][3:6]\n",
    "\n",
    "            NAcc_seg3 = current_sub_dic[trailer_keys[trailer_id]][\"Bilateral_NAcc\"][6:9]\n",
    "            AIns_seg3 = current_sub_dic[trailer_keys[trailer_id]][\"Bilateral_AIns\"][6:9]\n",
    "            MPFC_seg3 = current_sub_dic[trailer_keys[trailer_id]][\"Bilateral_MPFC\"][6:9]\n",
    "\n",
    "            NAcc_seg4 = current_sub_dic[trailer_keys[trailer_id]][\"Bilateral_NAcc\"][9:12]\n",
    "            AIns_seg4 = current_sub_dic[trailer_keys[trailer_id]][\"Bilateral_AIns\"][9:12]\n",
    "            MPFC_seg4 = current_sub_dic[trailer_keys[trailer_id]][\"Bilateral_MPFC\"][9:12]\n",
    "\n",
    "            NAcc_seg5 = current_sub_dic[trailer_keys[trailer_id]][\"Bilateral_NAcc\"][12:15]\n",
    "            AIns_seg5 = current_sub_dic[trailer_keys[trailer_id]][\"Bilateral_AIns\"][12:15]\n",
    "            MPFC_seg5 = current_sub_dic[trailer_keys[trailer_id]][\"Bilateral_MPFC\"][12:15]\n",
    "\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"NAcc_seg1\"] = np.mean(NAcc_seg1)\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"AIns_seg1\"] = np.mean(AIns_seg1)\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"MPFC_seg1\"] = np.mean(MPFC_seg1)\n",
    "\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"NAcc_seg2\"] = np.mean(NAcc_seg2)\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"AIns_seg2\"] = np.mean(AIns_seg2)\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"MPFC_seg2\"] = np.mean(MPFC_seg2)\n",
    "\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"NAcc_seg3\"] = np.mean(NAcc_seg3)\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"AIns_seg3\"] = np.mean(AIns_seg3)\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"MPFC_seg3\"] = np.mean(MPFC_seg3)\n",
    "\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"NAcc_seg4\"] = np.mean(NAcc_seg4)\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"AIns_seg4\"] = np.mean(AIns_seg4)\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"MPFC_seg4\"] = np.mean(MPFC_seg4)\n",
    "\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"NAcc_seg5\"] = np.mean(NAcc_seg5)\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"AIns_seg5\"] = np.mean(AIns_seg5)\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"MPFC_seg5\"] = np.mean(MPFC_seg5)\n",
    "\n",
    "            # Get the long avg segments. \n",
    "            NAcc_long_avg = current_sub_dic[trailer_keys[trailer_id]][\"Bilateral_NAcc\"][3:12]\n",
    "            AIns_long_avg = current_sub_dic[trailer_keys[trailer_id]][\"Bilateral_AIns\"][3:12]\n",
    "            MPFC_long_avg = current_sub_dic[trailer_keys[trailer_id]][\"Bilateral_MPFC\"][3:12]\n",
    "\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"NAcc_long_avg\"] = np.mean(NAcc_long_avg)\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"AIns_long_avg\"] = np.mean(AIns_long_avg)\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"MPFC_long_avg\"] = np.mean(MPFC_long_avg)\n",
    "\n",
    "            # Get subgroup peaks (dem peak). \n",
    "            NAcc_dem_peak_subgroup = current_sub_dic[trailer_keys[trailer_id]][\"Bilateral_NAcc\"][int(all_peaks_id[trailer_keys[trailer_id] + \"_NAcc_dem_peak_subgroup\"]):int(all_peaks_id[trailer_keys[trailer_id] + \"_NAcc_dem_peak_subgroup\"]) + 1]\n",
    "            AIns_dem_peak_subgroup = current_sub_dic[trailer_keys[trailer_id]][\"Bilateral_AIns\"][int(all_peaks_id[trailer_keys[trailer_id] + \"_AIns_dem_peak_subgroup\"]):int(all_peaks_id[trailer_keys[trailer_id] + \"_AIns_dem_peak_subgroup\"]) + 1]\n",
    "            MPFC_dem_peak_subgroup = current_sub_dic[trailer_keys[trailer_id]][\"Bilateral_MPFC\"][int(all_peaks_id[trailer_keys[trailer_id] + \"_MPFC_dem_peak_subgroup\"]):int(all_peaks_id[trailer_keys[trailer_id] + \"_MPFC_dem_peak_subgroup\"]) + 1]\n",
    "\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"NAcc_DP_subgroup\"] = np.mean(NAcc_dem_peak_subgroup)\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"AIns_DP_subgroup\"] = np.mean(AIns_dem_peak_subgroup)\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"MPFC_DP_subgroup\"] = np.mean(MPFC_dem_peak_subgroup)\n",
    "\n",
    "            NAcc_avg_peak_subgroup = current_sub_dic[trailer_keys[trailer_id]][\"Bilateral_NAcc\"][int(all_peaks_id[trailer_keys[trailer_id] + \"_NAcc_avg_peak_subgroup\"]): int(all_peaks_id[trailer_keys[trailer_id] + \"_NAcc_avg_peak_subgroup\"]) + 1]\n",
    "            AIns_avg_peak_subgroup = current_sub_dic[trailer_keys[trailer_id]][\"Bilateral_AIns\"][int(all_peaks_id[trailer_keys[trailer_id] + \"_AIns_avg_peak_subgroup\"]): int(all_peaks_id[trailer_keys[trailer_id] + \"_AIns_avg_peak_subgroup\"]) + 1]\n",
    "            MPFC_avg_peak_subgroup = current_sub_dic[trailer_keys[trailer_id]][\"Bilateral_MPFC\"][int(all_peaks_id[trailer_keys[trailer_id] + \"_MPFC_avg_peak_subgroup\"]): int(all_peaks_id[trailer_keys[trailer_id] + \"_MPFC_avg_peak_subgroup\"]) + 1]\n",
    "\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"NAcc_AP_subgroup\"] = np.mean(NAcc_avg_peak_subgroup)\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"AIns_AP_subgroup\"] = np.mean(AIns_avg_peak_subgroup)\n",
    "            all_subs_events_df.loc[(all_subs_events_df[\"Participant\"] == current_sub) & (all_subs_events_df[\"Trailer\"] == trailer_keys[trailer_id]), \"MPFC_AP_subgroup\"] = np.mean(MPFC_avg_peak_subgroup)\n",
    "        \n",
    "    return all_subs_events_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_AggregateData(all_subs_events_df):\n",
    "\n",
    "    # Read aggregate metrics dataframe. \n",
    "    aggregate_metricts_path = \"/Users/la/Documents/SpanLab/Aggregate_metrics_final.csv\"\n",
    "    aggregate_metricts_path2 = \"/Users/la/Documents/SpanLab/Aggregate_metrics3.csv\"\n",
    "    aggregate_metricts_path3 = \"/Users/la/Documents/SpanLab/fmrimatch_aggregate_updated.csv\"\n",
    "\n",
    "    aggregate_metrics_df3 = pd.read_csv(aggregate_metricts_path3)\n",
    "\n",
    "    # Remove nan.\n",
    "    aggregate_metrics_df3.dropna(subset=['NumT_WK2_U'], inplace=True)\n",
    "\n",
    "    # Get the aggregate metrics for the neural activation data.\n",
    "    numeric_cols_agg = all_subs_events_df.select_dtypes(include=['number']).columns\n",
    "    aggregate_df = all_subs_events_df.groupby(\"Trailer\", as_index=False, dropna=False)[numeric_cols_agg].mean()\n",
    "    aggregate_df.reset_index(inplace=True)\n",
    "\n",
    "    # Add Type column.\n",
    "    type_list = []\n",
    "    for id_trailer in range(len(aggregate_df)):\n",
    "        if \"c\" in aggregate_df[\"Trailer\"][id_trailer]:\n",
    "            type_list.append(\"comedy\")\n",
    "        else:\n",
    "            type_list.append(\"horror\")\n",
    "\n",
    "    aggregate_df[\"Type\"] = type_list\n",
    "\n",
    "    # Re-order columns.\n",
    "    cols_ordered = ['Trailer', 'Type', 'Pos_arousal', 'Neg_arousal', 'Pos_arousal_scaled', 'Neg_arousal_scaled', #'W_score', 'W_score_scaled',\n",
    "            'NAcc_onset', 'AIns_onset', 'MPFC_onset',\n",
    "            'NAcc_middle', 'AIns_middle', 'MPFC_middle', 'NAcc_offset',\n",
    "            'AIns_offset', 'MPFC_offset', 'NAcc_whole', 'AIns_whole', 'MPFC_whole',\n",
    "\n",
    "            \n",
    "            \"NAcc_DP\", \"AIns_DP\", \"MPFC_DP\",\n",
    "            \"NAcc_DP1\", \"NAcc_DP2\", \"NAcc_DP3\",\n",
    "            \"AIns_DP1\", \"AIns_DP2\", \"AIns_DP3\",\n",
    "            \"AIns_DP4\", \"AIns_DP5\", \"AIns_DP6\", \"AIns_DP7\", \"AIns_DP8\",\n",
    "            \"MPFC_DP2\", \"MPFC_DP3\", \"MPFC_DP4\",\n",
    "            \"NAcc_DPy\", \"AIns_DPy\", \"MPFC_DPy\",\n",
    "            \"NAcc_DPy1\", \"NAcc_DPy2\", \"NAcc_DPy3\",\n",
    "\n",
    "            \"NAcc_IP\", \"AIns_IP\", \"MPFC_IP\",\n",
    "            \"NAcc_IP1\", \"NAcc_IP2\", \"NAcc_IP3\",\n",
    "            \"AIns_IP1\", \"AIns_IP2\", \"AIns_IP3\",\n",
    "            \"MPFC_IP1\", \"MPFC_IP2\", \"MPFC_IP3\",\n",
    "            \"NAcc_ISSP\", \"AIns_ISSP\", \"MPFC_ISSP\",\n",
    "            \"NAcc_ISSP1\", \"NAcc_ISSP2\", \"NAcc_ISSP3\",\n",
    "\n",
    "            \"NAcc_AP\", \"AIns_AP\", \"MPFC_AP\",\n",
    "            \"NAcc_AP1\", \"NAcc_AP2\", \"NAcc_AP3\",\n",
    "            \"NAcc_APy\", \"AIns_APy\", \"MPFC_APy\",\n",
    "            \"NAcc_AP2\", \"AIns_AP2\", \"MPFC_AP2\",\n",
    "            \"NAcc_APy2\", \"AIns_APy2\", \"MPFC_APy2\",\n",
    "            \"NAcc_APy1\", \"NAcc_APy2\", \"NAcc_APy3\",\n",
    "\n",
    "            \"NAcc_DP_subgroup\", \"AIns_DP_subgroup\", \"MPFC_DP_subgroup\",\n",
    "            \"NAcc_AP_subgroup\", \"AIns_AP_subgroup\", \"MPFC_AP_subgroup\",\n",
    "\n",
    "            \"NAcc_seg1\", \"NAcc_seg2\", \"NAcc_seg3\", \"NAcc_seg4\", \"NAcc_seg5\",\n",
    "            \"AIns_seg1\", \"AIns_seg2\", \"AIns_seg3\", \"AIns_seg4\", \"AIns_seg5\",\n",
    "            \"MPFC_seg1\", \"MPFC_seg2\", \"MPFC_seg3\", \"MPFC_seg4\", \"MPFC_seg5\",\n",
    "\n",
    "            \"NAcc_TR1\" , \"NAcc_TR2\", \"NAcc_TR3\", \"NAcc_TR4\", \"NAcc_TR5\", \"NAcc_TR6\", \"NAcc_TR7\", \"NAcc_TR8\",\n",
    "            \"NAcc_TR9\", \"NAcc_TR10\", \"NAcc_TR11\", \"NAcc_TR12\", \"NAcc_TR13\", \"NAcc_TR14\", \"NAcc_TR15\",\n",
    "            \"AIns_TR1\" , \"AIns_TR2\", \"AIns_TR3\", \"AIns_TR4\", \"AIns_TR5\", \"AIns_TR6\", \"AIns_TR7\", \"AIns_TR8\",\n",
    "            \"AIns_TR9\", \"AIns_TR10\", \"AIns_TR11\", \"AIns_TR12\", \"AIns_TR13\", \"AIns_TR14\", \"AIns_TR15\",\n",
    "            \"MPFC_TR1\" , \"MPFC_TR2\", \"MPFC_TR3\", \"MPFC_TR4\", \"MPFC_TR5\", \"MPFC_TR6\", \"MPFC_TR7\", \"MPFC_TR8\",\n",
    "            \"MPFC_TR9\", \"MPFC_TR10\", \"MPFC_TR11\", \"MPFC_TR12\", \"MPFC_TR13\", \"MPFC_TR14\", \"MPFC_TR15\",\n",
    "\n",
    "            ]\n",
    "\n",
    "    aggregate_df = aggregate_df[cols_ordered]\n",
    "\n",
    "    # Join both dataframes.\n",
    "    aggregate_combined_df = aggregate_df.set_index('Trailer').join(aggregate_metrics_df3.set_index('label'), on='Trailer')   \n",
    "\n",
    "    aggregate_combined_df.dropna(subset=['DG_WK1_U'], inplace=True)\n",
    "\n",
    "    aggregate_combined_df.reset_index(inplace=True)\n",
    "\n",
    "    aggregate_combined_col = aggregate_combined_df.columns\n",
    "\n",
    "    print(\"The shape of the combined dataframe is: \", aggregate_combined_df.shape)\n",
    "\n",
    "    columns_to_drop = [\"Onset\", \"Offset\", \n",
    "\n",
    "        'year', 'Release time', 'genre',\n",
    "        'released (1 = yes, 0 = no)', 'url to trailer', #'Unnamed: 33', 'Unnamed: 34', 'Unnamed: 39', 'Unnamed: 40'\n",
    "        'The tool used to get the thumbnail: https://youtu.be/otNh9bTjXWg?si=NJaRcikG3tZUpNBl , size of thumbnail 640*480',\n",
    "        'downloaded', 'video trimmed', 'audio extracted', 'budget']\n",
    "\n",
    "    # Drop columns that are not needed.\n",
    "    print(\"The shape of the aggregate combined dataframe is: \", aggregate_combined_df.shape)\n",
    "\n",
    "    # Calculate first month gross for US.\n",
    "\n",
    "    aggregate_combined_df[\"Gross_M1_updated\"] = aggregate_combined_df[\"DG_WK1_U\"] + aggregate_combined_df[\"DG_WK2_U\"] + aggregate_combined_df[\"DG_WK3_U\"] + aggregate_combined_df[\"DG_WK4_U\"]\n",
    "    aggregate_combined_df[\"Gross_M1_updated_rank\"] = aggregate_combined_df[\"Gross_M1_updated\"].rank()\n",
    "    aggregate_combined_df[\"Gross_M1_updated_log\"] = np.log(aggregate_combined_df[\"Gross_M1_updated\"])\n",
    "\n",
    "    aggregate_combined_horror_df = aggregate_combined_df[aggregate_combined_df[\"Type\"] == \"horror\"]\n",
    "    aggregate_combined_horror_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    aggregate_combined_comedy_df = aggregate_combined_df[aggregate_combined_df[\"Type\"] == \"comedy\"]\n",
    "    aggregate_combined_comedy_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return aggregate_combined_df, aggregate_combined_comedy_df, aggregate_combined_horror_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the combined dataframe is:  (30, 161)\n",
      "The shape of the aggregate combined dataframe is:  (30, 161)\n",
      "When dropping participant:  sub-01\n",
      "The correlation for NAcc_DP1 is:  0.28662332024369636\n",
      "The correlation for NAcc_DP2 is:  0.5752620240268287\n",
      "The correlation for NAcc_DP3 is:  0.6242191374626663\n",
      "The correlation for NAcc_TR3 is:  0.2446895886230237\n",
      "The correlation for NAcc_TR4 is:  0.05674173745816015\n",
      "The correlation for NAcc_seg1 is:  0.3410634888530809\n",
      "The shape of the combined dataframe is:  (30, 161)\n",
      "The shape of the aggregate combined dataframe is:  (30, 161)\n",
      "When dropping participant:  sub-02\n",
      "The correlation for NAcc_DP1 is:  0.13179466497182693\n",
      "The correlation for NAcc_DP2 is:  0.42595007937830043\n",
      "The correlation for NAcc_DP3 is:  0.43946187264274805\n",
      "The correlation for NAcc_TR3 is:  0.252401027204532\n",
      "The correlation for NAcc_TR4 is:  0.018268119815414915\n",
      "The correlation for NAcc_seg1 is:  0.40673021181636115\n",
      "The shape of the combined dataframe is:  (30, 161)\n",
      "The shape of the aggregate combined dataframe is:  (30, 161)\n",
      "When dropping participant:  sub-03\n",
      "The correlation for NAcc_DP1 is:  0.4170554101640181\n",
      "The correlation for NAcc_DP2 is:  0.5873359222052006\n",
      "The correlation for NAcc_DP3 is:  0.6294676895098589\n",
      "The correlation for NAcc_TR3 is:  0.24989215269040532\n",
      "The correlation for NAcc_TR4 is:  0.01646713650254958\n",
      "The correlation for NAcc_seg1 is:  0.4378361298483353\n",
      "The shape of the combined dataframe is:  (30, 161)\n",
      "The shape of the aggregate combined dataframe is:  (30, 161)\n",
      "When dropping participant:  sub-04\n",
      "The correlation for NAcc_DP1 is:  0.37201444849204157\n",
      "The correlation for NAcc_DP2 is:  0.5686045543010998\n",
      "The correlation for NAcc_DP3 is:  0.5238376821590082\n",
      "The correlation for NAcc_TR3 is:  0.26413951283867954\n",
      "The correlation for NAcc_TR4 is:  0.00493903620725579\n",
      "The correlation for NAcc_seg1 is:  0.3790037943503819\n",
      "The shape of the combined dataframe is:  (30, 161)\n",
      "The shape of the aggregate combined dataframe is:  (30, 161)\n",
      "When dropping participant:  sub-08\n",
      "The correlation for NAcc_DP1 is:  0.3440892142874102\n",
      "The correlation for NAcc_DP2 is:  0.6264478421768916\n",
      "The correlation for NAcc_DP3 is:  0.6222143427191834\n",
      "The correlation for NAcc_TR3 is:  0.3605716879835359\n",
      "The correlation for NAcc_TR4 is:  -0.06887571691644946\n",
      "The correlation for NAcc_seg1 is:  0.4575755720592313\n",
      "The shape of the combined dataframe is:  (30, 161)\n",
      "The shape of the aggregate combined dataframe is:  (30, 161)\n",
      "When dropping participant:  sub-09\n",
      "The correlation for NAcc_DP1 is:  0.4050611564656987\n",
      "The correlation for NAcc_DP2 is:  0.6145031918114138\n",
      "The correlation for NAcc_DP3 is:  0.6291824759151199\n",
      "The correlation for NAcc_TR3 is:  0.25135211162391247\n",
      "The correlation for NAcc_TR4 is:  0.12775549079984821\n",
      "The correlation for NAcc_seg1 is:  0.4430454178245011\n",
      "The shape of the combined dataframe is:  (30, 161)\n",
      "The shape of the aggregate combined dataframe is:  (30, 161)\n",
      "When dropping participant:  sub-10\n",
      "The correlation for NAcc_DP1 is:  0.4068999166626896\n",
      "The correlation for NAcc_DP2 is:  0.5830003867415066\n",
      "The correlation for NAcc_DP3 is:  0.6517557864420531\n",
      "The correlation for NAcc_TR3 is:  0.32142177869531297\n",
      "The correlation for NAcc_TR4 is:  -0.013329364333496421\n",
      "The correlation for NAcc_seg1 is:  0.474071770426327\n",
      "The shape of the combined dataframe is:  (30, 161)\n",
      "The shape of the aggregate combined dataframe is:  (30, 161)\n",
      "When dropping participant:  sub-11\n",
      "The correlation for NAcc_DP1 is:  0.32021437271926756\n",
      "The correlation for NAcc_DP2 is:  0.5553627901132031\n",
      "The correlation for NAcc_DP3 is:  0.5997363971503129\n",
      "The correlation for NAcc_TR3 is:  0.25074863436290473\n",
      "The correlation for NAcc_TR4 is:  -0.0016379749140153083\n",
      "The correlation for NAcc_seg1 is:  0.4001550829831747\n",
      "The shape of the combined dataframe is:  (30, 161)\n",
      "The shape of the aggregate combined dataframe is:  (30, 161)\n",
      "When dropping participant:  sub-12\n",
      "The correlation for NAcc_DP1 is:  0.43707531838948566\n",
      "The correlation for NAcc_DP2 is:  0.5692716440527665\n",
      "The correlation for NAcc_DP3 is:  0.6026648019706662\n",
      "The correlation for NAcc_TR3 is:  0.3556476230231741\n",
      "The correlation for NAcc_TR4 is:  0.06327204921218516\n",
      "The correlation for NAcc_seg1 is:  0.5148878741204521\n",
      "The shape of the combined dataframe is:  (30, 161)\n",
      "The shape of the aggregate combined dataframe is:  (30, 161)\n",
      "When dropping participant:  sub-13\n",
      "The correlation for NAcc_DP1 is:  0.40222055923136546\n",
      "The correlation for NAcc_DP2 is:  0.6111038597717942\n",
      "The correlation for NAcc_DP3 is:  0.6593378762037543\n",
      "The correlation for NAcc_TR3 is:  0.2523526739532078\n",
      "The correlation for NAcc_TR4 is:  -0.014196958847331673\n",
      "The correlation for NAcc_seg1 is:  0.4465848996745574\n",
      "The shape of the combined dataframe is:  (30, 161)\n",
      "The shape of the aggregate combined dataframe is:  (30, 161)\n",
      "When dropping participant:  sub-14\n",
      "The correlation for NAcc_DP1 is:  0.461578141967678\n",
      "The correlation for NAcc_DP2 is:  0.632602451301801\n",
      "The correlation for NAcc_DP3 is:  0.6514158332923259\n",
      "The correlation for NAcc_TR3 is:  0.3434103387770142\n",
      "The correlation for NAcc_TR4 is:  -0.04062992613359799\n",
      "The correlation for NAcc_seg1 is:  0.46644915581560903\n",
      "The shape of the combined dataframe is:  (30, 161)\n",
      "The shape of the aggregate combined dataframe is:  (30, 161)\n",
      "When dropping participant:  sub-15\n",
      "The correlation for NAcc_DP1 is:  0.4221063388179439\n",
      "The correlation for NAcc_DP2 is:  0.552989510406342\n",
      "The correlation for NAcc_DP3 is:  0.5892665621433096\n",
      "The correlation for NAcc_TR3 is:  0.25336673827765294\n",
      "The correlation for NAcc_TR4 is:  -0.059997468967223105\n",
      "The correlation for NAcc_seg1 is:  0.4122506029718667\n",
      "The shape of the combined dataframe is:  (30, 161)\n",
      "The shape of the aggregate combined dataframe is:  (30, 161)\n",
      "When dropping participant:  sub-16\n",
      "The correlation for NAcc_DP1 is:  0.439260068687359\n",
      "The correlation for NAcc_DP2 is:  0.5511498249225277\n",
      "The correlation for NAcc_DP3 is:  0.5571947240984552\n",
      "The correlation for NAcc_TR3 is:  0.26558807574605764\n",
      "The correlation for NAcc_TR4 is:  0.05534676643178932\n",
      "The correlation for NAcc_seg1 is:  0.3894929818492931\n",
      "The shape of the combined dataframe is:  (30, 161)\n",
      "The shape of the aggregate combined dataframe is:  (30, 161)\n",
      "When dropping participant:  sub-17\n",
      "The correlation for NAcc_DP1 is:  0.1716966820400822\n",
      "The correlation for NAcc_DP2 is:  0.47585977015568226\n",
      "The correlation for NAcc_DP3 is:  0.4834854996400157\n",
      "The correlation for NAcc_TR3 is:  0.3063006114457395\n",
      "The correlation for NAcc_TR4 is:  0.0522398579492617\n",
      "The correlation for NAcc_seg1 is:  0.4176609227468592\n",
      "The shape of the combined dataframe is:  (30, 161)\n",
      "The shape of the aggregate combined dataframe is:  (30, 161)\n",
      "When dropping participant:  sub-18\n",
      "The correlation for NAcc_DP1 is:  0.307900686182466\n",
      "The correlation for NAcc_DP2 is:  0.5132830645288\n",
      "The correlation for NAcc_DP3 is:  0.48192415188441284\n",
      "The correlation for NAcc_TR3 is:  0.32825914548304225\n",
      "The correlation for NAcc_TR4 is:  0.05301229010310676\n",
      "The correlation for NAcc_seg1 is:  0.49748112150057255\n",
      "The shape of the combined dataframe is:  (30, 161)\n",
      "The shape of the aggregate combined dataframe is:  (30, 161)\n",
      "When dropping participant:  sub-19\n",
      "The correlation for NAcc_DP1 is:  0.4006593230121483\n",
      "The correlation for NAcc_DP2 is:  0.5890978635270198\n",
      "The correlation for NAcc_DP3 is:  0.6623076386246398\n",
      "The correlation for NAcc_TR3 is:  0.2882053638403328\n",
      "The correlation for NAcc_TR4 is:  0.08755713894939417\n",
      "The correlation for NAcc_seg1 is:  0.4739452949516008\n",
      "The shape of the combined dataframe is:  (30, 161)\n",
      "The shape of the aggregate combined dataframe is:  (30, 161)\n",
      "When dropping participant:  sub-21\n",
      "The correlation for NAcc_DP1 is:  0.2561327802838058\n",
      "The correlation for NAcc_DP2 is:  0.5532258055444651\n",
      "The correlation for NAcc_DP3 is:  0.5236094220576856\n",
      "The correlation for NAcc_TR3 is:  0.2541163447146413\n",
      "The correlation for NAcc_TR4 is:  0.0588663468357655\n",
      "The correlation for NAcc_seg1 is:  0.4083165592738144\n",
      "The shape of the combined dataframe is:  (30, 161)\n",
      "The shape of the aggregate combined dataframe is:  (30, 161)\n",
      "When dropping participant:  sub-22\n",
      "The correlation for NAcc_DP1 is:  0.3790556426701727\n",
      "The correlation for NAcc_DP2 is:  0.5929640714108526\n",
      "The correlation for NAcc_DP3 is:  0.6135870141368822\n",
      "The correlation for NAcc_TR3 is:  0.23942968252465596\n",
      "The correlation for NAcc_TR4 is:  0.06812176168153349\n",
      "The correlation for NAcc_seg1 is:  0.4066170005011206\n",
      "The shape of the combined dataframe is:  (30, 161)\n",
      "The shape of the aggregate combined dataframe is:  (30, 161)\n",
      "When dropping participant:  sub-23\n",
      "The correlation for NAcc_DP1 is:  0.5044177960152374\n",
      "The correlation for NAcc_DP2 is:  0.6444464460027715\n",
      "The correlation for NAcc_DP3 is:  0.676900887784345\n",
      "The correlation for NAcc_TR3 is:  0.361361553948114\n",
      "The correlation for NAcc_TR4 is:  0.06946117217094773\n",
      "The correlation for NAcc_seg1 is:  0.48551584537690595\n",
      "The shape of the combined dataframe is:  (30, 161)\n",
      "The shape of the aggregate combined dataframe is:  (30, 161)\n",
      "When dropping participant:  sub-25\n",
      "The correlation for NAcc_DP1 is:  0.3225500336147219\n",
      "The correlation for NAcc_DP2 is:  0.5290897347313362\n",
      "The correlation for NAcc_DP3 is:  0.576281349930148\n",
      "The correlation for NAcc_TR3 is:  0.26183340538837396\n",
      "The correlation for NAcc_TR4 is:  -0.012603532879595147\n",
      "The correlation for NAcc_seg1 is:  0.46272159275910163\n",
      "The shape of the combined dataframe is:  (30, 161)\n",
      "The shape of the aggregate combined dataframe is:  (30, 161)\n",
      "When dropping participant:  sub-26\n",
      "The correlation for NAcc_DP1 is:  0.16551261907152387\n",
      "The correlation for NAcc_DP2 is:  0.5256026240676186\n",
      "The correlation for NAcc_DP3 is:  0.4989975622046797\n",
      "The correlation for NAcc_TR3 is:  0.2805742077071439\n",
      "The correlation for NAcc_TR4 is:  0.03212959852350766\n",
      "The correlation for NAcc_seg1 is:  0.428530627235244\n",
      "The shape of the combined dataframe is:  (30, 161)\n",
      "The shape of the aggregate combined dataframe is:  (30, 161)\n",
      "When dropping participant:  sub-27\n",
      "The correlation for NAcc_DP1 is:  0.427292724463317\n",
      "The correlation for NAcc_DP2 is:  0.6106996235553459\n",
      "The correlation for NAcc_DP3 is:  0.6046095460103416\n",
      "The correlation for NAcc_TR3 is:  0.23397881455165143\n",
      "The correlation for NAcc_TR4 is:  0.07359809974717915\n",
      "The correlation for NAcc_seg1 is:  0.3967837995420567\n",
      "The shape of the combined dataframe is:  (30, 161)\n",
      "The shape of the aggregate combined dataframe is:  (30, 161)\n",
      "When dropping participant:  sub-28\n",
      "The correlation for NAcc_DP1 is:  0.35187499322389054\n",
      "The correlation for NAcc_DP2 is:  0.582183259637941\n",
      "The correlation for NAcc_DP3 is:  0.6038915216419402\n",
      "The correlation for NAcc_TR3 is:  0.2253458015088058\n",
      "The correlation for NAcc_TR4 is:  0.09883655937947976\n",
      "The correlation for NAcc_seg1 is:  0.4246780089942354\n",
      "The shape of the combined dataframe is:  (30, 161)\n",
      "The shape of the aggregate combined dataframe is:  (30, 161)\n",
      "When dropping participant:  sub-29\n",
      "The correlation for NAcc_DP1 is:  0.4937916981285533\n",
      "The correlation for NAcc_DP2 is:  0.617569677005035\n",
      "The correlation for NAcc_DP3 is:  0.6186453143874227\n",
      "The correlation for NAcc_TR3 is:  0.23727161165246727\n",
      "The correlation for NAcc_TR4 is:  0.08171203654714546\n",
      "The correlation for NAcc_seg1 is:  0.334224160053484\n",
      "The shape of the combined dataframe is:  (30, 161)\n",
      "The shape of the aggregate combined dataframe is:  (30, 161)\n",
      "When dropping participant:  sub-30\n",
      "The correlation for NAcc_DP1 is:  0.456607539779402\n",
      "The correlation for NAcc_DP2 is:  0.6952851561050979\n",
      "The correlation for NAcc_DP3 is:  0.6972295608228722\n",
      "The correlation for NAcc_TR3 is:  0.23761852737584455\n",
      "The correlation for NAcc_TR4 is:  0.03821067096199783\n",
      "The correlation for NAcc_seg1 is:  0.4096297543752465\n",
      "The shape of the combined dataframe is:  (30, 161)\n",
      "The shape of the aggregate combined dataframe is:  (30, 161)\n",
      "When dropping participant:  sub-31\n",
      "The correlation for NAcc_DP1 is:  0.20514705556219318\n",
      "The correlation for NAcc_DP2 is:  0.5271521540419224\n",
      "The correlation for NAcc_DP3 is:  0.4868220190165834\n",
      "The correlation for NAcc_TR3 is:  0.2889982661842259\n",
      "The correlation for NAcc_TR4 is:  0.015446143105383336\n",
      "The correlation for NAcc_seg1 is:  0.39512860999063293\n",
      "The shape of the combined dataframe is:  (30, 161)\n",
      "The shape of the aggregate combined dataframe is:  (30, 161)\n",
      "When dropping participant:  sub-32\n",
      "The correlation for NAcc_DP1 is:  0.2981124380534571\n",
      "The correlation for NAcc_DP2 is:  0.5543041529392777\n",
      "The correlation for NAcc_DP3 is:  0.5457020714694978\n",
      "The correlation for NAcc_TR3 is:  0.19976496428403984\n",
      "The correlation for NAcc_TR4 is:  0.08131044196867103\n",
      "The correlation for NAcc_seg1 is:  0.40849988241397295\n",
      "The shape of the combined dataframe is:  (30, 161)\n",
      "The shape of the aggregate combined dataframe is:  (30, 161)\n",
      "When dropping participant:  sub-33\n",
      "The correlation for NAcc_DP1 is:  0.41781440731212616\n",
      "The correlation for NAcc_DP2 is:  0.567953134632143\n",
      "The correlation for NAcc_DP3 is:  0.598395574471541\n",
      "The correlation for NAcc_TR3 is:  0.2688972703193892\n",
      "The correlation for NAcc_TR4 is:  0.06051372991217316\n",
      "The correlation for NAcc_seg1 is:  0.44429460025578044\n",
      "The shape of the combined dataframe is:  (30, 161)\n",
      "The shape of the aggregate combined dataframe is:  (30, 161)\n",
      "When dropping participant:  sub-34\n",
      "The correlation for NAcc_DP1 is:  0.4226107610324801\n",
      "The correlation for NAcc_DP2 is:  0.6143616659760853\n",
      "The correlation for NAcc_DP3 is:  0.6429200965184596\n",
      "The correlation for NAcc_TR3 is:  0.3034715725801669\n",
      "The correlation for NAcc_TR4 is:  0.023292678231694744\n",
      "The correlation for NAcc_seg1 is:  0.41926832980867357\n",
      "The shape of the combined dataframe is:  (30, 161)\n",
      "The shape of the aggregate combined dataframe is:  (30, 161)\n",
      "When dropping participant:  sub-35\n",
      "The correlation for NAcc_DP1 is:  0.2949282756525771\n",
      "The correlation for NAcc_DP2 is:  0.5623143893750743\n",
      "The correlation for NAcc_DP3 is:  0.5555200789767056\n",
      "The correlation for NAcc_TR3 is:  0.2777286454387284\n",
      "The correlation for NAcc_TR4 is:  0.02538583090892413\n",
      "The correlation for NAcc_seg1 is:  0.38552804856745726\n",
      "The shape of the combined dataframe is:  (30, 161)\n",
      "The shape of the aggregate combined dataframe is:  (30, 161)\n",
      "When dropping participant:  sub-36\n",
      "The correlation for NAcc_DP1 is:  0.3316335397408927\n",
      "The correlation for NAcc_DP2 is:  0.6058970348315478\n",
      "The correlation for NAcc_DP3 is:  0.6240341276632573\n",
      "The correlation for NAcc_TR3 is:  0.25378399704245047\n",
      "The correlation for NAcc_TR4 is:  0.10323743013166407\n",
      "The correlation for NAcc_seg1 is:  0.4208124887475799\n",
      "The shape of the combined dataframe is:  (30, 161)\n",
      "The shape of the aggregate combined dataframe is:  (30, 161)\n",
      "When dropping participant:  sub-37\n",
      "The correlation for NAcc_DP1 is:  0.439942688198295\n",
      "The correlation for NAcc_DP2 is:  0.5978184915723548\n",
      "The correlation for NAcc_DP3 is:  0.6293340054887604\n",
      "The correlation for NAcc_TR3 is:  0.2345216148475648\n",
      "The correlation for NAcc_TR4 is:  0.0093366071790838\n",
      "The correlation for NAcc_seg1 is:  0.4149688563114768\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "participant_list = [participant_id for participant_id in participant_dictionaries.keys()]\n",
    "\n",
    "NAcc_DP1_AggCom = []\n",
    "NAcc_DP2_AggCom = []\n",
    "NAcc_DP3_AggCom = []\n",
    "NAcc_TR1_AggCom = []\n",
    "NAcc_TR2_AggCom = []\n",
    "NAcc_TR3_AggCom = []\n",
    "NAcc_TR4_AggCom = []\n",
    "NAcc_seg1_AggCom = []\n",
    "\n",
    "\n",
    "for sub_id in participant_list:\n",
    "\n",
    "    current_participant_list = participant_list.copy()\n",
    "\n",
    "    # drop current participant from the list.\n",
    "    current_participant_list.remove(sub_id)\n",
    "\n",
    "\n",
    "    all_subs_events_df = get_SegData(current_participant_list, participant_dictionaries_zscored, participant_dictionaries)\n",
    "\n",
    "    # drop the participant from dataframe.\n",
    "    all_subs_events_df = all_subs_events_df[all_subs_events_df[\"Participant\"] != sub_id] \n",
    "\n",
    "    all_subs_events_df.reset_index(drop=True, inplace=True) \n",
    "\n",
    "    all_peaks_id = get_PeakInformation(all_subs_events_df, r1_keys_sorted, r2_keys_sorted)\n",
    "\n",
    "    all_peaks_id = get_PeakInformation2(all_subs_events_df, all_peaks_id)\n",
    "\n",
    "    all_peaks_id, Like_horror_list, Like_comedy_list = get_PeakInformation3(all_subs_events_df, all_peaks_id)\n",
    "\n",
    "    all_subs_events_df  = FinalIndividualProcessing(all_subs_events_df, all_peaks_id)\n",
    "\n",
    "    aggregate_df, aggregate_comedy_df, aggregate_horror = get_AggregateData(all_subs_events_df)\n",
    "\n",
    "    # Get correlations.\n",
    "    corr_NAcc_DP1 = pearsonr(aggregate_comedy_df[\"NAcc_DP1\"], aggregate_comedy_df[\"Gross_M1_updated_log\"])[0]\n",
    "    corr_NAcc_DP2 = pearsonr(aggregate_comedy_df[\"NAcc_DP2\"], aggregate_comedy_df[\"Gross_M1_updated_log\"])[0]\n",
    "    corr_NAcc_DP3 = pearsonr(aggregate_comedy_df[\"NAcc_DP3\"], aggregate_comedy_df[\"Gross_M1_updated_log\"])[0]\n",
    "    corr_NAcc_TR1 = pearsonr(aggregate_comedy_df[\"NAcc_TR1\"], aggregate_comedy_df[\"Gross_M1_updated_log\"])[0]\n",
    "    corr_NAcc_TR2 = pearsonr(aggregate_comedy_df[\"NAcc_TR2\"], aggregate_comedy_df[\"Gross_M1_updated_log\"])[0]\n",
    "    corr_NAcc_TR3 = pearsonr(aggregate_comedy_df[\"NAcc_TR3\"], aggregate_comedy_df[\"Gross_M1_updated_log\"])[0]\n",
    "    corr_NAcc_TR4 = pearsonr(aggregate_comedy_df[\"NAcc_TR4\"], aggregate_comedy_df[\"Gross_M1_updated_log\"])[0]\n",
    "    corr_NAcc_Seg1 = pearsonr(aggregate_comedy_df[\"NAcc_seg1\"], aggregate_comedy_df[\"Gross_M1_updated_log\"])[0]\n",
    "\n",
    "    # Append to the list.\n",
    "    NAcc_DP1_AggCom.append(corr_NAcc_DP1)\n",
    "    NAcc_DP2_AggCom.append(corr_NAcc_DP2)\n",
    "    NAcc_DP3_AggCom.append(corr_NAcc_DP3)\n",
    "    NAcc_TR1_AggCom.append(corr_NAcc_TR1)\n",
    "    NAcc_TR2_AggCom.append(corr_NAcc_TR2)\n",
    "    NAcc_TR3_AggCom.append(corr_NAcc_TR3)\n",
    "    NAcc_TR4_AggCom.append(corr_NAcc_TR4)\n",
    "    NAcc_seg1_AggCom.append(corr_NAcc_Seg1)\n",
    "\n",
    "\n",
    "    print(\"When dropping participant: \", sub_id)\n",
    "    print(\"The correlation for NAcc_DP1 is: \", corr_NAcc_DP1)\n",
    "    print(\"The correlation for NAcc_DP2 is: \", corr_NAcc_DP2)\n",
    "    print(\"The correlation for NAcc_DP3 is: \", corr_NAcc_DP3)\n",
    "    print(\"The correlation for NAcc_TR3 is: \", corr_NAcc_TR3)\n",
    "    print(\"The correlation for NAcc_TR4 is: \", corr_NAcc_TR4)\n",
    "    print(\"The correlation for NAcc_seg1 is: \", corr_NAcc_Seg1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 0., 3., 1., 2., 0., 3., 1., 1., 5., 5., 4., 4., 1., 1.]),\n",
       " array([0.43946187, 0.45664639, 0.4738309 , 0.49101541, 0.50819992,\n",
       "        0.52538444, 0.54256895, 0.55975346, 0.57693797, 0.59412249,\n",
       "        0.611307  , 0.62849151, 0.64567602, 0.66286054, 0.68004505,\n",
       "        0.69722956]),\n",
       " <BarContainer object of 15 artists>)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAAGdCAYAAABO2DpVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAW4ElEQVR4nO3da4xU9f348c8KMljLLlUEFkWUtoA3bNVC8NJqpVYkin2ExlpqrE3M2tRQk8oDxW21S2Nq27SGWosS09q1NvES66XVZDVW8YKhBYxW6BLXC2Ix7gKtY909/we/uP2vXGSWz7A7y+uVTOKcPXPO93znOPvO7AynriiKIgAAEuw30AMAAIYOYQEApBEWAEAaYQEApBEWAEAaYQEApBEWAEAaYQEApBm+t3fY09MTb7zxRowaNSrq6ur29u4BgH4oiiK2bNkSEyZMiP322/n7Ens9LN54442YOHHi3t4tAJCgo6MjDjvssJ3+fK+HxahRoyLi/wZWX1+/t3cPAPRDV1dXTJw4sff3+M7s9bD48M8f9fX1wgIAaszHfYzBhzcBgDTCAgBIIywAgDTCAgBIIywAgDTCAgBIIywAgDTCAgBIIywAgDTCAgBIU1FYXHfddVFXV9fnNm3atGqNDQCoMRVfK+SYY46JRx999H8bGL7XLzcCAAxSFVfB8OHDY/z48dUYCwBQ4yr+jMUrr7wSEyZMiMmTJ8dFF10Ur7766i7XL5fL0dXV1ecGAAxNdUVRFLu78kMPPRRbt26NqVOnxptvvhnNzc3x+uuvx5o1a3Z6ffbrrrsumpubt1ve2dnpsunAoHDE1X8a6CHsVRuWzB3oIVCDurq6oqGh4WN/f1cUFh/17rvvxqRJk+Kmm26KSy+9dIfrlMvlKJfLfQY2ceJEYQEMGsICPt7uhsUeffJy9OjRMWXKlFi3bt1O1ymVSlEqlfZkNwBAjdijf8di69atsX79+mhsbMwaDwBQwyoKi6uuuioef/zx2LBhQzz11FPxta99LYYNGxYXXnhhtcYHANSQiv4U8tprr8WFF14YmzdvjkMOOSROPfXUWLFiRRxyyCHVGh8AUEMqCovW1tZqjQMAGAJcKwQASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0exQWS5Ysibq6urjyyiuThgMA1LJ+h8Vzzz0Xt9xyS0yfPj1zPABADetXWGzdujUuuuiiuPXWW+NTn/pU9pgAgBrVr7BoamqKuXPnxuzZsz923XK5HF1dXX1uAMDQNLzSB7S2tsYLL7wQzz333G6t39LSEs3NzRUPDIDqOOLqPw30EPaqDUvmDvQQ9ikVvWPR0dER3/3ud+N3v/tdjBw5crces2jRoujs7Oy9dXR09GugAMDgV9E7FitXroxNmzbFCSec0Lusu7s7nnjiifjlL38Z5XI5hg0b1ucxpVIpSqVSzmgBgEGtorA488wzY/Xq1X2WXXLJJTFt2rT4/ve/v11UAAD7lorCYtSoUXHsscf2WXbggQfGwQcfvN1yAGDf41/eBADSVPytkI9qa2tLGAYAMBR4xwIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASFNRWCxdujSmT58e9fX1UV9fH7NmzYqHHnqoWmMDAGpMRWFx2GGHxZIlS2LlypXx/PPPx5e//OWYN29erF27tlrjAwBqyPBKVj733HP73L/hhhti6dKlsWLFijjmmGNSBwYA1J6KwuL/193dHXfffXds27YtZs2atdP1yuVylMvl3vtdXV393SUAMMhVHBarV6+OWbNmxXvvvRef/OQn45577omjjz56p+u3tLREc3PzHg2SHTvi6j8NyH43LJk7IPvdF3mOgVpT8bdCpk6dGqtWrYpnnnkmLr/88liwYEG8+OKLO11/0aJF0dnZ2Xvr6OjYowEDAINXxe9YjBgxIj7zmc9ERMSJJ54Yzz33XPz85z+PW265ZYfrl0qlKJVKezZKAKAm7PG/Y9HT09PnMxQAwL6roncsFi1aFHPmzInDDz88tmzZEnfeeWe0tbXFI488Uq3xAQA1pKKw2LRpU3zjG9+IN998MxoaGmL69OnxyCOPxFe+8pVqjQ8AqCEVhcWyZcuqNQ4AYAhwrRAAII2wAADSCAsAII2wAADSCAsAII2wAADSCAsAII2wAADSCAsAII2wAADSCAsAII2wAADSCAsAII2wAADSCAsAII2wAADSCAsAII2wAADSCAsAII2wAADSCAsAII2wAADSCAsAII2wAADSCAsAII2wAADSCAsAII2wAADSCAsAII2wAADSCAsAII2wAADSCAsAII2wAADSCAsAII2wAADSCAsAII2wAADSCAsAII2wAADSCAsAII2wAADSCAsAII2wAADSCAsAII2wAADSCAsAII2wAADSCAsAII2wAADSCAsAII2wAADSCAsAII2wAADSCAsAII2wAADSCAsAII2wAADSCAsAIE1FYdHS0hJf+MIXYtSoUTF27Ng4//zz4+WXX67W2ACAGlNRWDz++OPR1NQUK1asiL/85S/x3//+N84666zYtm1btcYHANSQ4ZWs/PDDD/e5v3z58hg7dmysXLkyvvjFL6YODACoPRWFxUd1dnZGRMRBBx2003XK5XKUy+Xe+11dXXuySwBgEOt3WPT09MSVV14Zp5xyShx77LE7Xa+lpSWam5v7uxvodcTVfxqQ/W5YMndA9gtQi/r9rZCmpqZYs2ZNtLa27nK9RYsWRWdnZ++to6Ojv7sEAAa5fr1jccUVV8QDDzwQTzzxRBx22GG7XLdUKkWpVOrX4ACA2lJRWBRFEd/5znfinnvuiba2tjjyyCOrNS4AoAZVFBZNTU1x5513xn333RejRo2KjRs3RkREQ0NDHHDAAVUZIABQOyr6jMXSpUujs7MzTj/99GhsbOy93XXXXdUaHwBQQyr+UwgAwM64VggAkEZYAABphAUAkEZYAABphAUAkEZYAABphAUAkEZYAABphAUAkEZYAABphAUAkEZYAABphAUAkEZYAABphAUAkEZYAABphAUAkEZYAABphAUAkEZYAABphAUAkEZYAABphAUAkEZYAABphAUAkEZYAABphAUAkEZYAABphAUAkEZYAABphAUAkEZYAABphAUAkEZYAABphAUAkEZYAABphAUAkEZYAABphAUAkEZYAABphAUAkEZYAABphAUAkEZYAABphAUAkEZYAABphAUAkEZYAABphAUAkEZYAABphAUAkEZYAABphAUAkEZYAABphAUAkEZYAABphAUAkEZYAABphAUAkEZYAABpKg6LJ554Is4999yYMGFC1NXVxb333luFYQEAtajisNi2bVscf/zxcfPNN1djPABADRte6QPmzJkTc+bMqcZYAIAaV3FYVKpcLke5XO6939XVVe1dAgADpOph0dLSEs3NzdXeTUREHHH1n/bKfj5qw5K5A7LfgTJQ88ze4zlmKNnXzueB/p1U9W+FLFq0KDo7O3tvHR0d1d4lADBAqv6ORalUilKpVO3dAACDgH/HAgBIU/E7Flu3bo1169b13m9vb49Vq1bFQQcdFIcffnjq4ACA2lJxWDz//PNxxhln9N5fuHBhREQsWLAgli9fnjYwAKD2VBwWp59+ehRFUY2xAAA1zmcsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0wgIASCMsAIA0/QqLm2++OY444ogYOXJkzJw5M5599tnscQEANajisLjrrrti4cKFsXjx4njhhRfi+OOPj69+9auxadOmaowPAKghFYfFTTfdFJdddllccsklcfTRR8evfvWr+MQnPhG33XZbNcYHANSQ4ZWs/P7778fKlStj0aJFvcv222+/mD17djz99NM7fEy5XI5yudx7v7OzMyIiurq6+jPeXeop/zt9m7ujGseyOwbqePc1A/X8RniOgcpV6zXrw+0WRbHL9SoKi3/961/R3d0d48aN67N83Lhx8dJLL+3wMS0tLdHc3Lzd8okTJ1ay60Gt4WcDPQKqyfML1JJqv2Zt2bIlGhoadvrzisKiPxYtWhQLFy7svd/T0xPvvPNOHHzwwVFXV1ft3fdLV1dXTJw4MTo6OqK+vn6ghzOkmNvqMr/VY26rx9xWV9b8FkURW7ZsiQkTJuxyvYrCYsyYMTFs2LB46623+ix/6623Yvz48Tt8TKlUilKp1GfZ6NGjK9ntgKmvr3eSV4m5rS7zWz3mtnrMbXVlzO+u3qn4UEUf3hwxYkSceOKJ8dhjj/Uu6+npicceeyxmzZpV+QgBgCGl4j+FLFy4MBYsWBAnnXRSzJgxI372s5/Ftm3b4pJLLqnG+ACAGlJxWMyfPz/efvvtuPbaa2Pjxo3xuc99Lh5++OHtPtBZy0qlUixevHi7P+Gw58xtdZnf6jG31WNuq2tvz29d8XHfGwEA2E2uFQIApBEWAEAaYQEApBEWAECafSIs+nuZ99bW1qirq4vzzz+/z/JvfvObUVdX1+d29tlnV2HktaGS+V2+fPl2czdy5Mg+6xRFEddee200NjbGAQccELNnz45XXnml2ocxKGXPrXP3fyp9XXj33XejqakpGhsbo1QqxZQpU+LBBx/co20OZdnze91112137k6bNq3ahzEoVTK3p59++nbzVldXF3Pnzu1dJ/01txjiWltbixEjRhS33XZbsXbt2uKyyy4rRo8eXbz11lu7fFx7e3tx6KGHFqeddloxb968Pj9bsGBBcfbZZxdvvvlm7+2dd96p4lEMXpXO7+23317U19f3mbuNGzf2WWfJkiVFQ0NDce+99xZ/+9vfivPOO6848sgji//85z9745AGjWrMrXP3/1Q6t+VyuTjppJOKc845p3jyySeL9vb2oq2trVi1alW/tzmUVWN+Fy9eXBxzzDF9zt233357bx3SoFHp3G7evLnPnK1Zs6YYNmxYcfvtt/euk/2aO+TDYsaMGUVTU1Pv/e7u7mLChAlFS0vLTh/zwQcfFCeffHLxm9/8pliwYMEOw+Kjy/ZVlc7v7bffXjQ0NOx0ez09PcX48eOLG2+8sXfZu+++W5RKpeL3v/992rhrQfbcFoVz90OVzu3SpUuLyZMnF++//37aNoeyaszv4sWLi+OPPz57qDVnT8+zn/70p8WoUaOKrVu3FkVRndfcIf2nkA8v8z579uzeZR93mfeIiB/84AcxduzYuPTSS3e6TltbW4wdOzamTp0al19+eWzevDl17LWgv/O7devWmDRpUkycODHmzZsXa9eu7f1Ze3t7bNy4sc82GxoaYubMmbvc5lBTjbn90L5+7vZnbu+///6YNWtWNDU1xbhx4+LYY4+NH/3oR9Hd3d3vbQ5V1ZjfD73yyisxYcKEmDx5clx00UXx6quvVvVYBpuM82zZsmVxwQUXxIEHHhgR1XnNHdJhsavLvG/cuHGHj3nyySdj2bJlceutt+50u2effXbccccd8dhjj8WPf/zjePzxx2POnDnb/U8w1PVnfqdOnRq33XZb3HffffHb3/42enp64uSTT47XXnstIqL3cZVscyiqxtxGOHcj+je3//znP+OPf/xjdHd3x4MPPhjXXHNN/OQnP4nrr7++39scqqoxvxERM2fOjOXLl8fDDz8cS5cujfb29jjttNNiy5YtVT2ewWRPz7Nnn3021qxZE9/61rd6l1XjNbfql02vJVu2bImLL744br311hgzZsxO17vgggt6//u4446L6dOnx6c//eloa2uLM888c28MtWbNmjWrzwXrTj755DjqqKPilltuiR/+8IcDOLLatztz69ztn56enhg7dmz8+te/jmHDhsWJJ54Yr7/+etx4442xePHigR5ezdud+Z0zZ07v+tOnT4+ZM2fGpEmT4g9/+MMu313mf5YtWxbHHXdczJgxo6r7GdLvWFR6mff169fHhg0b4txzz43hw4fH8OHD44477oj7778/hg8fHuvXr9/hfiZPnhxjxoyJdevWVeU4BqtK53dH9t9///j85z/fO3cfPm5PtjkUVGNud2RfPHf7M7eNjY0xZcqUGDZsWO+yo446KjZu3Bjvv/9+yvM1VFRjfndk9OjRMWXKFOdu7N55tm3btmhtbd0uwqrxmjukw6LSy7xPmzYtVq9eHatWreq9nXfeeXHGGWfEqlWrYuLEiTvcz2uvvRabN2+OxsbGqh3LYFTp/O5Id3d3rF69unfujjzyyBg/fnyfbXZ1dcUzzzyz29scCqoxtzuyL567/ZnbU045JdatWxc9PT29y/7xj39EY2NjjBgxIuX5GiqqMb87snXr1li/fr1zdzfPs7vvvjvK5XJ8/etf77O8Kq+5/frIZw1pbW0tSqVSsXz58uLFF18svv3tbxejR4/u/RrexRdfXFx99dU7ffxHP0W/ZcuW4qqrriqefvrpor29vXj00UeLE044ofjsZz9bvPfee9U+nEGn0vltbm4uHnnkkWL9+vXFypUriwsuuKAYOXJksXbt2t51lixZUowePbq47777ir///e/FvHnz9tmvm2bOrXP3fyqd21dffbUYNWpUccUVVxQvv/xy8cADDxRjx44trr/++t3e5r6kGvP7ve99r2hrayva29uLv/71r8Xs2bOLMWPGFJs2bdrrxzeQ+vs77dRTTy3mz5+/w21mv+YO+bAoiqL4xS9+URx++OHFiBEjihkzZhQrVqzo/dmXvvSlYsGCBTt97EfD4t///ndx1llnFYccckix//77F5MmTSouu+yyffLF40OVzO+VV17Zu+64ceOKc845p3jhhRf6bK+np6e45pprinHjxhWlUqk488wzi5dffnlvHc6gkjm3zt2+Kn1deOqpp4qZM2cWpVKpmDx5cnHDDTcUH3zwwW5vc1+TPb/z588vGhsbixEjRhSHHnpoMX/+/GLdunV763AGlUrn9qWXXioiovjzn/+8w+1lv+a6bDoAkGZIf8YCANi7hAUAkEZYAABphAUAkEZYAABphAUAkEZYAABphAUAkEZYAABphAUAkEZYAABphAUAkOb/Ad7wViP9diiGAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "np.mean(NAcc_DP3_AggCom)\n",
    "\n",
    "plt.hist(NAcc_DP3_AggCom, bins=15)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
